= "Got OpenShift Storage" Workshop Guide

// Start OCP3+OCS3 lab with custom lab guide (30 mins)
// Lab Environment 
// Verification including Prometheus + Heketi 
// Create new users via LDAP that have cluster-reader for console login
// Investigate gluster and install rails+postgresql 
// Look at gluster volume usage (PVC) available in Prometheus (kublet_volume)
// Start OCP3+OCS4 lab (40 mins)
// Use everything in readme.adoc except deploy rails+postgresql (should readme.adoc section2 be repeated)

// https://github.com/openshift/openshift-cns-testdrive/tree/master/labguide
// https://github.com/travisn/rook/tree/openshift-commons-demo/workshop

:numbered:
== OpenShift Container Platform Environment Overview

You will be interacting with an OpenShift 3.11 cluster that is running on Amazon Web Services. During the lab you will also use OpenShift Container Storage 3.11 and and install Rook on top of the OpenShift Cluster.

The complete environment consists of the following systems:

* 1 master node
* 1 infrastructure node
* 6 worker nodes
** 3 will run workload and the initial Container Native Storage instances
** 3 will be added to the cluster later and will be used to deploy Rook
* 1 server running Red Hat Identity Management (IdM, for LDAP authentication)

.Lab Environment Overview
[options="header"]
|==============================================
| Role | Internal FQDN
| Master Node | master.internal.aws.testdrive.openshift.com
| Infrastructure Node | infra.internal.aws.testdrive.openshift.com
| Application Node #1 | node01.internal.aws.testdrive.openshift.com
| Application Node #2 | node02.internal.aws.testdrive.openshift.com
| Application Node #3 | node03.internal.aws.testdrive.openshift.com
| Application Node #4 | node04.internal.aws.testdrive.openshift.com
| Application Node #5 | node05.internal.aws.testdrive.openshift.com
| Application Node #6 | node06.internal.aws.testdrive.openshift.com
| IdM Server | idm.internal.aws.testdrive.openshift.com
|==============================================

All addresses are internal to the lab environment. The only system you
publicly access via SSH and the browser is the OpenShift Master node:

.Public Lab Access
[options="header"]
|==============================================
| Role | Public FQDN
| Master Node | *Consult your handout for the actual master URL* (it will be similar to `master.1234567890.aws.testdrive.openshift.com`)
|==============================================

Note that references to product documentation will be specifically pointing
to the 3.11 versions, but newer software and documentation versions may be
available.

=== Conventions

You will see various code and command blocks throughout these exercises. Some of the command blocks can be copy/pasted directly. Others will require modification of the command before execution. If you see a command block with a red border (see below), the command will require slight modification.

[source,none,role="copypaste copypaste-warning"]
----
some command to modify
----

Most command blocks support auto highlighting with a click. If you hover over the command block above and click, it should automatically highlight all the text to make for easier copying.

=== Logging in

Most of the exercises in this lab will be facilitated using the OpenShift command line client on the master node. For convenient access to the master's command line we provide a web-based SSH console:

*Consult the Handout for actual URL of the SSH Console*. It will be similar to `http://ssh.external.aws.testdrive.openshift.com:8080/ssh/host/master.internal.aws.testdrive.openshift.com`

Use the user name `cloud-user` and the password provided to you on the handout when prompted.

If you prefer to use an SSH client on your system you can do that too, using the same credentials:

[source,bash,role="copypaste"]
----
ssh -l cloud-user <Your Master URL from the handout>
----

Once you are logged in you end up on the OpenShift Master Node:

----
[cloud-user@master ~]$
----

The `cloud-user` account has password-less sudo privileges and SSH login on
all systems using internal addressing from the table above.

// WK: Part 2 Installation / Verification
=== Installation and Verification

The primary method of installing OpenShift Container Platform is based on
Ansible playbooks. These playbooks ship as part of the product in the
`openshift-ansible` package.

This method has, in the past, been referred to as the `advanced installation
method` and it involves Ansible directly running the installation playbooks.
The advanced installer supports many configuration and customization options.
It also covers installation of supporting infrastructure like
OpenShift Container storage, logging and metrics components.

Your environment comes with a preinstalled cluster that has been deployed
using the installer's configuration file (`/etc/ansible/hosts`) when you
started the lab.

For more information on installing OpenShift Container Platform, please refer to the link:https://docs.openshift.com/container-platform/3.11/install/index.html[installation section] of the product documentation.

[NOTE]
====
At this point you should be logged in as `cloud-user` on the OpenShift Master
node via SSH.
====

### Examining the provided Ansible inventory file
First, let's examine the provided installer configuration file.

#### Look at the file
Use `cat`, `less`, or an editor to look at the `/etc/ansible/hosts` file:

[source,bash,role="copypaste"]
----
cat /etc/ansible/hosts
----

General settings and other variable information is defined on lines within the `[OSEv3:vars]` section. There are also various host groups defined for things like *Masters* and *Nodes*.

The top-level playbook in
`/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml` triggers
the installation of the cluster and all of it's components. It's idempotent,
which means you can execute this playbook multiple times without harm. This
also allows you to deploy additional components after the initial install by
simply modifying the configuration in `/etc/ansible/hosts` and re-run the
installer.

In addition to this, there are playbooks that only deploy a specific
component or service, which makes them faster to execute.

[TIP]
====
A typical multi-host installation like this might normally take around an
hour depending on the speed of your internet connection. Disconnected
installation options are also available. Prerequisites and other information
is all covered in the documentation.
====

=== Verifying the Installation

Let's do some basic tests with your installation. As an administrator, most
of your interaction with OpenShift will be from the command line. The `oc`
program is a command line interface that talks to the OpenShift API.

During the OpenShift installation, the `root` system account on the `master`
host is configured to use a special OpenShift "super administrator" (`system:admin`) account.
Because of this, it is vitally important that you protect access to the
`root` system account, or remove this preconfigured config. Otherwise, anyone
who can `sudo` on the master has super user privileges on the entire cluster.

==== Login on the master
Additionally, your Linux system account on the master, `cloud-user`, is
preconfigured to access this OpenShift "super administrator" without a
password. Type the following command to login as the internal super-user on
OpenShift:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

You will see that you got logged in to a project called 'default'. More on
projects later.

----
Logged into "https://master.internal.aws.testdrive.openshift.com:443" as "system:admin" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-public
    kube-system
    management-infra
    openshift
    openshift-console
    openshift-infra
    openshift-logging
    openshift-metrics
    openshift-monitoring
    openshift-node
    openshift-sdn
    openshift-web-console
    storage

Using project "default".
----

==== Look at the Nodes

Execute the following command to see a list of the *Nodes* that OpenShift knows about:

[source,bash,role="copypaste"]
----
oc get nodes
----

The output should look something like the following:

----
NAME                                          STATUS    ROLES     AGE	VERSION
infra.internal.aws.testdrive.openshift.com    Ready     infra     1m	v1.11.0+d4cacc0
master.internal.aws.testdrive.openshift.com   Ready     master    1m	v1.11.0+d4cacc0
node01.internal.aws.testdrive.openshift.com   Ready     compute   1m	v1.11.0+d4cacc0
node02.internal.aws.testdrive.openshift.com   Ready     compute   1m	v1.11.0+d4cacc0
node03.internal.aws.testdrive.openshift.com   Ready     compute   1m	v1.11.0+d4cacc0
----

All of the systems listed in the `[nodes]` group in the `/etc/ansible/hosts` file should be listed here: 1 Infrastructure Node, 1 Master and 3 Worker nodes.

The OpenShift *Master* is also a *Node* because it needs to participate in the software defined network (SDN). The *Infra* node will only run workloads related to supporting OpenShift infrastructure.

=== Verify the Storage cluster

In your environment Red Hat OpenShift Container Storage was installed as part of OpenShift. It will serve robust and persistent storage to both business applications as well as OpenShift infrastructure. It is based on Red Hat Gluster Storage, running in containers on OpenShift nodes and an additional API server called `heketi` that enables the API integration with OpenShift.

We will now use a command line client on the *master* to talk via this server to the container storage cluster. It's password protected, so let's export a couple of environment variables first to configure the client:

[source,bash,role="copypaste"]
----
export HEKETI_CLI_SERVER=http://$(oc get route heketi-storage -n storage -o jsonpath --template='{.spec.host}')
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=myS3cr3tpassw0rd
----

Then use the CLI tool `heketi-cli` to query `heketi` about all the storage clusters it knows about:

[source,bash,role="copypaste"]
----
heketi-cli cluster list
----

`heketi` will list all known clusters with internal UUIDs:

----
Clusters:
Id:998294af2211ff544338490e3e19db65 [file][block]<1>
----
<1> This is the internal UUID of the OCS cluster

[NOTE]
====
The cluster UUID will be different for you since it's automatically generated.
====

To get more detailed information about the topology of your OCS cluster (i.e.
nodes, devices and volumes heketi has discovered) run the following command
(output abbreviated):

[source,bash,role="copypaste"]
----
heketi-cli topology info
----

You will get a lengthy output that describes the GlusterFS cluster topology as it is known by `heketi`:

----
Cluster Id: 998294af2211ff544338490e3e19db65

    File:  true
    Block: true

    Volumes:

	Name: heketidbstorage <1>
	Size: 2
	Id: 0a9dd2d7c931dae933e5a6e6e701d49c
	Cluster Id: 998294af2211ff544338490e3e19db65
	Mount: 10.0.3.28:heketidbstorage
	Mount Options: backup-volfile-servers=10.0.4.14,10.0.1.83
	Durability Type: replicate
	Replica: 3
	Snapshot: Disabled

		Bricks:
			Id: 11b26cef66e828ece65d834138ffe976
			Path: /var/lib/heketi/mounts/vg_f3668aa3855cd9a84642ca29db45af1c/brick_11b26cef66e828ece65d834138ffe976/brick
			Size (GiB): 2
			Node: 7c43c7bf6d505c74c4a71cf4f7cc8b6a
			Device: f3668aa3855cd9a84642ca29db45af1c

			Id: 2a3d7a2b4392139fd26cc76d8354d474
			Path: /var/lib/heketi/mounts/vg_5a46f5d3788ed61352f565385edce8d5/brick_2a3d7a2b4392139fd26cc76d8354d474/brick
			Size (GiB): 2
			Node: 5a284ad7ed633f2d9879b3ff3833607b
			Device: 5a46f5d3788ed61352f565385edce8d5

			Id: 358a23c9511817a660a51aaaec90df08
			Path: /var/lib/heketi/mounts/vg_550bc327799e3c436a2e35e4b584c2ca/brick_358a23c9511817a660a51aaaec90df08/brick
			Size (GiB): 2
			Node: 7a814aa4abcebfad2ede80d51dc417b3
			Device: 550bc327799e3c436a2e35e4b584c2ca


    Nodes:

	Node Id: 5a284ad7ed633f2d9879b3ff3833607b
	State: online
	Cluster Id: 998294af2211ff544338490e3e19db65
	Zone: 2
	Management Hostnames: node02.internal.aws.testdrive.openshift.com
	Storage Hostnames: 10.0.3.28
	Devices:
		Id:5a46f5d3788ed61352f565385edce8d5   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
			Bricks:
				Id:2a3d7a2b4392139fd26cc76d8354d474   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_5a46f5d3788ed61352f565385edce8d5/brick_2a3d7a2b4392139fd26cc76d8354d474/brick

	Node Id: 7a814aa4abcebfad2ede80d51dc417b3
	State: online
	Cluster Id: 998294af2211ff544338490e3e19db65
	Zone: 3
	Management Hostnames: node03.internal.aws.testdrive.openshift.com
	Storage Hostnames: 10.0.4.14
	Devices:
		Id:550bc327799e3c436a2e35e4b584c2ca   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
			Bricks:
				Id:358a23c9511817a660a51aaaec90df08   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_550bc327799e3c436a2e35e4b584c2ca/brick_358a23c9511817a660a51aaaec90df08/brick

	Node Id: 7c43c7bf6d505c74c4a71cf4f7cc8b6a
	State: online
	Cluster Id: 998294af2211ff544338490e3e19db65
	Zone: 1
	Management Hostnames: node01.internal.aws.testdrive.openshift.com
	Storage Hostnames: 10.0.1.83
	Devices:
		Id:f3668aa3855cd9a84642ca29db45af1c   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
			Bricks:
				Id:11b26cef66e828ece65d834138ffe976   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_f3668aa3855cd9a84642ca29db45af1c/brick_11b26cef66e828ece65d834138ffe976/brick
----
<1> An internal GlusterFS volume that is automatically generated by the setup routine to hold the heketi database.

This output tells you that Red Hat OpenShift Container Storage currently
consists of a single cluster, which consists of 3 nodes, each with a single
block device `/dev/xvdd` of 50GiB in size. The GlusterFS layer will turn
these 3 devices/hosts into a single, flat storage pool from which OpenShift
will be able to carve out either distinct filesystem volumes or block devices
that serve as persistent storage for containers.

// WK: Part 2: OCS
== OpenShift Container Storage Concepts

In this lab we are going to provide a view 'under the hood' of OpenShift `PersistentVolumes` provided by OpenShift Container Storage (OCS). For this purpose we will examine volumes leveraged by example applications using
different volume access modes.

=== How OpenShift Container Storage runs

Make sure you are still logged on as the super user:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

OpenShift Container Storage is GlusterFS running in containers, specifically in pods managed by OpenShift. We have looked at the pods making up the storage cluster already in the introduction chapter. Go ahead and switch to the storage project:

[source,bash,role="copypaste"]
----
oc project storage
----

Then, take a look at the storage *Pods*:

[source,bash,role="copypaste"]
----
oc get pods -o wide
----

Which yields:

----
NAME                      READY     STATUS    RESTARTS   AGE       IP           NODE                                             NOMINATED NODE
glusterfs-storage-l5sxd   1/1       Running   0          3h        10.0.1.83    node01.internal.aws.testdrive.openshift.com <1>   <none>
glusterfs-storage-l99db   1/1       Running   0          3h        10.0.4.14    node03.internal.aws.testdrive.openshift.com <1>  <none>
glusterfs-storage-tsr4g   1/1       Running   0          3h        10.0.3.28    node02.internal.aws.testdrive.openshift.com <1>  <none>
heketi-storage-1-c6tt8    1/1       Running   0          3h        10.128.2.7   infra.internal.aws.testdrive.openshift.com  <2>  <none>
----
<1> OCS *Pods*, with each of the designated nodes running exactly one.
<2> heketi API frontend pod

[NOTE]
====
The exact *pod* names will be different in your environment, since they are
auto-generated. Also the heketi *pod* might run on any node.
====

The OCS *Pods* use the host's network and block devices to run the software-defined storage system. See schematic below for a visualization.

.GlusterFS pods in OCS in detail.
image::./images/cns_diagram_pod.png[]

`heketi` is a component that exposes an API to the storage system for OpenShift. This allows OpenShift to dynamically allocate storage from OCS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our example heketi runs on the OpenShift application nodes, not on the infrastructure node.

.heketi pod running in OCS
image::./images/cns_diagram_heketi.png[]

==== Examine heketi

To expose heketi's API outside of OpenShift for administrators (for
monitoring and maintenance), a *Service* named _heketi-storage_ and a *Route*
has been set up:

[source,bash,role="copypaste"]
----
oc get service,route
----

You will see something like:

----
NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/heketi-db-storage-endpoints   ClusterIP   172.30.170.71   <none>        1/TCP      3h
service/heketi-storage                ClusterIP   172.30.54.200   <none>        8080/TCP   3h

NAME                                      HOST/PORT                                                              PATH      SERVICES         PORT      TERMINATION   WILDCARD
route.route.openshift.io/heketi-storage   heketi-storage-storage.apps.538432900127.aws.testdrive.openshift.com             heketi-storage   <all>                   None
----

You may verify external availability of this API and heketi being alive with a  rivial health check:

[source,bash,role="copypaste"]
----
curl -w "\n" http://$(oc get route heketi-storage -n storage -o jsonpath --template='{.spec.host}')/hello
----

This should return:

----
Hello from Heketi
----

This how the heketi API is made available to both external clients, like `heketi-cli` which we examined in the introduction. But mainly it is leveraged by OpenShift to provision storage dynamically. Let's look at this use case.

=== A Simple OCS Use Case

We are going to deploy a sample application that ships with OpenShift which creates a PVC as part of the deployment. Log on to the system as `fancyuser1`, using the password `openshift` and create a project with the name `my-database-app`.

==== Create/Deploy the Application

[source,bash,role="copypaste"]
----
oc login -u fancyuser1 -p openshift
oc new-project my-database-app
----

The example application ships in the form of ready-to-use resource templates. Enter the following command to look at the template for a sample Ruby on Rails application with a PostgreSQL database:

[source,bash,role="copypaste"]
----
oc get template/rails-pgsql-persistent -n openshift
----

This template creates a Rails Application instance which mimics a very basic weblog. The articles and comments are saved in a PostgreSQL database which runs in another pod.

As part of the resource template, a PVC is created in the YAML. Run the ollowing command to `grep` the relavant part:


[source,bash,role="copypaste"]
----
oc get template/rails-pgsql-persistent -n openshift -o yaml | grep PersistentVolumeClaim -A8
----

This shows the basic structure of a `PersistentVolumeClaim`:

[source,yaml]
----
kind: PersistentVolumeClaim
metadata:
  name: ${DATABASE_SERVICE_NAME}
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: ${VOLUME_CAPACITY}
----

This will request a *PersistentVolume* in ReadWriteOnce (`RWO`) mode. Storage provided in this mode can only be mounted by a single pod at a time. For a database that is usually what you want. The requested capacity under `spec.resources.requests.storage` is coming in via a parameter when the template is parsed. This is how storage is _requested_.

Using persistent storage is done via a `PersistentVolume` provided in
response to this `PersistentVolumeClaim`. A `PersistentVolume` is a
representation of some physical storage capacity provisioned by the backing
storage system. It will supply the PostgreSQL pod with persistent storage on
the mount point `/var/lib/pgsql/data`.

You can see this when inspecting how the pod is described as part of the
`DeploymentConfig`:

[source,bash,role="copypaste"]
----
oc get template/rails-pgsql-persistent -n openshift -o yaml | grep mountPath -B58 -A5
----

Will show:

[source,yaml]
----
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      description: Defines how to deploy the database
      template.alpha.openshift.io/wait-for-ready: "true"
    name: ${DATABASE_SERVICE_NAME}
  spec:
    replicas: 1
    selector:
      name: ${DATABASE_SERVICE_NAME}
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          name: ${DATABASE_SERVICE_NAME}
        name: ${DATABASE_SERVICE_NAME}
      spec:
        containers:
        - env:
          - name: POSTGRESQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${NAME}
          - name: POSTGRESQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${NAME}
          - name: POSTGRESQL_DATABASE
            value: ${DATABASE_NAME}
          - name: POSTGRESQL_MAX_CONNECTIONS
            value: ${POSTGRESQL_MAX_CONNECTIONS}
          - name: POSTGRESQL_SHARED_BUFFERS
            value: ${POSTGRESQL_SHARED_BUFFERS}
          image: ' '
          livenessProbe:
            initialDelaySeconds: 30
            tcpSocket:
              port: 5432
            timeoutSeconds: 1
          name: postgresql
          ports:
          - containerPort: 5432
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -i
              - -c
              - psql -h 127.0.0.1 -U ${POSTGRESQL_USER} -q -d ${POSTGRESQL_DATABASE}
                -c 'SELECT 1'
            initialDelaySeconds: 5
            timeoutSeconds: 1
          resources:
            limits:
              memory: ${MEMORY_POSTGRESQL_LIMIT}
          volumeMounts:
          - mountPath: /var/lib/pgsql/data <1>
            name: ${DATABASE_SERVICE_NAME}-data <2>
        volumes:
        - name: ${DATABASE_SERVICE_NAME}-data <2>
          persistentVolumeClaim:
            claimName: ${DATABASE_SERVICE_NAME} <3>
----
<1> The mount path where the persistent storage should appear inside the container
<2> The name of the volume known by the container
<3> The `PersistentVolumeClaim` from which this volume should come from

[TIP]
====
In the above snippet you see there are even more parameters in this template.
If you want to see more about the parameters or other details of this
template, you can execute the following:

 oc describe template rails-pgsql-persistent -n openshift
====

The following diagram sums up how storage get's provisioned in OpenShift and
depicts the relationship of `PersistentVolumes`, `PersistentVolumeClaims` and
`StorageClasses`:

.OpenShift Persistent Volume Framework
image::./images/cns_diagram_pvc.png[]

Let's try it out. The storage size parameter in the template is called
`VOLUME_CAPACITY`. The `new-app` command will again handle processing and
interpreting a *Template* into the appropriate OpenShift objects. We will
specify that we want _5Gi_ of storage as part of deploying a new app from the
template as follows:

[source,bash,role="copypaste"]
----
oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi
----

[NOTE]
====
The `new-app` command will automatically check for templates in the special
`openshift` namespace. In fact, `new-app` tries to do quite a lot of interesting
automagic things, including code introspection when pointed at code
repositories. It is a developer's good friend.
====

You will then see something like the following:

----
--> Deploying template "openshift/rails-pgsql-persistent" to project my-database-app                                                                                                                       [2/1622]

     Rails + PostgreSQL
     ---------
     An example Rails application with a PostgreSQL database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/rails-ex/blob/master/README.md.

     The following service(s) have been created in your project: rails-pgsql-persistent, postgresql.
     
     For more information about using this template, including OpenShift considerations, see https://github.com/openshift/rails-ex/blob/master/README.md.

     * With parameters:
        * Name=rails-pgsql-persistent
        * Namespace=openshift
        * Memory Limit=512Mi
        * Memory Limit (PostgreSQL)=512Mi
        * Volume Capacity=5Gi
        * Git Repository URL=https://github.com/openshift/rails-ex.git
        * Git Reference=
        * Context Directory=
        * Application Hostname=
        * GitHub Webhook Secret=pIXDthfeGR7PHxxbASEjCM7jQ0hAJ8Ph8HTIttvl # generated
        * Secret Key=ij54gqv7w04habvy6dn2sninbbdgmlicwnsvpfwa1gdn6of2rrxgo211njqaekqlhg1503xdnvo2oc7h3dk7dd3cmk7h8mvnmijikovjw5jnl2w2pnfrukkwx0sq0uj # generated
        * Application Username=openshift
        * Application Password=secret
        * Rails Environment=production
        * Database Service Name=postgresql
        * Database Username=userAFJ # generated
        * Database Password=pn6A2x3B # generated
        * Database Name=root
        * Maximum Database Connections=100
        * Shared Buffer Amount=12MB
        * Custom RubyGems Mirror URL=

--> Creating resources ...
    secret "rails-pgsql-persistent" created
    service "rails-pgsql-persistent" created
    route.route.openshift.io "rails-pgsql-persistent" created
    imagestream.image.openshift.io "rails-pgsql-persistent" created
    buildconfig.build.openshift.io "rails-pgsql-persistent" created
    deploymentconfig.apps.openshift.io "rails-pgsql-persistent" created
    persistentvolumeclaim "postgresql" created
    service "postgresql" created
    deploymentconfig.apps.openshift.io "postgresql" created
--> Success
    Access your application via route 'rails-pgsql-persistent-my-database-app.apps.790442527540.aws.testdrive.openshift.com' 
    Build scheduled, use 'oc logs -f bc/rails-pgsql-persistent' to track its progress.
    Run 'oc status' to view your app.
----

You can now follow the deployment process here by watching the pods.

[source,bash,role="copypaste"]
----
watch oc get pod
----

Hit `Ctrl-C` when both pods (postgresql-1-xxxxx and rails-pqsqsl-persistent-1-xxxxx) show Ready (`1/1`) and Running. This can take a while because first there is a build pod (`rails-pgsql-persistent-1-build`) that is building the container image to be used in the application from Ruby source code.

[NOTE]
====
It may take up to 5 minutes for the deployment to complete.
====

On the CLI, you should now see a PVC that has been issued and has a status of _Bound_. state.

[source,bash,role="copypaste"]
----
oc get pvc
----

You will see something like:

----
NAME         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
postgresql   Bound     pvc-1cbd111b-6b5c-11e9-ad48-0a0e0711ec88   5Gi        RWO            glusterfs-storage   3m
----

[TIP]
====
This PVC has been automatically fulfilled by OCS because the `glusterfs-storage` *StorageClass* was set up as the system-wide default as part of the installation. The responsible parameter in the inventory file was: `openshift_storage_glusterfs_storageclass_default=true`
====

==== Test the Application

Now go ahead and try out the application. Get it the route of the application on the CLI like this:

[source,bash,role="copypaste"]
----
oc get route
----

You will see something like:

----
NAME                     HOST/PORT                                                                              PATH      SERVICES                 PORT      TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-database-app.apps.538432900127.aws.testdrive.openshift.com             rails-pgsql-persistent   <all>                   None
----

Following this output, point your browser to:

*http://<ROUTE HOST>/articles*

The username/password to create articles and comments is by default '_openshift_'/'_secret_'.

You should be able to successfully create articles and comments. When they are saved they are actually saved in the PostgreSQL database which stores its table spaces on a GlusterFS volume provided by OCS.

[NOTE]
====
This application's template included a *Route* object definition, which is
why the *Service* was automatically exposed. This is a good practice. Note
how the actual application is hosted under the */articles* path of the URL.
====

==== Explore the underlying OCS artifacts
Now let's take a look at how this was deployed on the GlusterFS side. First you
need to acquire necessary permissions:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

Select the example project of the user `fancyuser1` if not already/still selected:

[source,bash,role="copypaste"]
----
oc project my-database-app
----

Look at the PVC to determine the PV:

[source,bash,role="copypaste"]
----
oc get pvc
----

You will see the PVC in a `BOUND` state and the name of the PV it has been bound to in the `VOLUME` column:

----
NAME         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
postgresql   Bound     pvc-1cbd111b-6b5c-11e9-ad48-0a0e0711ec88   5Gi        RWO            glusterfs-storage   5m
----

[NOTE]
====
Your PV name will be different as it's dynamically generated. A lot of the
following things contain dynamically generated names.
*Use the supplied bash shortcuts to easy copying and pasting.*
====

Here's a little bash shortcut to store the name of the PVC in a Bash environment variable:

[source,bash,role="copypaste"]
----
export PGSQL_PV_NAME=$(oc get pvc/postgresql -o jsonpath="{.spec.volumeName}" -n my-database-app)
echo $PGSQL_PV_NAME
----

Look at the details of the PV bound to the PVC, in this case `pvc-1cbd111b-6b5c-11e9-ad48-0a0e0711ec88` (your's will be different, use the bash variable):

[source,bash,role="copypaste"]
----
oc describe pv $PGSQL_PV_NAME
----

You will see something like:

----
Name:            pvc-1cbd111b-6b5c-11e9-ad48-0a0e0711ec88 <1>
Labels:          <none>
Annotations:     Description=Gluster-Internal: Dynamically provisioned PV
                 gluster.kubernetes.io/heketi-volume-id=7da624d82941c50d704dd01b366c5806
                 gluster.org/type=file
                 kubernetes.io/createdby=heketi-dynamic-provisioner
                 pv.beta.kubernetes.io/gid=2001
                 pv.kubernetes.io/bound-by-controller=yes
                 pv.kubernetes.io/provisioned-by=kubernetes.io/glusterfs
                 volume.beta.kubernetes.io/mount-options=auto_unmount
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:	   glusterfs-storage
Status:          Bound
Claim:           my-database-app/postgresql
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        5Gi
Node Affinity:   <none>
Message:         
Source:
    Type:           Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)
    EndpointsName:  glusterfs-dynamic-postgresql
    Path:		        vol_fbf686c62a5087e85cbdab5171f31583 <2>
    ReadOnly:       false
Events:             <none>
----
<1> The unique name of this PV in the system OpenShift refers to
<2> The unique volume name backing the PV known to GlusterFS

Note the GlusterFS volume name, in this case
*vol_fbf686c62a5087e85cbdab5171f31583*. The following is another Bash
shortcut to store the name of the GlusterFS volume backing the
`PersistentVolume`:

[source,bash,role="copypaste"]
----
export PGSQL_GLUSTER_VOLUME=$(oc get pv $PGSQL_PV_NAME -o jsonpath='{.spec.glusterfs.path}')
echo $PGSQL_GLUSTER_VOLUME
----

Now let's switch to the namespace we used for OCS deployment:

[source,bash,role="copypaste"]
----
oc project storage
----

Look at the GlusterFS pods running and pick one (which one is not important):

[source,bash,role="copypaste"]
----
oc get pods -o wide -l glusterfs=storage-pod
----

You will see something like:

----
NAME                      READY     STATUS    RESTARTS   AGE       IP          NODE                                          NOMINATED NODE
glusterfs-storage-l5sxd   1/1       Running   0          3h        10.0.1.83   node01.internal.aws.testdrive.openshift.com   <none>
glusterfs-storage-l99db   1/1       Running   0          3h        10.0.4.14   node03.internal.aws.testdrive.openshift.com   <none>
glusterfs-storage-tsr4g   1/1       Running   0          3h        10.0.3.28   node02.internal.aws.testdrive.openshift.com   <none>
----

We are now going to select the first pod (which one doesn't really matter)
and, store it's IP address in above example that is: *10.0.1.83*
of pod *glusterfs-storage-l5sxd*.

Again, for easy copying and pasting, here are some Bash shortcuts:

[source,bash,role="copypaste"]
----
export FIRST_GLUSTER_POD=$(oc get pods -o jsonpath='{.items[0].metadata.name}' -l glusterfs=storage-pod)
export FIRST_GLUSTER_IP=$(oc get pods -o jsonpath='{.items[0].status.podIP}' -l glusterfs=storage-pod)
echo $FIRST_GLUSTER_POD
echo $FIRST_GLUSTER_IP
----

We will again use the `oc rsh` facility to log on to the selected GlusterFS
pod which has the GlusterFS CLI utilities installed. This time we will use
the non-interactive mode which immediately drops out after executing the
supplied command.

Query GlusterFS from inside the first GlusterFS pod for all known volumes:

[source,bash,role="copypaste"]
----
oc rsh $FIRST_GLUSTER_POD gluster volume list
----

You will immediately drop back out to your shell and you will see something like:

----
heketidbstorage <1>
vol_fbf686c62a5087e85cbdab5171f31583 <2>
----
<1> A special volume dedicated to heketi's internal database.
<2> The volume backing the PV of the PostgreSQL database we asked you to remember.

Query GlusterFS about the topology of this volume:

[source,bash,role="copypaste"]
----
oc rsh $FIRST_GLUSTER_POD gluster volume info $PGSQL_GLUSTER_VOLUME
----

You will see something like:

----
Volume Name: vol_fbf686c62a5087e85cbdab5171f31583
Type: Replicate
Volume ID: 5e2a8d04-ef55-4dac-984a-23ba62aaf6d0
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: 10.0.3.28:/var/lib/heketi/mounts/vg_5a46f5d3788ed61352f565385edce8d5/brick_9cb9259e8894dde38a8b4decd9788cd8/brick
Brick2: 10.0.1.83:/var/lib/heketi/mounts/vg_f3668aa3855cd9a84642ca29db45af1c/brick_cb043b2bb44dd6a80bfe826ebdd3c61a/brick <1>
Brick3: 10.0.4.14:/var/lib/heketi/mounts/vg_550bc327799e3c436a2e35e4b584c2ca/brick_5e09b7f1b8c86e8da3364579a4f181da/brick
Options Reconfigured:
server.tcp-user-timeout: 42
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
cluster.brick-multiplex: on
----
<1> According to the output of `oc get pods -o wide` this is the container we are logged on to.

[NOTE]
====
Identify the right brick by looking at the host IP of the GlusterFS pod
you have just logged on to. `oc get pods -o wide` will give you this
information. The host's IP will be noted next to one of the bricks.
====

GlusterFS created this volume as a 3-way replica set across all GlusterFS
pods, and therefore across all your OpenShift App nodes running OCS. Data
written to such a replica volume is replicated 3 times to all *bricks*.
*Bricks* are local storage in GlusterFS nodes, usually backed by a local SAS
*disk or NVMe device. Each node exposes its local storage via the GlusterFS
*protocol. The brick itself is simply a directory on a block device formatted
*with XFS. Hence you can look with a simple `ls` command and see how the data
*is actually stored in each brick.

For easy copying and pasting, here's another bash shortcut to extract the
brick directory path of our PostgreSQL volume from the fist GlusterFS pod in
the list:

[source,bash,role="copypaste"]
----
export PGSQL_GLUSTER_BRICK=$(echo -n $(oc rsh $FIRST_GLUSTER_POD gluster vol info $PGSQL_GLUSTER_VOLUME | grep $FIRST_GLUSTER_IP) | cut -d ':' -f 3 | tr -d $'\r' )
echo $PGSQL_GLUSTER_BRICK
----

You can look at the brick directory of the first GlusterFS pod and see how
GlusterFS stores the files from the clients in a brick:

[source,bash,role="copypaste"]
----
oc rsh $FIRST_GLUSTER_POD ls -ahl $PGSQL_GLUSTER_BRICK
----

You will see something like:

----
total 16K
drwxrwsr-x.   4 root       2000   40 Apr 30 15:25 .
drwxr-xr-x.   3 root       root   19 Apr 30 15:24 ..
drw---S---. 262 root       2000 8.0K Apr 30 15:35 .glusterfs
drwx------.  20 1000080000 2000 8.0K Apr 30 15:25 userdata
----

Dig a bit deeper, try looking at the `userdata` folder:

[source,bash,role="copypaste"]
----
oc rsh $FIRST_GLUSTER_POD ls -ahl $PGSQL_GLUSTER_BRICK/userdata
----

You will see the PostgreSQL database folder structure:

----
total 68K
drwx------. 20 1000080000 2000 8.0K Apr 30 15:25 .
drwxrwsr-x.  4 root       2000   40 Apr 30 15:25 ..
-rw-------.  2 1000080000 root    4 Apr 30 15:25 PG_VERSION
drwx------.  6 1000080000 root   54 Apr 30 15:25 base
drwx------.  2 1000080000 root 8.0K Apr 30 15:26 global
drwx------.  2 1000080000 root   18 Apr 30 15:25 pg_clog
drwx------.  2 1000080000 root    6 Apr 30 15:25 pg_commit_ts
drwx------.  2 1000080000 root    6 Apr 30 15:25 pg_dynshmem
-rw-------.  2 1000080000 root 4.6K Apr 30 15:25 pg_hba.conf
-rw-------.  2 1000080000 root 1.6K Apr 30 15:25 pg_ident.conf
drwx------.  2 1000080000 root   32 Apr 30 15:25 pg_log
drwx------.  4 1000080000 root   39 Apr 30 15:25 pg_logical
drwx------.  4 1000080000 root   36 Apr 30 15:25 pg_multixact
drwx------.  2 1000080000 root   18 Apr 30 15:25 pg_notify
drwx------.  2 1000080000 root    6 Apr 30 15:25 pg_replslot
drwx------.  2 1000080000 root    6 Apr 30 15:25 pg_serial
drwx------.  2 1000080000 root    6 Apr 30 15:25 pg_snapshots
drwx------.  2 1000080000 root    6 Apr 30 15:25 pg_stat
drwx------.  2 1000080000 root   84 Apr 30 16:00 pg_stat_tmp
drwx------.  2 1000080000 root   18 Apr 30 15:25 pg_subtrans
drwx------.  2 1000080000 root    6 Apr 30 15:25 pg_tblspc
drwx------.  2 1000080000 root    6 Apr 30 15:25 pg_twophase
drwx------.  3 1000080000 root   60 Apr 30 15:25 pg_xlog
-rw-------.  2 1000080000 root   88 Apr 30 15:25 postgresql.auto.conf
-rw-------.  2 1000080000 root  22K Apr 30 15:25 postgresql.conf
-rw-------.  2 1000080000 root   46 Apr 30 15:25 postmaster.opts
-rw-------.  2 1000080000 root   89 Apr 30 15:25 postmaster.pid
----

You are looking at the PostgreSQL internal data file structure from the
perspective of the GlusterFS server side. It's a normal local filesystem here.

Clients, like the OpenShift nodes and their application pods talk to this set
of replicated brick storage via the GlusterFS protocol. Which abstracts the
3-way replication behind a single FUSE mount point - this is called a
`volume` in GlusterFS. When a pod starts that mounts storage from a `PV`
backed by GlusterFS, OpenShift will mount the GlusterFS volume on the right
app node and then _bind-mount_ this directory to the right pod. This is
happening transparently to the application inside the pod and looks like a
normal local filesystem.

=== Providing Scalable, Shared Storage With OCS

Historically very few options, like basic NFS support, existed to provide a
*PersistentVolume* to more than one container at a time. The access mode used
for this in OpenShift is `ReadWriteMany`. Traditional block-based storage
solutions are not able to provide *PersistentVolumes* with this access mode.

Also, once provisioned, most storage cannot easily be resized.

With OCS these capabilities are now available to all OpenShift deployments, no
matter where they are deployed. To illustrate the benefit of this, we will
deploy a PHP file uploader application that has multiple front-end instances
sharing a common storage repository.

==== Deploy the File Uploader Application

First log back in as `fancyuser1` using the password `openshift` and create a new project:

[source,bash,role="copypaste"]
----
oc login -u fancyuser1 -p openshift
oc new-project my-shared-storage
----

Next deploy the example PHP application called `file-uploader`:

[source,bash,role="copypaste"]
----
oc new-app openshift/php:7.1~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader
----

You will see something like:

----
--> Found image 691930e (5 weeks old) in image stream "openshift/php" under tag "7.1" for "openshift/php:7.1"

    Apache 2.4 with PHP 7.1 
    ----------------------- 
    PHP 7.1 available as container is a base platform for building and running various PHP 7.1 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php71, rh-php71

    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created
      * The resulting image will be pushed to image stream tag "file-uploader:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "file-uploader"
    * Ports 8080/tcp, 8443/tcp will be load balanced by service "file-uploader"
      * Other containers can access this service through the hostname "file-uploader"

--> Creating resources ...
    imagestream.image.openshift.io "file-uploader" created
    buildconfig.build.openshift.io "file-uploader" created
    deploymentconfig.apps.openshift.io "file-uploader" created
    service "file-uploader" created
--> Success
    Build scheduled, use 'oc logs -f bc/file-uploader' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/file-uploader' 
    Run 'oc status' to view your app.
----

Watch and wait for the application to be deployed:

[source,bash,role="copypaste"]
----
oc logs -f bc/file-uploader
----

You will see something like:

----
Cloning "https://github.com/christianh814/openshift-php-upload-demo" ...
	Commit:	7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)
	Author:	Christian Hernandez <christianh814@users.noreply.github.com>
	Date:	Thu Mar 23 09:59:38 2017 -0700
---> Installing application source

[...]

Pushing image docker-registry.default.svc:5000/my-shared-storage/file-uploader:latest ...
Pushed 2/6 layers, 34% complete
Pushed 3/6 layers, 55% complete
Pushed 4/6 layers, 82% complete
Pushed 5/6 layers, 97% complete
Pushed 6/6 layers, 100% complete
Push successful
----

The command prompt returns out of the tail mode once you see _Push successful_.

[NOTE]
====
This use of the `new-app` command directly asked for application code to be
built and did not involve a template. That's why it only created a *single
Pod* deployment with a *Service* and no *Route*.
====

Let's make our application production ready by exposing it via a `Route` and
scale to 3 instances for high availability:

[source,bash,role="copypaste"]
----
oc expose svc/file-uploader
oc scale --replicas=3 dc/file-uploader
----

Now, check the *Route* that has been created:

[source,bash,role="copypaste"]
----
oc get route
----

You will see something like:

----
NAME            HOST/PORT                                                                       PATH      SERVICES        PORT       TERMINATION   WILDCARD
file-uploader   file-uploader-my-shared-storage.apps.538432900127.aws.testdrive.openshift.com             file-uploader   8080-tcp                 None
----

Point your browser to the web application using the URL advertised by the route
(http://<ROUTE HOST>, e.g. http://file-uploader-my-shared-storage.apps.538432900127.aws.testdrive.openshift.com in the example above).

The web app simply lists all previously uploaded files and offers the ability
to upload new ones as well as download the existing data. Right now there is
nothing.

Select an arbitrary file from your local machine and upload it to the app.

.A simple PHP-based file upload tool
image::./images/uploader_screen_upload.png[]

Once done click *_List uploaded files_* to see the list of all currently
uploaded files.

Do you see it? Don't worry if you don't.

Change back to the command line and look at the running pods.

[source,bash,role="copypaste"]
----
oc get pods -l app=file-uploader
----

You will see 3 pods running:

----
NAME                    READY     STATUS    RESTARTS   AGE
file-uploader-1-2c5cd   1/1       Running   0          4m
file-uploader-1-chjj7   1/1       Running   0          3m
file-uploader-1-fnh27   1/1       Running   0          3m
----

Now let's look back at where this file got stored inside the pods. Again use
the `oc rsh` utility via a scriptlet to execute an `ls` command on the
`upload` directory that the PHP code uses to store the files:

[source,bash,role="copypaste"]
----
for pod in $(oc get pod -l app=file-uploader --no-headers | awk '{print $1}'); do echo $pod; oc rsh $pod ls -hl uploaded; done
----


You will see that only one of the pods has the uploaded file
----
file-uploader-1-2c5cd
total 0
file-uploader-1-chjj7
total 352K
-rw-r--r--. 1 1000380000 root 352K Oct 29 16:00 firefly-episode-list.txt
file-uploader-1-fnh27
total 0
----

Why is that? These pods currently do not use any persistent storage. They
store the file locally in the container root file system. That means the
application cannot effectively be scaled since the pods do not share data and
every client would see different uploaded files. To verify this, try
accessing the URL with a second _Icognito_ browser session.

[CAUTION]
====
Never attempt to store persistent data in a *Pod* that has no persistent
volume associated with it. *Pods* and their containers are ephemeral by
definition, and any stored data will be lost as soon as the *Pod* terminates
for whatever reason.
====

The app is of course not useful like this. We can fix this by providing shared
storage to this app.

You can create a *PersistentVolumeClaim* and attach it into an application with
the `oc set volume` command. Execute the following

[source,bash,role="copypaste"]
----
oc set volume dc/file-uploader --add --name=my-shared-storage \
-t pvc --claim-mode=ReadWriteMany --claim-size=1Gi \
--claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded
----

This command will:

* create a *PersistentVolumeClaim*
* update the *DeploymentConfig* to include a `volume` definition
* update the *DeploymentConfig* to attach a `volumemount` into the specified
  `mount-path`
* cause a new deployment of the application *Pods*

For more information on what `oc set volume` is capable of, look at its help output
with `oc set volume -h`. Now, let's look at the result of adding the volume:

[source,bash,role="copypaste"]
----
oc get pvc
----

You will see something like:

----
NAME                STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
my-shared-storage   Bound     pvc-0e66d9f3-6b62-11e9-ad48-0a0e0711ec88   1Gi        RWX            glusterfs-storage   24s
----

Notice the `ACCESSMODE` being set to *RWX* (short for `ReadWriteMany`,
equivalent to "shared storage"). Without this `ACCESSMODE`, OpenShift will
not attempt to attach multiple *Pods* to the same *PersistentVolume*
reliably. If you attempt to scale up deployments that are using
`ReadWriteOnce` storage, they will actually all become co-located on the same
node.

The app has now re-deployed (in a rolling fashion) with the new settings -
all pods will mount the volume identified by the PVC under
`/opt/app-root/src/upload`.

Check you have a new set of pods:

[source,bash,role="copypaste"]
----
oc get pods -l app=file-uploader
----

You will see something like:

----
NAME                    READY     STATUS    RESTARTS   AGE
file-uploader-2-6dst5   1/1       Running   0          43s
file-uploader-2-8dlcn   1/1       Running   0          36s
file-uploader-2-hmfgr   1/1       Running   0          31s
----

Try it out in your file uploader web application using your browser. Upload
new files and see that they are visible from within all application pods.

[CAUTION]
====
Where is my previously uploaded file?

Since the pod redeployed the file has been lost with the previous container's
root filesystem going away as part of the configuration update. One more
reason to provide persistent storage!
====

Once done, return to the command line and look at the contents of pods:

[source,bash,role="copypaste"]
----
for pod in $(oc get pod -l app=file-uploader --no-headers | awk '{print $1}'); do echo $pod; oc rsh $pod ls -hl uploaded; done
----


You will see that now all of the pods have the uploaded file:
----
file-uploader-2-6dst5
total 352K
-rw-r--r--. 1 1000380000 2002 352K Oct 29 16:10 firefly-episode-list.txt
file-uploader-2-8dlcn
total 352K
-rw-r--r--. 1 1000380000 2002 352K Oct 29 16:10 firefly-episode-list.txt
file-uploader-2-hmfgr
total 352K
-rw-r--r--. 1 1000380000 2002 352K Oct 29 16:10 firefly-episode-list.txt
----

That's it. You have successfully provided shared storage to pods throughout the
entire system, therefore avoiding the need for data to be replicated at the
application level to each pod.

With OCS this is available wherever OpenShift is deployed without external
dependencies like NFS.

=== Increasing volume capacity

However, what happens when the volume is full?

Let's try it. Run the following command to fill up the currently 1GiB of free
space in the persistent volume. Since it's shared, you can use any the 3
file-uploader pods:

[source,bash,role="copypaste"]
----
oc rsh $(oc get pod -l app=file-uploader --no-headers | head -n1 | awk '{print $1}') dd if=/dev/zero of=uploaded/bigfile bs=100M count=1000
----

The result after some time is:
----
dd: error writing 'uploaded/bigfile': No space left on device
dd: closing output file 'uploaded/bigfile': No space left on device
command terminated with exit code 1
----

Oops. The file system seems to have a problem. Let's check it:

[source,bash,role="copypaste"]
----
oc rsh $(oc get pod -l app=file-uploader --no-headers | head -n1 | awk '{print $1}') df -h /opt/app-root/src/uploaded
----

Clearly the file system is full:

----
Filesystem                                      Size  Used Avail Use% Mounted on
10.0.1.83:vol_9829c286608e9ce29b81df24eb08ce51 1019M 1019M     0 100% /opt/app-root/src/uploaded
----

If you were to try uploading another file via the web application it would fail with something along the lines:

----
[...]
failed to open stream: No space left on device in /opt/app-root/src/upload.php on line 26
[...]
----

First the `StorageClass` glusterfs-storage needs to be modified to include `allowVolumeExpansion: true`. To add this new parameter the following process is used.

[WARNING]
====
It is required that the feature-gates: (below) is added to the /etc/origin/master/master-config.yaml and the master services restarted before modifying the `StorageClass` glusterfs-storage. Our environment already has these enabled.

	kubernetesMasterConfig:
	  apiServerArguments:
	    feature-gates:
	    - ExpandPersistentVolumes=true
====

Switch back to the `system:admin` user.

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

Save the current Storage Class to a YAML file:

[source,bash,role="copypaste"]
----
oc get sc glusterfs-storage -o yaml > $HOME/glusterfs-storage.yaml
----

And then add the new parameter to the glusterfs-storage-new.yaml file.

[source,bash,role="copypaste"]
----
sed '/volumeBindingMode: Immediate/a allowVolumeExpansion: true' $HOME/glusterfs-storage.yaml > $HOME/glusterfs-storage-new.yaml
----

Now to modify this `StorageClass` the current glusterfs-storage needs to be deleted and the new glusterfs-storage-new.yaml used to create glusterfs-storage that containes the necessary parameter `allowVolumeExpansion: true`.

[source,bash,role="copypaste"]
----
oc delete sc glusterfs-storage
oc create -f $HOME/glusterfs-storage-new.yaml
----

Now do the following to validate the `StorageClass` is modified.

[source,bash,role="copypaste"]
----
oc get sc glusterfs-storage -o yaml
----

You will see something like below.

----
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  creationTimestamp: 2019-04-22T19:33:05Z
  name: glusterfs-storage

...
----

Also verify using this command:

[source,bash,role="copypaste"]
----
oc describe sc glusterfs-storage
----

You can see `AllowVolumeExpansion:  True` in this output as well.

----
Name:                  glusterfs-storage
IsDefaultClass:        Yes
Annotations:           storageclass.kubernetes.io/is-default-class=true
Provisioner:           kubernetes.io/glusterfs
Parameters:            resturl=http://heketi-storage.storage.svc:8080,restuser=admin,secretName=heketi-storage-admin-secret,secretNamespace=storage
AllowVolumeExpansion:  True
MountOptions:          <none>
ReclaimPolicy:         Delete
VolumeBindingMode:     Immediate
Events:                <none>
----

After the `StorageClass` is modified to allow `PersistentVolume` expansion, the volume size can be increased by the user or owner of the app, even without administrator intervention.

[WARNING]
====
If you are unfamiliar with the `vi` editor, please run the following command before continuing:

    export EDITOR=nano
====

Switch back to the `fancyuser1` OpenShift user.

[source,bash,role="copypaste"]
----
oc login -u fancyuser1
----

Use the `oc edit` command to edit the `PersistentVolumeClaim` that we used to
generate the `PersistentVolume`:

[source,bash,role="copypaste"]
----
oc edit pvc my-shared-storage
----

You end up in a `vi` session editing the `PVC` object properties in YAML. Go
to line that says `storage: 1Gi` below spec -> resources -> requests and
increase to `5Gi` like shown below:

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
    volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs
  creationTimestamp: 2018-04-18T10:17:24Z
  name: my-shared-storage
  namespace: my-shared-storage
  resourceVersion: "41960"
  selfLink: /api/v1/namespaces/my-shared-storage/persistentvolumeclaims/my-shared-storage
  uid: b0544244-42f1-11e8-8f68-02f9630bd644
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi <1>
  storageClassName: glusterfs-storage
  volumeName: pvc-b0544244-42f1-11e8-8f68-02f9630bd644
status:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 1Gi
  phase: Bound
----
<1> Set this to *5Gi*

Exit out of `vi` mode with the `:wq` command.

[TIP]
====
Upon writing the file the `oc edit` command will update the
`PersistentVolumeClaim` definition in OpenShift. This way of ad-hoc editing
works with many objects in OpenShift.
====

Give it a couple of seconds and then check the filesystem again:

[source,bash,role="copypaste"]
----
oc rsh $(oc get pod -l app=file-uploader --no-headers | head -n1 | awk '{print $1}') df -h /opt/app-root/src/uploaded
----

The situation should look much better now:

----
Filesystem                                      Size  Used Avail Use% Mounted on
10.0.1.83:vol_9829c286608e9ce29b81df24eb08ce51  5.0G  1.1G  4.0G  22% /opt/app-root/src/uploaded
----

// WK: Part 3, Scaleup, Prepare for Rook
== Infrastructure Management, Adding Nodes to your Cluster

In this lab you will explore various aspects of managing cluster infrastructure. This includes extending the OpenShift cluster to enable us to install Rook later in this lab.

=== Extending the Cluster

[NOTE]
====
It is required that you `sudo -i` to `root` before performing these exercises.
====

Switch to the `root` user:

[source,bash,role="copypaste"]
----
sudo -i
----

Extending the cluster is easy. Simply add a new set of hosts to an Ansible group called `new_nodes` in the `openshift-ansible` installer's inventory. Then, run the `scaleup` playbook.

==== Configure the Installer

Your environment already has 3 additional VMs provisioned, but you have not used them so far. They are already configured in the inventory file, but commented out with a `#scaleup_` prefix.

To see the lines run:

[source,bash,role="copypaste"]
----
grep '#scaleup_' /etc/ansible/hosts
----

Remove the `#scaleup_` comment prefix by running the below `sed` command:

[source,bash,role="copypaste"]
----
sudo sed -i 's/#scaleup_//g' /etc/ansible/hosts
----

When finished, your inventory file should look like the following:

[source,ini]
./etc/ansible/hosts
----
[OSEv3:children]
masters
nodes
etcd
glusterfs
new_nodes

[...]

[new_nodes]
node04.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node04.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node05.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node05.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node06.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node06.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'

[...]
----

Now that these hosts are properly defined (uncommented), you can use Ansible to
verify that they are, in fact, online:

[source,bash,role="copypaste"]
----
ansible new_nodes -m ping
----

You will see:

----
node04.internal.aws.testdrive.openshift.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node05.internal.aws.testdrive.openshift.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node06.internal.aws.testdrive.openshift.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
----

These new VMs have all of the link:https://docs.openshift.com/container-platform/3.11/install_config/install/prerequisites.html[prerequisites] already taken care of.

==== Run the Playbook to Extend the Cluster

To extend your cluster run the following playbook:

[source,bash,role="copypaste"]
----
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-node/scaleup.yml
----

The playbook takes 1-2 minutes to complete. When done, you can verify that there are now 6 `compute` nodes:

[source,bash,role="copypaste"]
----
oc get nodes -l node-role.kubernetes.io/compute=true
----

You will see:

----
NAME                                          STATUS    ROLES     AGE       VERSION
node01.internal.aws.testdrive.openshift.com   Ready     compute   2h        v1.11.0+d4cacc0
node02.internal.aws.testdrive.openshift.com   Ready     compute   2h        v1.11.0+d4cacc0
node03.internal.aws.testdrive.openshift.com   Ready     compute   2h        v1.11.0+d4cacc0
node04.internal.aws.testdrive.openshift.com   Ready     compute   2m        v1.11.0+d4cacc0
node05.internal.aws.testdrive.openshift.com   Ready     compute   2m        v1.11.0+d4cacc0
node06.internal.aws.testdrive.openshift.com   Ready     compute   2m        v1.11.0+d4cacc0
----

After the scaleup succeeds you need to remove the `new_nodes` entry from [osev3:children]. You also need to remove the '[new_nodes]' section to add the new nodes to the regular [nodes] section of the inventory file. This ensures that any further update will be applied to all nodes, old and new.

Check the two lines that got added to enable the scaleup operation:

[source,bash,role="copypaste"]
----
grep new_nodes /etc/ansible/hosts
----

You will see:

----
new_nodes
[new_nodes]
----

Remove [new_nodes] to add new nodes to the [nodes] section in the inventory file. 

[source,bash,role="copypaste"]
----
sudo sed -i '/^\[new_nodes/d' /etc/ansible/hosts
----

Remove new_nodes from [osev3:children] section of the inventory file.

[source,bash,role="copypaste"]
----
sudo sed -i '/^new_nodes/d' /etc/ansible/hosts
----

Your modified inventory file should now look like this:

[source,ini]
----
[OSEv3:children]
masters
nodes
etcd
glusterfs
#ocsinfra_glusterfs_registry

[...]

[nodes]
master.internal.aws.testdrive.openshift.com openshift_public_hostname=master.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-master'
infra.internal.aws.testdrive.openshift.com openshift_public_hostname=infra.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-infra'
node01.internal.aws.testdrive.openshift.com openshift_public_hostname=node01.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node02.internal.aws.testdrive.openshift.com openshift_public_hostname=node02.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node03.internal.aws.testdrive.openshift.com openshift_public_hostname=node03.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'

node04.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node04.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node05.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node05.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'
node06.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_public_hostname=node06.538432900127.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute'

[...]
----

Finally exit out of your `root` shell by typing `exit` (you should then be `cloud-user` again).

// WK: Part 4: Rook

== Deploying and Managing OpenShift Container Storage with Rook-Ceph Operator

In this section you are learning how to deploy and manage OpenShift Container Storage (OCS). In this lab you will be using OpenShift Container Platform 3.11 (OCP) and Rook.io v0.9 to deploy Ceph as a persistent storage solution for OCP workloads.

*In this lab you will learn how to*

* Configure and deploy containerized Ceph using Rooks cluster CustomResourceDefinitions (CRD)
* Validate deployment of Ceph Luminous containerized using OpenShift CLI
* Deploy the Rook toolbox to run common ceph and rados commands
* Create a Persistent Volume (PV) on the Ceph cluster using a Rook OCP storageclass for deployment of Rails application using a PostgreSQL database.
* Upgrade Ceph version from Luminous to Mimic using the Rook operator
* Add more storage to the Ceph cluster

=== Deploy Ceph using Rook.io

==== Download Rook deployment files and install Ceph

In this section necessary files will be downloaded using the `curl -O` command and OCP resources created using the `oc create` command and the Rook.io yaml files.

Make sure you are logged in as `system:admin`:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

Labeling the new OCP nodes with role=storage-node will make sure that the OCP resources (OSD, MON, MGR pods) are scheduled on these nodes.

[source,bash,role="copypaste"]
----
oc label node node04.internal.aws.testdrive.openshift.com role=storage-node
oc label node node05.internal.aws.testdrive.openshift.com role=storage-node
oc label node node06.internal.aws.testdrive.openshift.com role=storage-node
oc get nodes --show-labels | grep storage-node
----

Next you will download Rook.io scc.yaml, operator.yaml and cluster.yaml to create OCP resources. After downloading each on view the file using the `cat` command before creating the resources using `oc create`.

[source,bash,role="copypaste"]
----
cd $HOME
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/scc.yaml
oc create -f scc.yaml
----

Validate that rook-ceph has been added to securitycontextconstraints.security.openshift.io.

[source,bash,role="copypaste"]
----
oc get scc rook-ceph
----

Install the Rook operator next.

[source,bash,role="copypaste"]
----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/operator.yaml
oc create -f $HOME/operator.yaml
oc project rook-ceph-system
watch oc get pods -o wide
----

Wait for all rook-ceph-agent, rook-discover and rook-ceph-operator pods to be in a Running state.

The pod list should look like this:

[source,text]
----
NAME                                  READY     STATUS    RESTARTS   AGE       IP           NODE                                          NOMINATED NODE
rook-ceph-agent-48ckp                 1/1	Running   0          5m        10.0.3.28    node02.internal.aws.testdrive.openshift.com   <none>
rook-ceph-agent-4wsd8                 1/1	Running   0          5m        10.0.1.216   node04.internal.aws.testdrive.openshift.com   <none>
rook-ceph-agent-d69pp                 1/1	Running   0          5m        10.0.4.14    node03.internal.aws.testdrive.openshift.com   <none>
rook-ceph-agent-h8ds6                 1/1	Running   0          5m        10.0.4.41    node06.internal.aws.testdrive.openshift.com   <none>
rook-ceph-agent-nmsp6                 1/1	Running   0          5m        10.0.3.144   node05.internal.aws.testdrive.openshift.com   <none>
rook-ceph-agent-wjhkv                 1/1	Running   0          5m        10.0.1.83    node01.internal.aws.testdrive.openshift.com   <none>
rook-ceph-operator-76c97f94c4-gt7ld   1/1	Running   0          6m        10.130.2.4   node06.internal.aws.testdrive.openshift.com   <none>
rook-discover-4lh4w                   1/1	Running   0          5m        10.129.0.4   node03.internal.aws.testdrive.openshift.com   <none>
rook-discover-8zb6r                   1/1	Running   0          5m        10.130.0.4   node02.internal.aws.testdrive.openshift.com   <none>
rook-discover-fdt9b                   1/1	Running   0          5m        10.131.2.4   node04.internal.aws.testdrive.openshift.com   <none>
rook-discover-fm659                   1/1	Running   0          5m        10.129.2.7   node05.internal.aws.testdrive.openshift.com   <none>
rook-discover-m7xxx                   1/1	Running   0          5m        10.131.0.4   node01.internal.aws.testdrive.openshift.com   <none>
rook-discover-x4dlh                   1/1	Running   0          5m        10.130.2.5   node06.internal.aws.testdrive.openshift.com   <none>
----

The log for the rook-ceph-operator pod should show that the operator is looking for a cluster. Look for `the server could not find the requested resource (get clusters.ceph.rook.io)` at the end of the log file. Replace `xxxxxxxxx-xxxxx` below with your rook-ceph-operator pod name.

[source,bash,role="copypaste"]
----
oc get pod -l app=rook-ceph-operator
oc logs rook-ceph-operator-xxxxxxxxx-xxxxx
----

Next step is to download and install the cluster CRD to create Ceph MON, MGR and OSD pods.

[source,bash,role="copypaste"]
----
oc new-project rook-ceph
oc adm pod-network make-projects-global rook-ceph
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/cluster.yaml
----

Take a look at the cluster.yaml file. It specifies the version of Ceph, the label used for the rook resources (role=storage-node) added at the start of this section, and the nodes and storage devices used for OSDs.

----
cat cluster.yaml
...omitted...
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
...omitted...
    image: ceph/ceph:v12.2.11-20190201
...omitted...
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    nodes:
    # Each node's 'name' field should match their 'kubernetes.io/hostname' label.
    - name: "node04.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
    - name: "node05.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
    - name: "node06.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
----

Now create the MONs, MGR and OSD pods.

[source,bash,role="copypaste"]
----
oc create -f cluster.yaml
----

Disregard this message Error from server (AlreadyExists): error when creating "cluster.yaml": namespaces "rook-ceph" already exists

Switch to the project `rook-ceph` and watch the pods come up (press `Ctrl-C` when your pod list looks like the one below).

[source,bash,role="copypaste"]
----
oc project rook-ceph
watch oc get pods

NAME                                                           READY     STATUS      RESTARTS   AGE
rook-ceph-mgr-a-5887d4d48b-gm8mg                               1/1       Running     0          49s
rook-ceph-mon-a-5c7587f7c7-d6t5d                               1/1       Running     0          2m
rook-ceph-mon-b-d85c69845-hzv78                                1/1       Running     0          1m
rook-ceph-mon-c-8567bb8597-g48m7                               1/1       Running     0          1m
rook-ceph-osd-0-d576d5989-9zr78                                1/1	 Running     0          17s
rook-ceph-osd-1-6b9f5d9b78-mgswg                               1/1	 Running     0          16s
rook-ceph-osd-2-67659f7dc8-74k6j                               1/1	 Running     0          12s
rook-ceph-osd-prepare-89f1a63764fbcfe0f15eca7b510a7763-766xt   0/2	 Completed   0          40s
rook-ceph-osd-prepare-b9e4065b399354d3fb0f17127c7d01c7-knvd5   0/2	 Completed   0          41s
rook-ceph-osd-prepare-f4a3099a5aac291ccda3759e92f81c57-zjv2c   0/2	 Completed   0          39s
----

Once all pods are in a Running state it is time to verify that Ceph is operating correctly. Download toolbox.yaml to run Ceph commands.

[source,bash,role="copypaste"]
----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/toolbox.yaml
oc create -f toolbox.yaml -n rook-ceph
----

Login to toolbox pod to run Ceph commands.

[source,bash,role="copypaste"]
----
oc -n rook-ceph exec -it $(oc -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash

ceph status
ceph osd status
ceph osd tree
ceph df
rados df
exit
----

Disregard the health: HEALTH_WARN mons a,b,c are low on available space message when viewing results of `ceph status` command.

=== Create Rook storageclass for creating CephRBD block volumes

In this section you will download storageclass.yaml and then create the OCP storageclass `rook-ceph-block` that will be used by applications to dynamically claim persistent storage (PVCs). The Ceph pool `replicapool` is created when the storageclass is created.

[source,bash,role="copypaste"]
----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/storageclass.yaml
cat  storageclass.yaml
----

Notice the provisioner: ceph.rook.io/block and that replicated: size=2.

[source,bash,role="copypaste"]
----
oc create -f storageclass.yaml
----

Login to toolbox pod to run Ceph commands. Compare results for `ceph df` and `rados df` executed in prior section before the storageclass was created.

[source,bash,role="copypaste"]
----
oc -n rook-ceph exec -it $(oc -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash

ceph df
rados df
rados -p replicapool ls
exit
----

=== Create new OpenShift Application using CephRBD block volume

In this section the `rook-ceph-block` storageclass will be used by an application + database deployment to create persistent storage. The persistent storage will be a CephRBD volume (object) in the pool=replicapool.

Because the Rails + PostgreSQL deployment uses the `default` storageclass we need to modify the current default storageclass (glusterfs-storage) and then make `rook-ceph-block` the default storageclass.

[source,bash,role="copypaste"]
----
oc get storageclass
----

Make the glusterfs-storage class non-default and the rook-ceph-block storage class the new default:

[source,bash,role="copypaste"]
----
oc patch storageclass glusterfs-storage --patch '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}'

oc patch storageclass rook-ceph-block --patch '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
----

Double check that the rook-ceph-block storage class is now the default:

[source,bash,role="copypaste"]
----
oc get storageclass
----

.Sample Output
[source,texinfo]
----
NAME                        PROVISIONER               AGE
glusterfs-storage           kubernetes.io/glusterfs   38m
rook-ceph-block (default)   ceph.rook.io/block        3m
----

Now you are ready to start the Rails + PostgreSQL deployment.

[source,bash,role="copypaste"]
----
oc new-project ceph-database-app
oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi
oc status
oc get pvc
watch oc get pods
----

Wait until the build is finished and all pods are all in a Completed or Running state. This could take 5 minutes (Press `Ctrl-C` to exit).

----
NAME                             READY     STATUS      RESTARTS   AGE
postgresql-1-gsdc2               1/1       Running     0          2m
rails-pgsql-persistent-1-build   0/1       Completed   0          2m
rails-pgsql-persistent-1-z6j2s   1/1       Running     0          28s
----

Once the deployment is complete you can now test the application and the persistent storage CephRBD volume.

[source,bash,role="copypaste"]
----
oc get route
----

.Sample Output
[source,texinfo]
----
NAME                     HOST/PORT                                                                                PATH      SERVICES                 PORT      TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-ceph-database-app.apps.538432900127.aws.testdrive.openshift.com             rails-pgsql-persistent   <all>                   None
----

Results of this command will be similar to above. Replace `xxxxxxxxxxx` with your unique value and copy the URL to your browser to create articles.

----
http://rails-pgsql-persistent-my-database-app.apps.xxxxxxxxxxx.aws.testdrive.openshift.com/articles
----

Enter the username/password to create articles and comments. The articles and comments are saved in a PostgreSQL database which stores its table spaces on a CephRBD volume provided by OCS.

----
username: openshift
password: secret
----

Lets now take another look at the replicapool created by the OCP storageclass. Log into the toolbox pod again.

[source,bash,role="copypaste"]
----
oc -n rook-ceph exec -it $(oc -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
----

Run the same Ceph commands as before the application deployment and compare to results in prior section. Notice the number of objects in replicapool now.

[source,bash,role="copypaste"]
----
ceph df
rados df
rados -p replicapool ls | grep pvc
exit
----

Validate the OCP PVC is the same name as the PVC object in the replicapool.

[source,bash,role="copypaste"]
----
oc get pvc
----

=== Using Rook to Upgrade Ceph

In this section you will upgrade Ceph from from Luminous to Mimic using the Rook operator. The first thing we need to do is update the cluster CRD with the mimic image name and version.

[source,bash,role="copypaste"]
----
oc project rook-ceph
oc get cephcluster rook-ceph -o yaml | grep image

   image: ceph/ceph:v12.2.11-20190201
----

Modify the Ceph version in the cluster CR. Update the version using the following command:

[source,bash,role="copypaste"]
----
oc patch cephcluster rook-ceph -n rook-ceph --type merge --patch '{"spec": { "cephVersion": {"image": "ceph/ceph:v13.2.4-20190109"}}}'
----

Once the change to the ceph version is saved as shown above, the MONs, MGR, and OSD pods will be restarted. This could take 5 minutes (Press `Ctrl-C` to exit once all pods have been restarted and are running).

----
watch oc get pods

NAME                                                           READY     STATUS      RESTARTS   AGE
rook-ceph-mgr-a-7448c76545-wnhjd                               1/1	 Running     0          1m
rook-ceph-mon-a-65d8999987-w6t6v                               1/1	 Running     1	    3m
rook-ceph-mon-b-b886cb46d-9hcbf                                1/1	 Running     0  	  3m
rook-ceph-mon-c-8654c8d995-hlhjv                               1/1	 Running     0  	  2m
rook-ceph-osd-0-86d76c7f5-6k6z8                                1/1	 Running     0          1m
rook-ceph-osd-1-76b46d75b4-n5sgq                               1/1	 Running     0          50s
rook-ceph-osd-2-966fc6d6-wq672                                 1/1	 Running     0          1m
rook-ceph-osd-prepare-89f1a63764fbcfe0f15eca7b510a7763-cxt2x   0/2	 Completed   0          1m
rook-ceph-osd-prepare-b9e4065b399354d3fb0f17127c7d01c7-vh58j   0/2	 Completed   0          1m
rook-ceph-osd-prepare-f4a3099a5aac291ccda3759e92f81c57-dfjh5   0/2	 Completed   0          1m
rook-ceph-tools-76bf8448f6-2h9d4                               1/1	 Running     0          20m
----

Now let's check the version of Ceph to see if it is upgraded. First we need to login to the toolbox pod.

[source,bash,role="copypaste"]
----
oc -n rook-ceph exec -it $(oc -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
----

Running the `ceph versions` command shows each of the Ceph daemons have been upgraded to Mimic. Run other Ceph commands to satisfy yourself (e.g., ceph status) the system is healthy after the upgrade. You might even want to go back to the URL used for the Rails+PostgreSQL application and save a few more articles to make sure applications using Ceph storage are still working.

----
ceph versions
{
    "mon": {
        "ceph version 13.2.4 (b10be4d44915a4d78a8e06aa31919e74927b142e) mimic (stable)": 3
    },
    "mgr": {
        "ceph version 13.2.4 (b10be4d44915a4d78a8e06aa31919e74927b142e) mimic (stable)": 1
    },
    "osd": {
        "ceph version 13.2.4 (b10be4d44915a4d78a8e06aa31919e74927b142e) mimic (stable)": 3
    },
    "mds": {},
    "overall": {
        "ceph version 13.2.4 (b10be4d44915a4d78a8e06aa31919e74927b142e) mimic (stable)": 7
    }
}

exit
----

=== Adding storage to the Ceph Cluster

In this section you will add more storage to the cluster by increasing the number of OSDs per OCP nodes using spare storage devices on the nodes.

Before we make any changes to the cluster CRD let's see what storage is available on our OCP nodes. It is important that the available storage be a raw block device with no formatting or labeling. There should be a storage device available, all of the same size, on the same nodes that were originally used.

[source,bash,role="copypaste"]
----
oc get nodes -l role=storage-node
NAME                                          STATUS    ROLES     AGE       VERSION
node04.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
node05.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
node06.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
----

To check the storage SSH to one of the OCP nodes that have the role=storage-node.

[source,bash,role="copypaste"]
----
ssh node04.internal.aws.testdrive.openshift.com
----

Check the storage devices on node. You can see that 50GB storage device `xvdd` is used already by Ceph. Storage device `xvde`, also 50GB, is not used yet.

[source,bash,role="copypaste"]
----
[cloud-user@node04 ~]$ lsblk
----

.Sample Output
[source,texinfo]
----
NAME                                                                    MAJ:MIN RM SIZE RO TYPE
...omitted...
xvdd                                                                    202:48   0  50G  0 disk
ceph--dbcea47d--6fa4--467e--ad5e--158d0032978f-osd--data--a2a40ce7--b366--48c4--a2d6--2aac94def755
                                                                        253:1    0  50G  0 lvm
xvde                                                                    202:64   0  50G  0 disk
----

Also /dev/xvde looks to be a raw block device with no labels, which is required.

[source,bash,role="copypaste"]
----
[cloud-user@node04 ~]$ sudo fdisk -l /dev/xvde
----

.Sample Output
[source,texinfo]
----
Disk /dev/xvde: 53.7 GB, 53687091200 bytes, 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
----

Exit from Node04 back to your Master

[source,bash,role="copypaste"]
----
[cloud-user@node04 ~]$ exit
----

After validating the available storage for increasing the number of OSDs we are ready to modify the cluster CRD and add an additional storage device, `xvde`.

To make this easier we have created a new cluster CRD yaml file that has the new storage device already added correctly instead of editing the cluster CRD using `oc edit`.

[source,bash,role="copypaste"]
----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/cluster_with_xvde.yaml
----

Take a look at the new cluster CRD yaml file.

----
cat cluster_with_xvde.yaml
...omitted...
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
    - name: "node04.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
      - name: "xvde"
    - name: "node05.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
      - name: "xvde"
    - name: "node06.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
      - name: "xvde"
----

Now add the additional storage device `xvde` to each node above.

[source,bash,role="copypaste"]
----
oc apply -f cluster_with_xvde.yaml
----

Once this new defiition is applied the 3 additonal *rook-ceph-osd* pods will start. Wait until they are in a Running state before proceeding. (again press `Ctrl-C` to exit).

[source,bash,role="copypaste"]
----
watch oc get pods
----

.Sample Output
[source,texinfo]
----
NAME                                                           READY     STATUS      RESTARTS   AGE
rook-ceph-mgr-a-7448c76545-wnhjd                               1/1	 Running     0          5m
rook-ceph-mon-a-65d8999987-w6t6v                               1/1	 Running     1          7m
rook-ceph-mon-b-b886cb46d-9hcbf                                1/1	 Running     0          6m
rook-ceph-mon-c-8654c8d995-hlhjv                               1/1	 Running     0          6m
rook-ceph-osd-0-86d76c7f5-6k6z8                                1/1	 Running     0          5m
rook-ceph-osd-1-76b46d75b4-n5sgq                               1/1	 Running     0          4m
rook-ceph-osd-2-966fc6d6-wq672                                 1/1	 Running     0          4m
rook-ceph-osd-3-546bb75744-zzczg                               1/1	 Running     0          22s
rook-ceph-osd-4-6d47648d4d-xmhvb                               1/1	 Running     0          18s
rook-ceph-osd-5-5fd8464cb8-hlnsh                               1/1	 Running     0          15s
rook-ceph-osd-prepare-89f1a63764fbcfe0f15eca7b510a7763-sps7t   0/2	 Completed   0          46s
rook-ceph-osd-prepare-b9e4065b399354d3fb0f17127c7d01c7-9bk6b   0/2	 Completed   0          48s
rook-ceph-osd-prepare-f4a3099a5aac291ccda3759e92f81c57-mgbq6   0/2	 Completed   0          44s
rook-ceph-tools-76bf8448f6-2h9d4                               1/1	 Running     0          23m
----

Let's now validate that Ceph is healthy and has the additional storage. We again login to the toolbox.

[source,bash,role="copypaste"]
----
oc -n rook-ceph exec -it $(oc -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
----

And run Ceph commands to see the new OSDs.

[source,bash,role="copypaste"]
----
ceph osd status
----

.Sample Output
[source,texinfo]
----
+----+---------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| id |                     host                    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
+----+---------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
| 0  | node04.internal.aws.testdrive.openshift.com | 1039M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 1  | node06.internal.aws.testdrive.openshift.com | 1036M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 2  | node05.internal.aws.testdrive.openshift.com | 1059M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 3  | node04.internal.aws.testdrive.openshift.com | 1049M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 4  | node05.internal.aws.testdrive.openshift.com | 1036M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 5  | node06.internal.aws.testdrive.openshift.com | 1045M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
+----+---------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
----

[source,bash,role="copypaste"]
----
ceph osd tree
----

.Sample Output
[source,texinfo]
----
ID CLASS WEIGHT  TYPE NAME                                            STATUS REWEIGHT PRI-AFF
-1       0.29279 root default
-3       0.09760     host node04-internal-aws-testdrive-openshift-com
 0   ssd 0.04880         osd.0                                            up  1.00000 1.00000
 3   ssd 0.04880         osd.3                                            up  1.00000 1.00000
-7       0.09760     host node05-internal-aws-testdrive-openshift-com
 2   ssd 0.04880         osd.2                                            up  1.00000 1.00000
 4   ssd 0.04880         osd.4                                            up  1.00000 1.00000
-5       0.09760     host node06-internal-aws-testdrive-openshift-com
 1   ssd 0.04880         osd.1                                            up  1.00000 1.00000
 5   ssd 0.04880         osd.5                                            up  1.00000 1.00000
----

[source,bash,role="copypaste"]
----
ceph status
----

.Sample Output
[source,texinfo]
----
...omitted...
   osd: 6 osds: 6 up, 6 in
...omitted
----

= The End

*Congratulations!* You reached the end of this Red Hat Summit 2019 Workshop!
