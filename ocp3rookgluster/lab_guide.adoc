= "Got OpenShift Storage" Workshop Guide

// Start OCP3+OCS3 lab with custom lab guide (30 mins)
// Lab Environment 
// Verification including Prometheus + Heketi 
// Create new users via LDAP that have cluster-reader for console login
// Investigate gluster and install rails+postgresql 
// Look at gluster volume usage (PVC) available in Prometheus (kublet_volume)
// Start OCP3+OCS4 lab (40 mins)
// Use everything in readme.adoc except deploy rails+postgresql (should readme.adoc section2 be repeated)

// https://github.com/openshift/openshift-cns-testdrive/tree/master/labguide
// https://github.com/travisn/rook/tree/openshift-commons-demo/workshop

:numbered:
== OpenShift Container Platform Environment Overview

You will be interacting with an OpenShift 3.11 cluster that is running on Amazon Web Services. During the lab you will also install OpenShift Container Storage 3.11 and Rook. The complete environment consists of the following systems:

* 1 master node
* 1 infrastructure node
* 6 worker nodes
** 3 will run workload and the initial Container Native Storage instances
** 3 will be added to the cluster later
// * 1 server running Red Hat Identity Management (IdM, for LDAP authentication)

.Lab Environment Overview
[options="header"]
|==============================================
| Role | Internal FQDN
| Master Node | master.internal.aws.testdrive.openshift.com
| Infrastructure Node | infra.internal.aws.testdrive.openshift.com
| Application Node #1 | node01.internal.aws.testdrive.openshift.com
| Application Node #2 | node02.internal.aws.testdrive.openshift.com
| Application Node #3 | node03.internal.aws.testdrive.openshift.com
| Application Node #4 | node04.internal.aws.testdrive.openshift.com
| Application Node #5 | node05.internal.aws.testdrive.openshift.com
| Application Node #6 | node06.internal.aws.testdrive.openshift.com
// | IdM Server | idm.internal.aws.testdrive.openshift.com
|==============================================

All addresses are internal to the lab environment. The only system you
publicly access via SSH and the browser is the OpenShift Master node:

.Public Lab Access
[options="header"]
|==============================================
| Role | Public FQDN
| Master Node | master.external.aws.testdrive.openshift.com
// WKTBD: How do we get the >actual< Master URL?
|==============================================

You will be installing OpenShift Container Platform v3.11 using the advanced
installation method, which involves executing various Ansible playbooks. You
will also install Container Native Storage v3.11.

Note that references to product documentation will be specifically pointing
to the 3.11 versions, but newer software and documentation versions may be
available.

=== Conventions

You will see various code and command blocks throughout these exercises. Some of
the command blocks can be copy/pasted directly. Others will require modification
of the command before execution. If you see a command block with a red border
(see below), the command will require slight modification.

[source,none,role="copypaste copypaste-warning"]
----
some command to modify
----

Most command blocks support auto highlighting with a click. If you hover over the command block above and click, it should automatically highlight all the text to make for easier copying.

=== Logging in

Most of the exercises in this lab will be facilitated using the OpenShift command line client on the master node. For convenient access to the master's command line we provide a web-based SSH console: http://ssh.external.aws.testdrive.openshift.com:8080/ssh/host/master.internal.aws.testdrive.openshift.com
// WKTBD: What is the actual SSH Console URL?

Use the user name `cloud-user` and the password `{{ KEYNAME }}` when prompted.
// WKTBD: What is the password??

.SSH Console Login
image::webssh_login.png[]

If you prefer to use an SSH client on your system you can do that too, using the same credentials:

[source,bash,role="copypaste"]
----
ssh -l cloud-user master.external.aws.testdrive.openshift.com
----
// WKTBD: What is the actual master URL?

Once you are logged in you end up on the OpenShift Master Node:

----
[cloud-user@master ~]$
----

The `cloud-user` account has password-less sudo privileges and SSH login on
all systems using internal addressing from the table above.

// WK: Part 2 Installation / Verification

## Installation and Verification

The primary method of installing OpenShift Container Platform is based on
Ansible playbooks. These playbooks ship as part of the product in the
`openshift-ansible` package.

This method has, in the past, been referred to as the `advanced installation
method` and it involves Ansible directly running the installation playbooks.
The advanced installer supports many configuration and customization options.
It also covers installation of supporting infrastructure like
OpenShift Container storage, logging and metrics components.

Your environment comes with a preinstalled cluster that has been deployed
using the installer's configuration file (`/etc/ansible/hosts`) when you
started the lab.

For more information on installing OpenShift Container Platform, please refer to
the
link:https://docs.openshift.com/container-platform/3.11/install/index.html[installation
section] of the product documentation.

[NOTE]
====
At this point you should be logged in as `cloud-user` on the OpenShift Master
node via SSH.
====

### Examining the provided Ansible inventory file
First, let's examine the provided installer configuration file.

#### Look at the file
Use `cat`, `less`, or an editor to look at the `/etc/ansible/hosts` file:

[source,bash,role="copypaste"]
----
cat /etc/ansible/hosts
----

General settings and other variable information is defined on lines within the
`[OSEv3:vars]` section. There are also various host groups defined for things
like *Masters* and *Nodes*.

For more details on how OpenShift uses Ansible for its installation, please
refer to the
link:https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html[Configuring Your Inventory File].

The top-level playbook in
`/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml` triggers
the installation of the cluster and all of it's components. It's idempotent,
which means you can execute this playbook multiple times without harm. This
also allows you to deploy additional components after the initial install by
simply modifying the configuration in `/etc/ansible/hosts` and re-run the
installer.

In addition to this, there are playbooks that only deploy a specific
component or service, which makes them faster to execute.

[TIP]
====
A typical multi-host installation like this might normally take around an
hour depending on the speed of your internet connection. Disconnected
installation options are also available. Prerequisites and other information
is all covered in the documentation.
====

### Verifying the Installation
Let's do some basic tests with your installation. As an administrator, most
of your interaction with OpenShift will be from the command line. The `oc`
program is a command line interface that talks to the OpenShift API.

During the OpenShift installation, the `root` system account on the `master`
host is configured to use a special OpenShift "super administrator" account.
Because of this, it is vitally important that you protect access to the
`root` system account, or remove this preconfigured config. Otherwise, anyone
who can `sudo` on the master has super user privileges on the entire cluster.

#### Login on the master
Additionally, your Linux system account on the master, `cloud-user`, is
preconfigured to access this OpenShift "super administrator" without a
password. Type the following command to login as the internal super-user on
OpenShift:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

You will see that you got logged in to a project called 'default'. More on
projects later.

----
Logged into "https://master.internal.aws.testdrive.openshift.com:443" as "system:admin" using existing credentials.

You have access to the following projects and can switch between them with 'oc project <projectname>':

  * default
    kube-public
    kube-system
    management-infra
    openshift
    openshift-console
    openshift-infra
    openshift-logging
    openshift-metrics
    openshift-monitoring
    openshift-node
    openshift-sdn
    openshift-web-console
    storage

Using project "default".
----

#### Look at the Nodes
Execute the following command to see a list of the *Nodes* that OpenShift knows
about:

[source,bash,role="copypaste"]
----
oc get nodes
----

The output should look something like the following:

----
NAME                                          STATUS    ROLES     AGE	VERSION
infra.internal.aws.testdrive.openshift.com    Ready     infra     1m	v1.11.0+d4cacc0
master.internal.aws.testdrive.openshift.com   Ready     master    1m	v1.11.0+d4cacc0
node01.internal.aws.testdrive.openshift.com   Ready     compute   1m	v1.11.0+d4cacc0
node02.internal.aws.testdrive.openshift.com   Ready     compute   1m	v1.11.0+d4cacc0
node03.internal.aws.testdrive.openshift.com   Ready     compute   1m	v1.11.0+d4cacc0
----

All of the systems listed in the `[nodes]` group in the `/etc/ansible/hosts`
file should be listed here. 1 Infrastructure Node, 1 Master and 3 Worker nodes.

The OpenShift *Master* is also a *Node* because it needs to participate in the
software defined network (SDN).
The *Infra* node will only run workloads related to supporting OpenShift infrastructure.

=== Verify the Storage cluster

In your environment Red Hat OpenShift Container Storage was installed as part of
OpenShift. It will serve robust and persistent storage to both business
applications as well as OpenShift infrastructure. It is based on Red Hat
Gluster Storage, running in containers on OpenShift nodes and an additional
API server called `heketi` that enables the API integration with OpenShift.

We will now use a command line client on the *master* to talk via this server
to the container storage cluster. It's password protected, so let's export a
couple of environment variables first to configure the client:

[source,bash,role="copypaste"]
----
export HEKETI_CLI_SERVER=http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY={{HEKETI_ADMIN_PW}}
----
//WKTBD: What are the values for the above?

Then use the CLI tool `heketi-cli` to query `heketi` about all the storage clusters it knows about:

[source,bash,role="copypaste"]
----
heketi-cli cluster list
----

`heketi` will list all known clusters with internal UUIDs:

----
Clusters:
ec7a9c8be8327a54839236791bf7ba24 [file][block]<1>
----
<1> This is the internal UUID of the OCS cluster

[NOTE]
====
The cluster UUID will be different for you since it's automatically generated.
====

To get more detailed information about the topology of your OCS cluster (i.e.
nodes, devices and volumes heketi has discovered) run the following command
(output abbreviated):

[source,bash,role="copypaste"]
----
heketi-cli topology info
----

You will get a lengthy output that describes the GlusterFS cluster topology as it is known by `heketi`:

//WKTBD: Replace Variables below
----
Cluster Id: ec7a9c8be8327a54839236791bf7ba24

    File:  true
    Block: true

    Volumes

        Name: heketidbstorage <1>
        Size: 2
        Id: 272c8d37828c62c4002a19027abd2feb
        Cluster Id: ec7a9c8be8327a54839236791bf7ba24
        Mount: {{NODE1_INTERNAL_IP}}:heketidbstorage
        Mount Options: backup-volfile-servers={{NODE2_INTERNAL_IP}},{{NODE2_INTERNAL_IP}}
        Durability Type: replicate
        Replica: 3
        Snapshot: Disabled

    Nodes:

	Node Id: 099b016da11a623bef37de9b85aaa2d7
	State: online
	Cluster Id: ec7a9c8be8327a54839236791bf7ba24
	Zone: 3
	Management Hostname: {{NODE3_INTERNAL_FQDN}}
	Storage Hostname: {{NODE3_INTERNAL_FQDN}}
	Devices:
		Id:e64fac664861c14bd75e3116f805b8fc   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):0       Free (GiB):49
			Bricks:
                            [...]

	Node Id: 43336d05323e6003be6740dbb7477bd6
	State: online
	Cluster Id: ec7a9c8be8327a54839236791bf7ba24
	Zone: 1
	Management Hostname: {{NODE1_INTERNAL_FQDN}}
	Storage Hostname: {{NODE1_INTERNAL_IP}}
	Devices:
		Id:11a148d8065f6a6220f89c2912d00d13   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):0       Free (GiB):49
			Bricks:
                            [...]

	Node Id: 6c738028f642e37b2368eca88f8c626c
	State: online
	Cluster Id: ec7a9c8be8327a54839236791bf7ba24
	Zone: 2
	Management Hostname: {{NODE2_INTERNAL_FQDN}}
	Storage Hostname: {{NODE2_INTERNAL_IP}}
	Devices:
		Id:cf7c0dfb258f07be25ac9cd4c4d2e6ae   Name:/dev/xvdd           State:online    Size (GiB):49      Used (GiB):0       Free (GiB):49
			Bricks:
                            [...]
----
<1> An internal GlusterFS volume that is automatically generated by the setup routine to hold the heketi database.

This output tells you that Red Hat OpenShift Container Storage currently
consists of a single cluster, which consists of 3 nodes, each with a single
block device `/dev/xvdd` of 50GiB in size. The GlusterFS layer will turn
these 3 devices/hosts into a single, flat storage pool from which OpenShift
will be able to carve out either distinct filesystem volumes or block devices
that serve as persistent storage for containers.

// WK: Part 2, Scaleup

== Infrastructure Management, Adding Nodes to your Cluster

In this lab you will explore various aspects of managing cluster infrastructure.
This includes extending the OpenShift cluster and installation of the
Logging and Metrics components, all automated by the installer. It also includes
some maintenance of nodes, as well as manipulating the multitenant network.

[NOTE]
====
It is required that you `sudo -i` to `root` before performing these exercises.
====

=== Extending the Cluster

Extending the cluster is easy. Simply add a new set of hosts to an Ansible group
called `new_nodes` in the `openshift-ansible` installer's inventory. Then, run
the `scaleup` playbook.

==== Configure the Installer

Your environment already has 3 additional nodes provisioned, but you have not used
them so far. They are already configured in the inventory file, but commented out with a `#scaleup_` prefix.

To see the lines run:

[source,bash,role="copypaste"]
----
grep '#scaleup_' /etc/ansible/hosts
----

Remove the `#scaleup_` comment prefix by running the below `sed` command:

[source,bash,role="copypaste"]
----
sudo sed -i 's/#scaleup_//g' /etc/ansible/hosts
----

When finished, your inventory file should look like the following:

[source,ini]
./etc/ansible/hosts
----
[OSEv3:children]
masters
nodes
etcd
glusterfs
new_nodes

...

[new_nodes]
node04.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_hostname=node04.internal.aws.testdrive.openshift.com openshift_public_hostname=node04.external.aws.testdrive.openshift.com
node05.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_hostname=node05.internal.aws.testdrive.openshift.com openshift_public_hostname=node05.external.aws.testdrive.openshift.com
node06.internal.aws.testdrive.openshift.com openshift_node_group_name='node-config-compute' openshift_hostname=node06.internal.aws.testdrive.openshift.com openshift_public_hostname=node06.external.aws.testdrive.openshift.com

...
----

Now that these hosts are properly defined (uncommented), you can use Ansible to
verify that they are, in fact, online:

[source,bash,role="copypaste"]
----
ansible new_nodes -m ping
----

You will see:

----
node04.internal.aws.testdrive.openshift.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node05.internal.aws.testdrive.openshift.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
node06.internal.aws.testdrive.openshift.com | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
----

Much like when you installed OpenShift originally, these new hosts have all of
the
link:https://docs.openshift.com/container-platform/3.11/install_config/install/prerequisites.html[prerequisites]
already taken care of.

#### Run the Playbook to Extend the Cluster
To extend your cluster run the following playbook:

[source,bash,role="copypaste"]
----
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/openshift-node/scaleup.yml
----

The playbook takes 1-2 minutes to complete. When done, you can verify that there are now 6 `compute` nodes:

[source,bash,role="copypaste"]
----
oc get nodes -l node-role.kubernetes.io/compute=true
----

You will see:

----
NAME                                          STATUS    ROLES     AGE       VERSION
node01.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
node02.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
node03.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
node04.internal.aws.testdrive.openshift.com   Ready     compute   18m       v1.11.0+d4cacc0
node05.internal.aws.testdrive.openshift.com   Ready     compute   18m       v1.11.0+d4cacc0
node06.internal.aws.testdrive.openshift.com   Ready     compute   18m       v1.11.0+d4cacc0
----

After the scaleup succeeds you need to remove the `new_nodes` entry from [osev3:children]. You also need to remove the '[new_nodes]' section to add the new nodes to the regular [nodes] section of the inventory file.

Check the two lines that got added to enable the scaleup operation:
----
grep new_nodes /etc/ansible/hosts
----

You will see:

----
new_nodes
[new_nodes]
----

Remove [new_nodes] to add new nodes to the [nodes] section in the inventory file. 

----
sudo sed -i '/^\[new_nodes/d' /etc/ansible/hosts
----

Remove new_nodes from [osev3:children] section of the inventory file.

----
sudo sed -i '/^new_nodes/d' /etc/ansible/hosts
----

Your modified inventory file should now look like this:

----
[OSEv3:children]
masters
nodes
etcd
glusterfs
#ocsinfra_glusterfs_registry

...

[nodes]
{{ MASTER_INTERNAL_FQDN }} openshift_node_group_name='node-config-master' openshift_hostname={{ MASTER_INTERNAL_FQDN }} openshift_public_hostname={ MASTER_EXTERNAL_FQDN }}
{{ INFRA_INTERNAL_FQDN }} openshift_node_group_name='node-config-infra' openshift_hostname={{ INFRA_INTERNAL_FQDN }} openshift_public_hostname={{INFRA_EXTERNAL_FQDN }}
{{ NODE1_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE1_INTERNAL_FQDN }} openshift_public_hostname={{ NODE1_EXTERNAL_FQDN }}
{{ NODE2_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE2_INTERNAL_FQDN }} openshift_public_hostname={{ NODE2_EXTERNAL_FQDN }}
{{ NODE3_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE3_INTERNAL_FQDN }} openshift_public_hostname={{ NODE3_EXTERNAL_FQDN }}

{{ NODE4_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE4_INTERNAL_FQDN }} openshift_public_hostname={{ NODE4_EXTERNAL_FQDN }}
{{ NODE5_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE5_INTERNAL_FQDN }} openshift_public_hostname={{ NODE5_EXTERNAL_FQDN }}
{{ NODE6_INTERNAL_FQDN }} openshift_node_group_name='node-config-compute' openshift_hostname={{ NODE6_INTERNAL_FQDN }} openshift_public_hostname={{ NODE6_EXTERNAL_FQDN }}

...
----


// WK: Part 3: OCS

== OpenShift Container Storage Concepts

In this lab we are going to provide a view 'under the hood' of OpenShift
`PersistentVolumes` provided by OpenShift Container Storage (OCS). For this
purpose we will examine volumes leveraged by example applications using
different volume access modes.

=== How OpenShift Container Storage runs

Make sure you are logged on as the super user in the `openshift-storage`:

[source,bash,role="copypaste"]
----
oc login -u system:admin -n openshift-storage
----

OpenShift Container Storage is GlusterFS running in containers, specifically
in pods managed by OpenShift. We have looked at the pods making up the
storage cluster already in the introduction chapter. Go ahead and switch to the
storage project:

[source,bash,role="copypaste"]
----
oc project openshift-storage
----

Then, take a look at the storage *Pods*:

[source,bash,role="copypaste"]
----
oc get pods -o wide
----

Which yields:

----
NAME                      READY     STATUS    RESTARTS   AGE       IP           NODE                                          NOMINATED NODE
glusterfs-storage-7qzsm   1/1       Running   0          2h        10.0.3.252   {{NODE1_INTERNAL_FQDN}} <1>   <none>
glusterfs-storage-7rds5   1/1       Running   0          2h        10.0.1.238   {{NODE2_INTERNAL_FQDN}} <1>   <none>
glusterfs-storage-x7chr   1/1       Running   0          2h        10.0.4.221   {{NODE3_INTERNAL_FQDN}} <1>   <none>
heketi-storage-1-bxqr2    1/1       Running   0          2h        10.131.0.6   {{INFRA_INTERNAL_FQDN}} <2>   <none>
----
<1> OCS *Pods*, with each of the designated nodes running exactly one.
<2> heketi API frontend pod

[NOTE]
====
The exact *pod* names will be different in your environment, since they are
auto-generated. Also the heketi *pod* might run on any node.
====

The OCS *Pods* use the host's network and block devices to run the
software-defined storage system. See schematic below for a visualization.

.GlusterFS pods in OCS in detail.
image::cns_diagram_pod.png[]

`heketi` is a component that exposes an API to the storage system for
OpenShift. This allows OpenShift to dynamically allocate storage from OCS in a
programmatic fashion. See below for a visualization. Note that for simplicity,
in our example heketi runs on the OpenShift application nodes, not on the
infrastructure node.

.heketi pod running in OCS
image::cns_diagram_heketi.png[]

==== Examine heketi

To expose heketi's API outside of OpenShift for administrators (for
monitoring and maintenance), a *Service* named _heketi-storage_ and a *Route*
has been set up:

[source,bash,role="copypaste"]
----
oc get service,route
----

You will see something like:

----
NAME                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
svc/heketi-db-storage-endpoints   ClusterIP   172.30.228.77   <none>        1/TCP      2h
svc/heketi-storage                ClusterIP   172.30.54.191   <none>        8080/TCP   2h

NAME                                      HOST/PORT                                                              PATH      SERVICES         PORT      TERMINATION   WILDCARD
route.route.openshift.io/heketi-storage   heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}             heketi-storage   <all>                   None
----

You may verify external availability of this API and heketi being alive with a trivial health check:

[source,bash,role="copypaste"]
----
curl -w "\n" http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}/hello
----

This should return:

----
Hello from Heketi
----

This how the heketi API is made available to both external clients, like
`heketi-cli` which we examined in the introduction. But mainly it is
leveraged by OpenShift to provision storage dynamically. Let's look at this
use case.

### A Simple OCS Use Case

We are going to deploy a sample application that ships with OpenShift which
creates a PVC as part of the deployment. Log on to the system as
`fancyuser1`, using the password `openshift` and create a project with the
name `my-database-app`.

#### Create/Deploy the Application

[source,bash,role="copypaste"]
----
oc login -u fancyuser1 -p openshift
oc new-project my-database-app
----

The example application ships in the form of ready-to-use resource templates. Enter
the following command to look at the template for a sample Ruby on Rails
application with a PostgreSQL database:

[source,bash,role="copypaste"]
----
oc get template/rails-pgsql-persistent -n openshift
----

This template creates a Rails Application instance which mimics a very basic
weblog. The articles and comments are saved in a PostgreSQL database which runs
in another pod.

As part of the resource template, a PVC is created in the YAML. Run the following command to `grep` the relavant part:


[source,bash,role="copypaste"]
----
oc get template/rails-pgsql-persistent -n openshift -o yaml | grep PersistentVolumeClaim -A8
----

This shows the basic structure of a `PersistentVolumeClaim`:

[source,yaml]
----
kind: PersistentVolumeClaim
metadata:
  name: ${DATABASE_SERVICE_NAME}
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: ${VOLUME_CAPACITY}
----

This will request a *PersistentVolume* in `RWO` mode. Storage provided in
this mode can only be mounted by a single pod at a time. For a database that
is usually what you want. The requested capacity under
`spec.resources.requests.storage` is coming in via a parameter when the
template is parsed. This is how storage is _requested_.

Using persistent storage is done via a `PersistentVolume` provided in
response to this `PersistentVolumeClaim`. A `PersistentVolume` is a
representation of some physical storage capacity provisioned by the backing
storage system. It will supply the PostgreSQL pod with persistent storage on
the mount point `/var/lib/pgsql/data`.

You can see this when inspecting how the pod is described as part of the
`DeploymentConfig`:

[source,bash,role="copypaste"]
----
oc get template/rails-pgsql-persistent -n openshift -o yaml | grep mountPath -B58 -A5
----

Will show:

[source,yaml]
----
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    annotations:
      description: Defines how to deploy the database
      template.alpha.openshift.io/wait-for-ready: "true"
    name: ${DATABASE_SERVICE_NAME}
  spec:
    replicas: 1
    selector:
      name: ${DATABASE_SERVICE_NAME}
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          name: ${DATABASE_SERVICE_NAME}
        name: ${DATABASE_SERVICE_NAME}
      spec:
        containers:
        - env:
          - name: POSTGRESQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: ${NAME}
          - name: POSTGRESQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: ${NAME}
          - name: POSTGRESQL_DATABASE
            value: ${DATABASE_NAME}
          - name: POSTGRESQL_MAX_CONNECTIONS
            value: ${POSTGRESQL_MAX_CONNECTIONS}
          - name: POSTGRESQL_SHARED_BUFFERS
            value: ${POSTGRESQL_SHARED_BUFFERS}
          image: ' '
          livenessProbe:
            initialDelaySeconds: 30
            tcpSocket:
              port: 5432
            timeoutSeconds: 1
          name: postgresql
          ports:
          - containerPort: 5432
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -i
              - -c
              - psql -h 127.0.0.1 -U ${POSTGRESQL_USER} -q -d ${POSTGRESQL_DATABASE}
                -c 'SELECT 1'
            initialDelaySeconds: 5
            timeoutSeconds: 1
          resources:
            limits:
              memory: ${MEMORY_POSTGRESQL_LIMIT}
          volumeMounts:
          - mountPath: /var/lib/pgsql/data <1>
            name: ${DATABASE_SERVICE_NAME}-data <2>
        volumes:
        - name: ${DATABASE_SERVICE_NAME}-data <2>
          persistentVolumeClaim:
            claimName: ${DATABASE_SERVICE_NAME} <3>
----
<1> The mount path where the persistent storage should appear inside the container
<2> The name of the volume known by the container
<3> The `PersistentVolumeClaim` from which this volume should come from

[TIP]
====
In the above snippet you see there are even more parameters in this template.
If you want to see more about the parameters or other details of this
template, you can execute the following:

 oc describe template rails-pgsql-persistent -n openshift
====

The following diagram sums up how storage get's provisioned in OpenShift and
depicts the relationship of `PersistentVolumes`, `PersistentVolumeClaims` and
`StorageClasses`:

.OpenShift Persistent Volume Framework
image::cns_diagram_pvc.png[]

Let's try it out. The storage size parameter in the template is called
`VOLUME_CAPACITY`. The `new-app` command will again handle processing and
interpreting a *Template* into the appropriate OpenShift objects. We will
specify that we want _5Gi_ of storage as part of deploying a new app from the
template as follows:

[source,bash,role="copypaste"]
----
oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi
----

[NOTE]
====
The `new-app` command will automatically check for templates in the special
`openshift` namespace. In fact, `new-app` tries to do quite a lot of interesting
automagic things, including code introspection when pointed at code
repositories. It is a developer's good friend.
====

You will then see something like the following:

----
--> Deploying template "openshift/rails-pgsql-persistent" to project my-database-app                                                                                                                       [2/1622]

     Rails + PostgreSQL
     ---------
     An example Rails application with a PostgreSQL database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/rails-ex/blob/master/README.md.

     The following service(s) have been created in your project: rails-pgsql-persistent, postgresql.
     
     For more information about using this template, including OpenShift considerations, see https://github.com/openshift/rails-ex/blob/master/README.md.

     * With parameters:
        * Name=rails-pgsql-persistent
        * Namespace=openshift
        * Memory Limit=512Mi
        * Memory Limit (PostgreSQL)=512Mi
        * Volume Capacity=5Gi
        * Git Repository URL=https://github.com/openshift/rails-ex.git
        * Git Reference=
        * Context Directory=
        * Application Hostname=
        * GitHub Webhook Secret=pIXDthfeGR7PHxxbASEjCM7jQ0hAJ8Ph8HTIttvl # generated
        * Secret Key=ij54gqv7w04habvy6dn2sninbbdgmlicwnsvpfwa1gdn6of2rrxgo211njqaekqlhg1503xdnvo2oc7h3dk7dd3cmk7h8mvnmijikovjw5jnl2w2pnfrukkwx0sq0uj # generated
        * Application Username=openshift
        * Application Password=secret
        * Rails Environment=production
        * Database Service Name=postgresql
        * Database Username=userAFJ # generated
        * Database Password=pn6A2x3B # generated
        * Database Name=root
        * Maximum Database Connections=100
        * Shared Buffer Amount=12MB
        * Custom RubyGems Mirror URL=

--> Creating resources ...
    secret "rails-pgsql-persistent" created
    service "rails-pgsql-persistent" created
    route.route.openshift.io "rails-pgsql-persistent" created
    imagestream.image.openshift.io "rails-pgsql-persistent" created
    buildconfig.build.openshift.io "rails-pgsql-persistent" created
    deploymentconfig.apps.openshift.io "rails-pgsql-persistent" created
    persistentvolumeclaim "postgresql" created
    service "postgresql" created
    deploymentconfig.apps.openshift.io "postgresql" created
--> Success
    Access your application via route 'rails-pgsql-persistent-my-database-app.apps.790442527540.aws.testdrive.openshift.com' 
    Build scheduled, use 'oc logs -f bc/rails-pgsql-persistent' to track its progress.
    Run 'oc status' to view your app.
----

Go back to the OpenShift web console:

*{{WEB_CONSOLE_URL}}*

Make sure you are logged in as _fancyuser1_ and find your newly created
project `my-database-app`. You can now follow the deployment process here.
The deployment is complete when both the PostgreSQL pod and the Ruby
application pod have one healthy instance (rings are dark, solid blue).

[NOTE]
====
It may take up to 5 minutes for the deployment to complete.
====

On the CLI, you should now see a PVC that has been issued and has a status of _Bound_.
state.

[source,bash,role="copypaste"]
----
oc get pvc
----

You will see something like:

----
NAME         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
postgresql   Bound     pvc-6de8449e-3f34-11e8-87ea-0298f449cc4c   5Gi        RWO            {{ CNS_STORAGECLASS }}   4m
----

Alternatively, in the web console, check the *"Storage"* menu.

[TIP]
====
This PVC has been automatically fulfilled by OCS because the `{{
CNS_STORAGECLASS }}` *StorageClass* was set up as the system-wide default as
part of the installation. The responsible parameter in the inventory file
was: `openshift_storage_glusterfs_storageclass_default=true`
====

==== Test the Application

Now go ahead and try out the application. The overview page in the OpenShift
web console will tell you the *Route* which has been deployed as well.
Otherwise get it on the CLI like this:

[source,bash,role="copypaste"]
----
oc get route
----

You will see something like:

----
NAME                     HOST/PORT                                                      PATH      SERVICES                 PORT      TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-database-app.{{OCP_ROUTING_SUFFIX}}            rails-pgsql-persistent   <all>                   None
----

Following this output, point your browser to:

*http://rails-pgsql-persistent-my-database-app.{{OCP_ROUTING_SUFFIX}}/articles*

The username/password to create articles and comments is by default
'_openshift_'/'_secret_'.

You should be able to successfully create articles and comments. When they are
saved they are actually saved in the PostgreSQL database which stores its table
spaces on a GlusterFS volume provided by OCS.

[NOTE]
====
This application's template included a *Route* object definition, which is
why the *Service* was automatically exposed. This is a good practice. Note
how the actual application is hosted under the */articles* path of the URL.
====

==== Explore the underlying OCS artifacts
Now let's take a look at how this was deployed on the GlusterFS side. First you
need to acquire necessary permissions:

[source,bash,role="copypaste"]
----
oc login -u system:admin
----

Select the example project of the user `fancyuser1` if not already/still selected:

[source,bash,role="copypaste"]
----
oc project my-database-app
----

Look at the PVC to determine the PV:

[source,bash,role="copypaste"]
----
oc get pvc
----

You will see the PVC in a `BOUND` state and the name of the PV it has been bound to in the `VOLUME` column:

----
NAME         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
postgresql   Bound     pvc-6de8449e-3f34-11e8-87ea-0298f449cc4c   5Gi        RWO            glusterfs-storage   144m
----

[NOTE]
====
Your PV name will be different as it's dynamically generated. A lot of the
following things contain dynamically generated names.
*Use the supplied bash shortcuts to easy copying and pasting.*
====

Here's a little bash shortcut to store the name of the PVC in a Bash environment variable:

[source,bash,role="copypaste"]
----
export PGSQL_PV_NAME=$(oc get pvc/postgresql -o jsonpath="{.spec.volumeName}" -n my-database-app)
echo $PGSQL_PV_NAME
----

Look at the details of the PV bound to the PVC, in this case
`pvc-6de8449e-3f34-11e8-87ea-0298f449cc4c` (your's will be different, use the bash variable):

[source,bash,role="copypaste"]
----
oc describe pv $PGSQL_PV_NAME
----

You will see something like:

----
Name:		     pvc-6de8449e-3f34-11e8-87ea-0298f449cc4c <1>
Labels:          <none>
Annotations:     Description=Gluster-Internal: Dynamically provisioned PV
                 gluster.kubernetes.io/heketi-volume-id=7da624d82941c50d704dd01b366c5806
                 gluster.org/type=file
                 kubernetes.io/createdby=heketi-dynamic-provisioner
                 pv.beta.kubernetes.io/gid=2001
                 pv.kubernetes.io/bound-by-controller=yes
                 pv.kubernetes.io/provisioned-by=kubernetes.io/glusterfs
                 volume.beta.kubernetes.io/mount-options=auto_unmount
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:	   {{ CNS_STORAGECLASS }}
Status:          Bound
Claim:           my-database-app/postgresql
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        5Gi
Node Affinity:   <none>
Message:         
Source:
    Type:           Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)
    EndpointsName:  glusterfs-dynamic-postgresql
    Path:		        vol_e8fe7f46fedf7af7628feda0dcbf2f60 <2>
    ReadOnly:       false
Events:             <none>
----
<1> The unique name of this PV in the system OpenShift refers to
<2> The unique volume name backing the PV known to GlusterFS

Note the GlusterFS volume name, in this case
*vol_e8fe7f46fedf7af7628feda0dcbf2f60*. The following is another Bash
shortcut to store the name of the GlusterFS volume backing the
`PersistentVolume`:

[source,bash,role="copypaste"]
----
export PGSQL_GLUSTER_VOLUME=$(oc get pv $PGSQL_PV_NAME -o jsonpath='{.spec.glusterfs.path}')
echo $PGSQL_GLUSTER_VOLUME
----

Now let's switch to the namespace we used for OCS deployment:

[source,bash,role="copypaste"]
----
oc project {{ CNS_NAMESPACE }}
----

Look at the GlusterFS pods running and pick one (which one is not important):

[source,bash,role="copypaste"]
----
oc get pods -o wide -l glusterfs=storage-pod
----

You will see something like:

----
NAME                      READY     STATUS    RESTARTS   AGE       IP           NODE                                          NOMINATED NODE
glusterfs-storage-7qzsm   1/1       Running   0          2h        10.0.3.252   {{NODE1_INTERNAL_FQDN}}   <none>
glusterfs-storage-7rds5   1/1       Running   0          2h        10.0.1.238   {{NODE2_INTERNAL_FQDN}}   <none>
glusterfs-storage-x7chr   1/1       Running   0          2h        10.0.4.221   {{NODE3_INTERNAL_FQDN}}   <none>
----

We are now going to select the first pod (which one doesn't really matter)
and, store it's IP address in above example that is: *{{NODE1_INTERNAL_IP}}*
of pod *glusterfs-storage-37vn8*.

Again, for easy copying and pasting, here are some Bash shortcuts:

[source,bash,role="copypaste"]
----
export FIRST_GLUSTER_POD=$(oc get pods -o jsonpath='{.items[0].metadata.name}' -l glusterfs=storage-pod)
export FIRST_GLUSTER_IP=$(oc get pods -o jsonpath='{.items[0].status.podIP}' -l glusterfs=storage-pod)
echo $FIRST_GLUSTER_POD
echo $FIRST_GLUSTER_IP
----

We will again use the `oc rsh` facility to log on to the selected GlusterFS
pod which has the GlusterFS CLI utilities installed. This time we will use
the non-interactive mode which immediately drops out after executing the
supplied command.

Query GlusterFS from inside the first GlusterFS pod for all known volumes:

[source,bash,role="copypaste"]
----
oc rsh $FIRST_GLUSTER_POD gluster volume list
----

You will immediately drop back out to your shell and you will see something like:

----
heketidbstorage <1>
vol_e8fe7f46fedf7af7628feda0dcbf2f60 <2>
vol_5e1cd71070734a3b02f58d822f89486a
vol_f2e8fda1d42a41efabbb4d4a3b4a5659
----
<1> A special volume dedicated to heketi's internal database.
<2> The volume backing the PV of the PostgreSQL database we asked you to remember.

Query GlusterFS about the topology of this volume:

[source,bash,role="copypaste"]
----
oc rsh $FIRST_GLUSTER_POD gluster volume info $PGSQL_GLUSTER_VOLUME
----

You will see something like:

----
Volume Name: vol_e8fe7f46fedf7af7628feda0dcbf2f60
Type: Replicate
Volume ID: c2bedd16-8b0d-432c-b9eb-4ab1274826dd
Status: Started
Snapshot Count: 0
Number of Bricks: 1 x 3 = 3
Transport-type: tcp
Bricks:
Brick1: {{NODE2_INTERNAL_IP}}:/var/lib/heketi/mounts/vg_63b05bee6695ee5a63ad95bfbce43bf7/brick_aa28de668c8c21192df55956a822bd3c/brick
Brick2: {{NODE1_INTERNAL_IP}}:/var/lib/heketi/mounts/vg_0246fd563709384a3cbc3f3bbeeb87a9/brick_684a01f8993f241a92db02b117e0b912/brick <1>
Brick3: {{NODE3_INTERNAL_IP}}:/var/lib/heketi/mounts/vg_5a8c767e65feef7455b58d01c6936b83/brick_25972cf5ed7ea81c947c62443ccb308c/brick
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
cluster.brick-multiplex: on
----
<1> According to the output of `oc get pods -o wide` this is the container we are logged on to.

[NOTE]
====
Identify the right brick by looking at the host IP of the GlusterFS pod
you have just logged on to. `oc get pods -o wide` will give you this
information. The host's IP will be noted next to one of the bricks.
====

GlusterFS created this volume as a 3-way replica set across all GlusterFS
pods, and therefore across all your OpenShift App nodes running OCS. Data
written to such a replica volume is replicated 3 times to all *bricks*.
*Bricks* are local storage in GlusterFS nodes, usually backed by a local SAS
*disk or NVMe device. Each node exposes its local storage via the GlusterFS
*protocol. The brick itself is simply a directory on a block device formatted
*with XFS. Hence you can look with a simple `ls` command and see how the data
*is actually stored in each brick.

For easy copying and pasting, here's another bash shortcut to extract the
brick directory path of our PostgreSQL volume from the fist GlusterFS pod in
the list:

[source,bash,role="copypaste"]
----
export PGSQL_GLUSTER_BRICK=$(echo -n $(oc rsh $FIRST_GLUSTER_POD gluster vol info $PGSQL_GLUSTER_VOLUME | grep $FIRST_GLUSTER_IP) | cut -d ':' -f 3 | tr -d $'\r' )
echo $PGSQL_GLUSTER_BRICK
----

You can look at the brick directory of the first GlusterFS pod and see how
GlusterFS stores the files from the clients in a brick:

[source,bash,role="copypaste"]
----
oc rsh $FIRST_GLUSTER_POD ls -ahl $PGSQL_GLUSTER_BRICK
----

You will see something like:

----
total 16K
drwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .
drwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..
drw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs
drwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan
drwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata
----

Dig a bit deeper, try looking at the `userdata` folder:

[source,bash,role="copypaste"]
----
oc rsh $FIRST_GLUSTER_POD ls -ahl $PGSQL_GLUSTER_BRICK/userdata
----

You will see the PostgreSQL database folder structure:

----
total 68K
drwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .
drwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..
-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION
drwx------.  6 1000080000 root   54 Jun  6 14:46 base
drwx------.  2 1000080000 root 8.0K Jun  6 14:47 global
drwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem
-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf
-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf
drwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log
drwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical
drwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact
drwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots
drwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat
drwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp
drwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc
drwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase
drwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog
-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf
-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf
-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts
-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid
----

You are looking at the PostgreSQL internal data file structure from the
perspective of the GlusterFS server side. It's a normal local filesystem here.

Clients, like the OpenShift nodes and their application pods talk to this set
of replicated brick storage via the GlusterFS protocol. Which abstracts the
3-way replication behind a single FUSE mount point - this is called a
`volume` in GlusterFS. When a pod starts that mounts storage from a `PV`
backed by GlusterFS, OpenShift will mount the GlusterFS volume on the right
app node and then _bind-mount_ this directory to the right pod. This is
happening transparently to the application inside the pod and looks like a
normal local filesystem.

=== Providing Scalable, Shared Storage With OCS

Historically very few options, like basic NFS support, existed to provide a
*PersistentVolume* to more than one container at a time. The access mode used
for this in OpenShift is `ReadWriteMany`. Traditional block-based storage
solutions are not able to provide *PersistentVolumes* with this access mode.

Also, once provisioned, most storage cannot easily be resized.

With OCS these capabilities are now available to all OpenShift deployments, no
matter where they are deployed. To illustrate the benefit of this, we will
deploy a PHP file uploader application that has multiple front-end instances
sharing a common storage repository.

#### Deploy the File Uploader Application

First log back in as `fancyuser1` using the password `openshift` and create a
new project:

[source,bash,role="copypaste"]
----
oc login -u fancyuser1 -p openshift
oc new-project my-shared-storage
----

Next deploy the example PHP application called `file-uploader`:

[source,bash,role="copypaste"]
----
oc new-app openshift/php:7.1~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader
----

You will see something like:

----
--> Found image 691930e (5 weeks old) in image stream "openshift/php" under tag "7.1" for "openshift/php:7.1"

    Apache 2.4 with PHP 7.1 
    ----------------------- 
    PHP 7.1 available as container is a base platform for building and running various PHP 7.1 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.

    Tags: builder, php, php71, rh-php71

    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created
      * The resulting image will be pushed to image stream tag "file-uploader:latest"
      * Use 'start-build' to trigger a new build
    * This image will be deployed in deployment config "file-uploader"
    * Ports 8080/tcp, 8443/tcp will be load balanced by service "file-uploader"
      * Other containers can access this service through the hostname "file-uploader"

--> Creating resources ...
    imagestream.image.openshift.io "file-uploader" created
    buildconfig.build.openshift.io "file-uploader" created
    deploymentconfig.apps.openshift.io "file-uploader" created
    service "file-uploader" created
--> Success
    Build scheduled, use 'oc logs -f bc/file-uploader' to track its progress.
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/file-uploader' 
    Run 'oc status' to view your app.
----

Watch and wait for the application to be deployed:

[source,bash,role="copypaste"]
----
oc logs -f bc/file-uploader
----

You will see something like:

----
Cloning "https://github.com/christianh814/openshift-php-upload-demo" ...
	Commit:	7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)
	Author:	Christian Hernandez <christianh814@users.noreply.github.com>
	Date:	Thu Mar 23 09:59:38 2017 -0700
---> Installing application source...
Pushing image 172.30.120.134:5000/my-shared-storage/file-uploader:latest ...
Pushed 0/5 layers, 2% complete
Pushed 1/5 layers, 20% complete
Pushed 2/5 layers, 40% complete
Push successful
----

The command prompt returns out of the tail mode once you see _Push successful_.

[NOTE]
====
This use of the `new-app` command directly asked for application code to be
built and did not involve a template. That's why it only created a *single
Pod* deployment with a *Service* and no *Route*.
====

Let's make our application production ready by exposing it via a `Route` and
scale to 3 instances for high availability:

[source,bash,role="copypaste"]
----
oc expose svc/file-uploader
oc scale --replicas=3 dc/file-uploader
----

Now, check the *Route* that has been created:

[source,bash,role="copypaste"]
----
oc get route
----

You will see something like:

----
NAME                     HOST/PORT                                                      PATH      SERVICES                 PORT       TERMINATION   WILDCARD
file-uploader            file-uploader-my-shared-storage.{{ OCP_ROUTING_SUFFIX}}                      file-uploader            8080-tcp                 None
...
----

Point your browser to the web application using the URL advertised by the route
(http://file-uploader-my-shared-storage.{{ OCP_ROUTING_SUFFIX}})

The web app simply lists all previously uploaded files and offers the ability
to upload new ones as well as download the existing data. Right now there is
nothing.

Select an arbitrary file from your local machine and upload it to the app.

.A simple PHP-based file upload tool
image::uploader_screen_upload.png[]

Once done click *_List uploaded files_* to see the list of all currently
uploaded files.

Do you see it? Don't worry if you don't.

Change back to the command line and look at the running pods.

[source,bash,role="copypaste"]
----
oc get pods -l app=file-uploader
----

You will see 3 pods running:

----
NAME                    READY     STATUS    RESTARTS   AGE
file-uploader-1-5hhqb   1/1       Running   0          6m
file-uploader-1-trkxr   1/1       Running   0          6m
file-uploader-1-vqszb   1/1       Running   0          7m
----

Now let's look back at where this file got stored inside the pods. Again use
the `oc rsh` utility via a scriptlet to execute an `ls` command on the
`upload` directory that the PHP code uses to store the files:

[source,bash,role="copypaste"]
----
for pod in $(oc get pod -l app=file-uploader --no-headers | awk '{print $1}'); do echo $pod; oc rsh $pod ls -hl uploaded; done
----


You will see that only one of the pods has the uploaded file
----
file-uploader-1-5hhqb
total 0
file-uploader-1-trkxr
total 352K
-rw-r--r--. 1 1000380000 root 352K Oct 29 16:00 firefly-episode-list.txt
file-uploader-1-vqszb
total 0
----

Why is that? These pods currently do not use any persistent storage. They
store the file locally in the container root file system. That means the
application cannot effectively be scaled since the pods do not share data and
every client would see different uploaded files. To verify this, try
accessing the URL with a second _Icognito_ browser session.

[CAUTION]
====
Never attempt to store persistent data in a *Pod* that has no persistent
volume associated with it. *Pods* and their containers are ephemeral by
definition, and any stored data will be lost as soon as the *Pod* terminates
for whatever reason.
====

The app is of course not useful like this. We can fix this by providing shared
storage to this app.

You can create a *PersistentVolumeClaim* and attach it into an application with
the `oc set volume` command. Execute the following

[source,bash,role="copypaste"]
----
oc set volume dc/file-uploader --add --name=my-shared-storage \
-t pvc --claim-mode=ReadWriteMany --claim-size=1Gi \
--claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded
----

Like with the `mapit` application in "_Application Management Basics_"
chapter, this command will:

* create a *PersistentVolumeClaim*
* update the *DeploymentConfig* to include a `volume` definition
* update the *DeploymentConfig* to attach a `volumemount` into the specified
  `mount-path`
* cause a new deployment of the application *Pods*

For more information on what `oc set volume` is capable of, look at its help output
with `oc set volume -h`. Now, let's look at the result of adding the volume:

[source,bash,role="copypaste"]
----
oc get pvc
----

You will see something like:

----
NAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
my-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   1Gi        RWX           22s
...
----

Notice the `ACCESSMODE` being set to *RWX* (short for `ReadWriteMany`,
equivalent to "shared storage"). Without this `ACCESSMODE`, OpenShift will
not attempt to attach multiple *Pods* to the same *PersistentVolume*
reliably. If you attempt to scale up deployments that are using
`ReadWriteOnce` storage, they will actually all become co-located on the same
node.

The app has now re-deployed (in a rolling fashion) with the new settings -
all pods will mount the volume identified by the PVC under
`/opt/app-root/src/upload`.

Check you have a new set of pods:

[source,bash,role="copypaste"]
----
oc get pods -l app=file-uploader
----

You will see something like:

----
NAME                    READY     STATUS    RESTARTS   AGE
file-uploader-2-4h7bx   1/1       Running   0          2m
file-uploader-2-gqbsn   1/1       Running   0          2m
file-uploader-2-pkmpj   1/1       Running   0          2m
----

Try it out in your file uploader web application using your browser. Upload
new files and see that they are visible from within all application pods.

[CAUTION]
====
Where is my previously uploaded file?

Since the pod redeployed the file has been lost with the previous container's
root filesystem going away as part of the configuration update. One more
reason to provide persistent storage!
====

Once done, return to the command line and look at the contents of pods:

[source,bash,role="copypaste"]
----
for pod in $(oc get pod -l app=file-uploader --no-headers | awk '{print $1}'); do echo $pod; oc rsh $pod ls -hl uploaded; done
----


You will see that now all of the pods have the uploaded file:
----
file-uploader-2-4h7bx
total 352K
-rw-r--r--. 1 1000380000 2002 352K Oct 29 16:10 firefly-episode-list.txt
file-uploader-2-gqbsn
total 352K
-rw-r--r--. 1 1000380000 2002 352K Oct 29 16:10 firefly-episode-list.txt
file-uploader-2-pkmpj
total 352K
-rw-r--r--. 1 1000380000 2002 352K Oct 29 16:10 firefly-episode-list.txt
----

That's it. You have successfully provided shared storage to pods throughout the
entire system, therefore avoiding the need for data to be replicated at the
application level to each pod.

With OCS this is available wherever OpenShift is deployed without external
dependencies like NFS.

=== Increasing volume capacity

However, what happens when the volume is full?

Let's try it. Run the following command to fill up the currently 1GiB of free
space in the persistent volume. Since it's shared, you can use any the 3
file-uploader pods:

[source,bash,role="copypaste"]
----
oc rsh $(oc get pod -l app=file-uploader --no-headers | head -n1 | awk '{print $1}') dd if=/dev/zero of=uploaded/bigfile bs=100M count=1000
----

The result after some time is:
----
dd: error writing 'uploaded/bigfile': No space left on device
dd: closing output file 'uploaded/bigfile': No space left on device
command terminated with exit code 1
----

Oops. The file system seems to have a problem. Let's check it:

[source,bash,role="copypaste"]
----
oc rsh $(oc get pod -l app=file-uploader --no-headers | head -n1 | awk '{print $1}') df -h /opt/app-root/src/uploaded
----

Clearly the file system is full:

----
Filesystem                                      Size  Used Avail Use% Mounted on
10.0.1.36:vol_6320cd6974d8573f49f85a5d7255a7f2 1019M 1019M     0 100% /opt/app-root/src/uploaded
----

If you were to try uploading another file via the web application it would fail with something along the lines:

----
[...]
failed to open stream: No space left on device in /opt/app-root/src/upload.php on line 26
[...]
----

First the `StorageClass` glusterfs-storage needs to be modified to include `allowVolumeExpansion: true`. To add this new parameter the following process is used.

[WARNING]
====
It is required that the feature-gates: (below) is added to the /etc/origin/master/master-config.yaml and the master services restarted before modifying the `StorageClass` glusterfs-storage.

	kubernetesMasterConfig:
	  apiServerArguments:
	    feature-gates:
	    - ExpandPersistentVolumes=true
====

----
oc get sc glusterfs-storage -o yaml > glusterfs-storage.yaml
----

And then add the new parameter to the glusterfs-storage-new.yaml file.

----
sed '/volumeBindingMode: Immediate/a allowVolumeExpansion: true' glusterfs-storage.yaml > glusterfs-storage-new.yaml
----

Now to modify this `StorageClass` the current glusterfs-storage needs to be deleted and the new glusterfs-storage-new.yaml used to create glusterfs-storage that containes the necessary parameter `allowVolumeExpansion: true`.

----
oc delete sc glusterfs-storage
oc create -f glusterfs-storage-new.yaml
----

Now do the following to validate the `StorageClass` is modified.

----
oc get sc glusterfs-storage -o yaml
----

You will see something like below.

----
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  creationTimestamp: 2019-04-22T19:33:05Z
  name: glusterfs-storage

...
----

Also verify using this command:

----
oc describe sc glusterfs-storage
----

You can see `AllowVolumeExpansion:  True` in this output as well.

----
Name:                  glusterfs-storage
IsDefaultClass:        Yes
Annotations:           storageclass.kubernetes.io/is-default-class=true
Provisioner:           kubernetes.io/glusterfs
Parameters:            resturl=http://heketi-storage.storage.svc:8080,restuser=admin,secretName=heketi-storage-admin-secret,secretNamespace=storage
AllowVolumeExpansion:  True
MountOptions:          <none>
ReclaimPolicy:         Delete
VolumeBindingMode:     Immediate
Events:                <none>
----

After the `StorageClass` is modified to allow `PersistentVolume` expansion, the volume size can be increased by the user or owner of the app, even without administrator intervention.

[WARNING]
====
If you are unfamiliar with the `vi` editor, please run the following command before continuing:

    export EDITOR=nano
====

Use the `oc edit` command to edit the `PersistentVolumeClaim` that we used to
generate the `PersistentVolume`:

[source,bash,role="copypaste"]
----
oc edit pvc my-shared-storage
----

You end up in a `vi` session editing the `PVC` object properties in YAML. Go
to line that says `storage: 1Gi` below spec -> resources -> requests and
increase to `5Gi` like shown below:

[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
    volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/glusterfs
  creationTimestamp: 2018-04-18T10:17:24Z
  name: my-shared-storage
  namespace: my-shared-storage
  resourceVersion: "41960"
  selfLink: /api/v1/namespaces/my-shared-storage/persistentvolumeclaims/my-shared-storage
  uid: b0544244-42f1-11e8-8f68-02f9630bd644
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 5Gi <1>
  storageClassName: glusterfs-storage
  volumeName: pvc-b0544244-42f1-11e8-8f68-02f9630bd644
status:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 1Gi
  phase: Bound
----
<1> Set this to *5Gi*

Exit out of `vi` mode with the `:wq` command.

[TIP]
====
Upon writing the file the `oc edit` command will update the
`PersistentVolumeClaim` definition in OpenShift. This way of ad-hoc editing
works with many objects in OpenShift.
====

Give it a couple of seconds and then check the filesystem again:

[source,bash,role="copypaste copypaste-warning"]
----
oc rsh $(oc get pod -l app=file-uploader --no-headers | head -n1 | awk '{print $1}') df -h /opt/app-root/src/uploaded
----

The situation should look much better now:

----
Filesystem                                      Size  Used Avail Use% Mounted on
10.0.1.36:vol_6320cd6974d8573f49f85a5d7255a7f2  5.0G  1.1G  4.0G  21% /opt/app-root/src/uploaded
----


### OCS Operations

#### Options to increase Storage Capacity in OCS

At some point the overall OCS cluster capacity may need to be expanded. There are a couple of ways to increase the storage capacity offered by OCS.

1. add a second, independent OCS cluster with its own management stack (`heketi`) (like you did in the _Infrastructure Management_ module )
2. add a second, independent OCS cluster to the existing management stack (as described in the link:https://access.redhat.com/documentation/en-us/container-native_storage/3.9/html-single/container-native_storage_for_openshift_container_platform/#idm140292314514720[documentation^])
3. add additional nodes to an existing OCS cluster (as described in the link:https://access.redhat.com/documentation/en-us/container-native_storage/3.9/html-single/container-native_storage_for_openshift_container_platform/#idm140292314767904[documentation^])
4. add additional devices to existing nodes

Option 1) is automated using `openshift-ansible`

Option 2) is an option you likely want to take when you have nodes with
different media types (SSD vs. HDD) and you want to offer quality of service.

Option 3) allows you to easily expand the cluster capacity in-place. In this
lab we however have no nodes left to add, so we will illustrate Option 4).

#### Adding Additional Devices to a OCS Cluster

To perform management operations we'll use the `heketi-cli` tool. It manages
several entities that make up OCS, that is: clusters, nodes, volumes and
devices.

For each entity there are several create/add, update, delete commands
available. For initial cluster setup `heketi-cli` also offers batch
processing via a JSON file.

In the following we will manually add devices from `node04`, `node05` and
`node06`, which form the OCS cluster for OpenShift infrastructure.

Like in the _Installation_ module, we first set up some Bash environment
variables to configure our `heketi-cli` client to talk to the second OCS
cluster. This time we take a shortcut by programmatically determining the URL
to heketi and the password by querying the `heketi` pod:

[source,bash,role="copypaste"]
----
export HEKETI_POD=$(oc get pods -l glusterfs=heketi-registry-pod -o jsonpath='{.items[0].metadata.name}' -n {{ CNS_INFRA_NAMESPACE }})
export HEKETI_CLI_SERVER=http://$(oc get route -l glusterfs=heketi-registry-route -o jsonpath='{.items[0].spec.host}' -n {{ CNS_INFRA_NAMESPACE }})
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=$(oc get pod/$HEKETI_POD -o jsonpath='{.spec.containers[0].env[?(@.name=="HEKETI_ADMIN_KEY")].value}' -n {{ CNS_INFRA_NAMESPACE }})
----

We can now query `heketi` about the nodes in this cluster:

[source,bash,role="copypaste"]
----
heketi-cli node list
----

And you will see something like:

----
Id:33e0045354db4be29b18728cbe817605	Cluster:ca777ae0285ef6d8cd7237c862bd591c
Id:d8443e7ee8314c0c9fb4d8274a370bbd	Cluster:ca777ae0285ef6d8cd7237c862bd591c
Id:caaed3927e424b22b1a89d261f7617ad	Cluster:ca777ae0285ef6d8cd7237c862bd591c
----

The UUIDs of the nodes will be different for you. We however need them to
tell `heketi` from which nodes to add a device. To avoid repetitive copying
and pasting here is another Bash short cut to parse above output in a Bash
variable:

Run the following command to store the `heketi`-internal ID of the OCS
cluster (there is only one for this `heketi` instance) in a bash variable:

[source,bash,role="copypaste"]
----
export CNS_INFRA_CLUSTER=$(heketi-cli cluster list --json | jq -r '.clusters[0]')
echo $CNS_INFRA_CLUSTER
----

Then get a list of the nodes of this cluster into a Bash variable:

[source,bash,role="copypaste"]
----
export NODES=$(heketi-cli cluster info $CNS_INFRA_CLUSTER --json | jq -r '.nodes[]')
export NODE_LIST=($NODES)
echo $NODES
----

To illustrate the before and after effect, first inspect the output of:

[source,bash,role="copypaste"]
----
heketi-cli topology info
----

You should see that every node currently has a single device: `{{NODE_BRICK_DEVICE}}`.

These nodes of the second OCS cluster, have an additional, unused storage
device `{{NODE_BRICK_DEVICE2}}`. For each node now go ahead and make `heketi`
aware of this device using the `device add` directive

[source,bash,role="copypaste"]
----
heketi-cli device add --name={{NODE_BRICK_DEVICE2}} --node=${NODE_LIST[0]}
heketi-cli device add --name={{NODE_BRICK_DEVICE2}} --node=${NODE_LIST[1]}
heketi-cli device add --name={{NODE_BRICK_DEVICE2}} --node=${NODE_LIST[2]}
----

Each command should return with the message `Device added successfully`.

Check `heketi-cli topology info` again to verify the presence of the new
devices.

That's it - the devices are now available to `heketi` and will be considered
the next time OCS serves a volume request. Adding devices and nodes are
online operations, meaning they are non-disruptive and can be run in
production without downtime.

### Replacing Failed Disks and Nodes

When a device fails, OCS transparently continues operations with the
remaining replicas. You will need to replace such components to move out of a
degraded state and get to 3 replicas again, either using other devices free
capacity in the same node or in different nodes.

For this exercise, let's assume the device `{{ NODE_BRICK_DEVICE }}` of your
node {{ NODE4_INTERNAL_FQDN }} failed and you need to replace it. You can do
that as long as there is enough spare capacity somewhere else in the cluster,
preferrable but not necessarily in the same failure domain (as specifed in
the topology).

[TIP]
====
OCS is aware of failure domains in your infrastructure. These could be racks
in a data center or availability zones in public cloud environments. The
zones are identified by distinct values in the `zone` parameter of each node.
Nodes with the same value for `zone` are considered part of the same failure
domain. OCS will try to do its best (but not enforce it) to replicate and
rebalance data across 3 different failure domains at all times.
====

The first step is to determine the OCS node's internal UUID in heketi's
database. You can do that manually:

[source,bash,role="copypaste"]
----
heketi-cli topology info | grep -B4 {{NODE4_INTERNAL_FQDN}}
----

...and see something like:

----
	Node Id: 33e0045354db4be29b18728cbe817605
	State: online
	Cluster Id: ca777ae0285ef6d8cd7237c862bd591c
	Zone: 1
	Management Hostname: {{NODE4_INTERNAL_FQDN}}
----

Or you can do it programmatically, for easy copying and pasting, by asking `heketi` and parsing its JSON output using `jq`:

[source,bash,role="copypaste"]
----
NODE_4_ID=$(heketi-cli topology info --json | jq -r ".clusters[] | select(.id==\"$CNS_INFRA_CLUSTER\") | .nodes[] | select(.hostnames.manage[0] == \"{{NODE4_INTERNAL_FQDN}}\") | .id")
echo $NODE_4_ID
----

This should yield, like above `33e0045354db4be29b18728cbe817605`

Second, determine the device's UUID by querying the node (indicated above by
`Node Id`):

Again, you could do this manually by looking at `heketi` information about the node:

[source,bash,role="copypaste"]
----
heketi-cli node info $NODE_4_ID
----

And then you will see:

----
Node Id: 33e0045354db4be29b18728cbe817605
State: online
Cluster Id: 119ea7f96ce132f15a04c28de9978018
Zone: 1
Management Hostname: {{ NODE4_INTERNAL_FQDN }}
Storage Hostname: {{ NODE4_INTERNAL_IP }}
Devices:
Id:0b32d5e57f2047485e42e6288405ad7f   Name:{{ NODE_BRICK_DEVICE2 }}           State:online    Size (GiB):49      Used (GiB):0       Free (GiB):49
Id:4fb2ae473d5ee451906d5489abfc653e   Name:{{ NODE_BRICK_DEVICE }}           State:online    Size (GiB):49      Used (GiB):42      Free (GiB):7
----

Or again, for easy copying and pasting, you can do it the smart way and retrieve the device ID of `{{NODE_BRICK_DEVICE}}` programmatically from the JSON output using `jq`:

[source,bash,role="copypaste"]
----
export FAILED_DEVICE_ID=$(heketi-cli node info $NODE_4_ID  --json | jq -r '.devices[] | select(.name=="{{ NODE_BRICK_DEVICE }}") | .id')
echo $FAILED_DEVICE_ID
----

You should get the UUID of `{{ NODE_BRICK_DEVICE }}` from this command, in
this example `4fb2ae473d5ee451906d5489abfc653e`.

With the UUID we can first mark the device as offline to stop heketi from
further attempts to allocate space from it:

[source,bash,role="copypaste"]
----
heketi-cli device disable $FAILED_DEVICE_ID
----

You will see something like:

----
Device 4fb2ae473d5ee451906d5489abfc653e is now offline
----

The device is now offline but it's still part of replicated volumes. To remove
it and trigger a self-healing operation in the background issue:

[source,bash,role="copypaste"]
----
heketi-cli device remove $FAILED_DEVICE_ID
----

You will see something like:

----
Device 4fb2ae473d5ee451906d5489abfc653e is now removed
----

[NOTE]
====
This command can take a bit long as it will go through the topology and
search for the next available device on the same node, in the same failure
domain or in the rest of the cluster (in that order) and trigger a
*brick-replacement operation*. That is, the data from the failed brick is
re-replicated to another health storage device and the 3-way replicated
storage volume moves out of degraded state.
====

This is an online operation and can absolutely be run in production.

Our failed device is still lurking around in _failed_ state. To finally get
rid of it issue:

[source,bash,role="copypaste"]
----
heketi-cli device delete $FAILED_DEVICE_ID
----

You will see something like:

----
Device 4fb2ae473d5ee451906d5489abfc653e deleted
----

[NOTE]
====
Only devices that are not currently used by other Gluster volumes can be
deleted. If that's not the case, `heketi-cli` will tell you about it. Devices
that are in use always need to have `remove` performed first.
====

You can now check that the device is gone from the topology by running:

[source,bash,role="copypaste"]
----
heketi-cli topology info
----

*Node deletion* is also possible and is basically comprised of:

1. successful execution of the `remove` operation on all devices of the node
2. running `heketi-cli node delete <node_id>` on the node in question

// WK: Part 4: rook

== Deploying and Managing OpenShift Container Storage with Rook-Ceph Operator

In this section you are learning how to deploy and manage OpenShift Container Storage (OCS). In this lab you will be using OpenShift Container Platform 3.11 (OCP) and Rook.io v0.9 to deploy Ceph as a persistent storage solution for OCP workloads.

=== In this lab you will learn how to

* Configure and deploy contanerized Ceph using Rooks cluster CustomResourceDefinitions (CRD)
* Validate deployment of Ceph Luminous containerized using OpenShift CLI
* Deploy the Rook toolbox to run common ceph and rados commands
* Create a Persistent Volume (PV) on the Ceph cluster using a Rook OCP storageclass for deployment of Rails application using a PostgreSQL database.
* Upgrade Ceph version from Luminous to Mimic using the Rook operator
* Add more storage to the Ceph cluster

[[labexercises]]
:numbered:
== Deploy Ceph using Rook.io

=== Download Rook deployment files and install Ceph

In this section necessary files will be downloaded using the `curl -O` command and OCP resources created using the `oc create` command and the Rook.io yaml files.

Labeling the new OCP nodes with role=storage-node will make sure that the OCP resources (OSD, MON, MGR pods) are scheduled on these nodes.

----
oc label node node04.internal.aws.testdrive.openshift.com role=storage-node
oc label node node05.internal.aws.testdrive.openshift.com role=storage-node
oc label node node06.internal.aws.testdrive.openshift.com role=storage-node
oc get nodes --show-labels | grep storage-node
----

Next you will download Rook.io scc.yaml, operator.yaml and cluster.yaml to create OCP resources. After downloading each on view the file using the `cat` command before creating the resources using `oc create`.

----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/scc.yaml
oc create -f scc.yaml
----

Validate that rook-ceph has been added to securitycontextconstraints.security.openshift.io.

----
oc get scc rook-ceph
oc export scc rook-ceph
----

Install the Rook operator next.

----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/operator.yaml
oc create -f operator.yaml
oc project rook-ceph-system
watch oc get pods -o wide
----

Wait for all rook-ceph-agent, rook-discover and rook-ceph-operator pods to be in a Running state. The log for the rook-ceph-operator pod should show that the operator is looking for a cluster. Look for `the server could not find the requested resource (get clusters.ceph.rook.io)` at the end of the log file. Replace `xxxxxxxxx-xxxxx` below with your rook-ceph-operator pod name.

----
oc get pod -l app=rook-ceph-operator
oc logs rook-ceph-operator-xxxxxxxxx-xxxxx
----

Next step is to download and install the cluster CRD to create Ceph MON, MGR and OSD pods.

----
oc new-project rook-ceph
oc adm pod-network make-projects-global rook-ceph
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/cluster.yaml
----

Take a look at the cluster.yaml file. It specifies the version of Ceph, the label used for the rook resources (role=storage-node) added at the start of this section, and the nodes and storage devices used for OSDs.

----
cat cluster.yaml
...omitted...
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
...omitted...
    image: ceph/ceph:v12.2.11-20190201
...omitted...
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    nodes:
    # Each node's 'name' field should match their 'kubernetes.io/hostname' label.
    - name: "node04.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
    - name: "node05.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
    - name: "node06.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
----

Now create the MONs, MGR and OSD pods.

----
oc create -f cluster.yaml
----

Disregard this message Error from server (AlreadyExists): error when creating "cluster.yaml": namespaces "rook-ceph" already exists

----
oc project rook-ceph
watch oc get pods
NAME                                        READY     STATUS      RESTARTS   AGE
rook-ceph-mgr-a-5887d4d48b-pz52j            1/1       Running     0          2m
rook-ceph-mon-a-5df5865956-gnsvs            1/1       Running     0          3m
rook-ceph-mon-b-66d74f475d-5n4jt            1/1       Running     0          2m
rook-ceph-mon-c-86bc6b98b7-5xfhf            1/1       Running     0          2m
rook-ceph-osd-0-96c9b769-qclw9              1/1	      Running     0          1m
rook-ceph-osd-1-7747889669-fcvsj            1/1	      Running     0          1m
rook-ceph-osd-2-7cc7bdf44d-ncqbr            1/1	      Running     0          1m
----

Once all pods are in a Running state it is time to verify that Ceph is operating correctly. Download toolbox.yaml to run Ceph commands.

----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/toolbox.yaml
oc create -f toolbox.yaml
----

Login to toolbox pod to run Ceph commands.

----
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
ceph status
ceph osd status
ceph osd tree
ceph df
rados df
exit
----

Disregard the health: HEALTH_WARN mons a,b,c are low on available space message when viewing results of `ceph status` command.

=== Create Rook storageclass for creating CephRBD block volumes

In this section you will download storageclass.yaml and then create the OCP storageclass `rook-ceph-block` that will be used by applications to dynamically claim persistent storage (PVCs). The Ceph pool `replicapool` is created when the storageclass is created.

----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/storageclass.yaml
cat  storageclass.yaml
----

Notice the provisioner: ceph.rook.io/block and that replicated: size=2.

----
oc create -f storageclass.yaml
----

Login to toolbox pod to run Ceph commands. Compare results for `ceph df` and `rados df` executed in prior section before the storageclass was created.

----
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
ceph df
rados df
rados -p replicapool ls
exit
----

== Create new OCP deployment using CephRBD block volume

In this section the `rook-ceph-block` storageclass will be used by an application + database deployment to create persistent storage. The persistent storage will be a CephRBD volume (object) in the pool=replicapool.

Because the Rails + PostgreSQL deployment uses the `default` storageclass we need to modify the current default storageclass (glusterfs-storage) and edit then make `rook-ceph-block` the default storageclass.

----
oc get storageclass
oc edit sc glusterfs-storage
----

Remove this portion shown below from storageclass `glusterfs-storage`. Make sure to note EXACTLY where this annotations is located in the storageclass (copying this portion and before and after syntax to clipboard would be good idea). The editing tool is `vi` when using `oc edit`.

----
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
----

Add the removed portion to `rook-ceph-block` in same place so it will be the default storageclass. Make sure to save your changes before exiting `:wq!`. Validate that `rook-ceph-block` is now the default storageclass before starting the OCP application deployment.

----
oc edit sc rook-ceph-block
oc get storageclass
----

After editing storageclass `rook-ceph-block` the result should be similar to below and `rook-ceph-block` should be the `default` storageclass.

----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  creationTimestamp: 2019-03-08T20:54:46Z
  name: rook-ceph-block
...omitted...
----

----
$ oc get sc
NAME                        PROVISIONER               AGE
glusterfs-storage           kubernetes.io/glusterfs   5h
rook-ceph-block (default)   ceph.rook.io/block        35m
----

Now you are ready to start the Rails + PostgreSQL deployment.

----
oc new-project my-database-app
oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi
oc status
oc get pvc
watch oc get pods
----

Wait until the pods are all in a Running state. This could take 5 minutes.

----
NAME                                 READY     STATUS      RESTARTS   AGE
postgresql-1-zktk2                   1/1       Running     0           3m
rails-pgsql-persistent-1-build       0/1       Completed   0           4m
rails-pgsql-persistent-1-sztht       1/1       Running     0           1m
----

Once the deployment is complete you can now test the application and the persistent storage CephRBD volume.

----
oc get route
NAME                     HOST/PORT                                                                              PATH      SERVICES                 PORT      TERMINATION   WILDCARD
rails-pgsql-persistent   rails-pgsql-persistent-my-database-app.apps.xxxxxxxxxxx.aws.testdrive.openshift.com
----

Results of this command will be similar to above. Replace `xxxxxxxxxxx` with your unique value and copy the URL to your browser to create articles.

----
http://rails-pgsql-persistent-my-database-app.apps.xxxxxxxxxxx.aws.testdrive.openshift.com/articles
----

Enter the username/password to create articles and comments. The articles and comments are saved in a PostgreSQL database which stores its table spaces on a CephRBD volume provided by OCS.

----
username: openshift
password: secret
----

Lets now take another look at the replicapool created by the OCP storageclass. Log into the toolbox pod again.

----
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
----

Run the same Ceph commands as before the application deployment and compare to results in prior section. Notice the number of objects in replicapool now.

----
ceph df
rados df
rados -p replicapool ls | grep pvc
exit
----

Validate the OCP PVC is the same name as the PVC object in the replicapool.

----
oc get pvc
----

== Using Rook to Upgrade Ceph

In this section you will upgrade Ceph from from Luminous to Mimic using the Rook operator. The first thing we need to do is update the cluster CRD with the mimic image name and version.

----
oc project rook-ceph
oc edit cephcluster rook-ceph
----

Modify the Ceph version in the cluster CRD. Using `oc edit` is the same as using editing tool `vi`.

----
spec:
  cephVersion:
    image: ceph/ceph:v12.2.11-20190201
----

To this version new below. Make sure to save `:wq!` the changes before exiting.

----
spec:
  cephVersion:
    image: ceph/ceph:v13.2.4-20190109
----

Once the change to the ceph version is saved as shown above, the MONs, MGR, and OSD pods will be restarted. This could take 5 minutes.

----
watch oc get pods

NAME                                         READY         STATUS      RESTARTS   AGE
rook-ceph-mgr-a-7448c76545-4kqjf             1/1	   Running     0          3m
rook-ceph-mon-a-54d7966c5-5xrz7              1/1	   Running     0          4m
rook-ceph-mon-b-7f6c449744-d8dbj             1/1	   Running     0          4m
rook-ceph-mon-c-5d666798c5-8q96l             1/1	   Running     0          4m
rook-ceph-osd-0-59cc694647-cpptn             1/1	   Running     0          5s
rook-ceph-osd-1-78b56fc845-bmw4h             1/1	   Running     0          3s
rook-ceph-osd-2-f78c88c48-w7mst              1/1	   Running     0          2s
----

Now let's check the version of Ceph to see if it is upgraded. First we need to login to the toolbox pod.

----
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
----

Running the `ceph versions` command shows each of the Ceph daemons have been upgraded to Mimic. Run other Ceph commands to satisfy yourself (e.g., ceph status) the system is healthy after the upgrade. You might even want to go back to the URL used for the Rails+PostgreSQL application and save a few more articles to make sure applications using Ceph storage are still working.

----
ceph versions
{
    "mon": {
        "ceph version 13.2.4 (b10be4d44915a4d78a8e06aa31919e74927b142e) mimic (stable)": 3
    },
    "mgr": {
        "ceph version 13.2.4 (b10be4d44915a4d78a8e06aa31919e74927b142e) mimic (stable)": 1
    },
    "osd": {
        "ceph version 13.2.4 (b10be4d44915a4d78a8e06aa31919e74927b142e) mimic (stable)": 3
    },
    "mds": {},
    "overall": {
        "ceph version 13.2.4 (b10be4d44915a4d78a8e06aa31919e74927b142e) mimic (stable)": 7
    }
}

exit
----

== Adding storage to the Ceph Cluster

In this section you will add more storage to the cluster by increasing the number of OSDs per OCP nodes using spare storage devices on the nodes.

Before we make any changes to the cluster CRD let's see what storage is available on our OCP nodes. It is important that the available storage be a raw block device with no formatting or labeling. There should be a storage device availalbe, all of the same size, on the same nodes that were originally used.

----
oc get nodes -l role=storage-node
NAME                                          STATUS    ROLES     AGE       VERSION
node04.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
node05.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
node06.internal.aws.testdrive.openshift.com   Ready     compute   1h        v1.11.0+d4cacc0
----

To check the storage SSH to one of the OCP nodes that have the role=storage-node.

----
ssh node04.internal.aws.testdrive.openshift.com
----

Check the storage devices on node. You can see that 50GB storage device `xvdd` is used already by Ceph. Storage device `xvde`, also 50GB, is not used yet.

----
[cloud-user@node04 ~]$ lsblk
NAME                                                                    MAJ:MIN RM SIZE RO TYPE
...omitted...
xvdd                                                                    202:48   0  50G  0 disk
ceph--dbcea47d--6fa4--467e--ad5e--158d0032978f-osd--data--a2a40ce7--b366--48c4--a2d6--2aac94def755
                                                                        253:1    0  50G  0 lvm
xvde                                                                    202:64   0  50G  0 disk
----

Also /dev/xvde looks to be a raw block device with no labels, which is required.

----
[cloud-user@node04 ~]$ sudo fdisk -l /dev/xvde

Disk /dev/xvde: 53.7 GB, 53687091200 bytes, 104857600 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

[cloud-user@node04 ~]$ exit
----

After validating the available storage for increasing the number of OSDs we are ready to modify the cluster CRD and add an additional storage device, `xvde`.

To make this easier we have created a new cluster CRD yaml file that has the new storage device already added correctly instead of editing the cluster CRD using `oc edit`.

----
curl -O https://raw.githubusercontent.com/travisn/rook/openshift-commons-demo/workshop/cluster_with_xvde.yaml
----

Take a look at the new cluster CRD yaml file.

----
cat cluster_with_xvde.yaml
...omitted...
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
    - name: "node04.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
      - name: "xvde"
    - name: "node05.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
      - name: "xvde"
    - name: "node06.internal.aws.testdrive.openshift.com"
      devices:
      - name: "xvdd"
      - name: "xvde"
----

Now add the additional storage device `xvde` to each node above.

----
oc apply -f cluster_with_xvde.yaml
----

Once this new defiition is applied the 3 additonal rook-ceph-osd pods will start. Wait until they are in a Running state before proceeding.

----
watch oc get pods
NAME                                       READY     STATUS      RESTARTS   AGE
rook-ceph-mgr-a-7448c76545-4kqjf           1/1       Running     0          1h
rook-ceph-mon-a-54d7966c5-5xrz7            1/1       Running     0          1h
rook-ceph-mon-b-7f6c449744-d8dbj           1/1       Running     0          1h
rook-ceph-mon-c-5d666798c5-8q96l           1/1       Running     0          1h
rook-ceph-osd-0-59cc694647-cpptn           1/1       Running     0          1h
rook-ceph-osd-1-78b56fc845-bmw4h           1/1       Running     0          1h
rook-ceph-osd-2-f78c88c48-w7mst            1/1       Running     0          1h
rook-ceph-osd-3-8d5b4f687-glwnf            1/1       Running     0          1m
rook-ceph-osd-4-85f44cc959-9tdhr           1/1       Running     0          1m
rook-ceph-osd-5-7444994795-ptnqz           1/1       Running     0          1m
----

Let's now validate that Ceph is healthy and has the additional storage. We again login to the toolbox.

----
kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') bash
----

And run Ceph commands to see the new OSDs.

----
ceph osd status
+----+---------------------------------------------+-------+-------+--------+---------+--------+
| id |                     host                    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
+----+---------------------------------------------+-------+-------+--------+---------+--------+
| 0  | node05.internal.aws.testdrive.openshift.com | 1025M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 1  | node04.internal.aws.testdrive.openshift.com | 1025M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 2  | node06.internal.aws.testdrive.openshift.com | 1025M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 3  | node04.internal.aws.testdrive.openshift.com | 1025M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 4  | node05.internal.aws.testdrive.openshift.com | 1025M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
| 5  | node06.internal.aws.testdrive.openshift.com | 1025M | 48.9G |    0   |     0   |    0   |     0   | exists,up |
+----+---------------------------------------------+-------+-------+--------+---------+--------+
----


----
ceph osd tree
ID CLASS WEIGHT  TYPE NAME                                            STATUS REWEIGHT PRI-AFF
-1       0.29279 root default
-5       0.09760     host node04-internal-aws-testdrive-openshift-com
 1   ssd 0.04880         osd.1                                            up  1.00000 1.00000
 3   ssd 0.04880         osd.3                                            up  1.00000 1.00000
-3       0.09760     host node05-internal-aws-testdrive-openshift-com
 0   ssd 0.04880         osd.0                                            up  1.00000 1.00000
 4   ssd 0.04880         osd.4                                            up  1.00000 1.00000
-7       0.09760     host node06-internal-aws-testdrive-openshift-com
 2   ssd 0.04880         osd.2                                            up  1.00000 1.00000
 5   ssd 0.04880         osd.5                                            up  1.00000 1.00000
----


----
ceph status
...omitted...
   osd: 6 osds: 6 up, 6 in
...omitted
----

= The End

*Congratulations!* You reached the end of this Red Hat Summit 2019 Workshop!
