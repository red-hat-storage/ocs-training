= OpenShift Metro Disaster Recovery with Advanced Cluster Management
:toc:
:toclevels: 4
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:hide-uri-scheme:

== Overview

The intent of this guide is to detail the `Metro Disaster Recovery` (Metro DR) steps and commands necessary to be able to failover an application from one `OpenShift Container Platform` (OCP) cluster to another and then failback the same application to the original *primary cluster*. In this case the OCP clusters will be created or imported using *Red Hat Advanced Cluster Management* or `RHACM` and have *_distance limitations between the OCP clusters of less than 10ms RTT latency_*. 

The persistent storage for applications will be provided by an external *Red Hat Ceph Storage* (RHCS) cluster stretched between the two locations with the OCP instances connected to this storage cluster. An arbiter node with a storage monitor service will be required at a third location (different location than where OCP instances are deployed) to establish quorum for the RHCS cluster in the case of a site outage. This third location does not have distance limitations and can be 100+ RTT latency from the storage cluster connected to the OCP instances. 

This is a general overview of the Metro DR steps required to configure and execute `OpenShift Disaster Recovery` (ODR) capabilities using OpenShift Data Foundation (ODF) *v4.11* and `RHACM` *v2.5* across two distinct OCP clusters separated by distance. In addition to these two cluster called `managed` clusters, there is currently a requirement to have a third OCP cluster that will be the `Advanced Cluster Management` (ACM) `hub` cluster.

NOTE: These steps are considered Technical Preview in ODF 4.11 and are provided for POC (Proof of Concept) purposes. OpenShift `Metro DR` will be supported for production usage in a future ODF release.

[start=1]
. *Install the ACM operator on the hub cluster.* +
After creating the OCP hub cluster, install from OperatorHub the ACM operator. After the operator and associated pods are running, create the MultiClusterHub resource.
. *Create or import managed OCP clusters into ACM hub.* +
Import or create the two managed clusters with adequate resources for ODF (compute nodes, memory, cpu) using the RHACM console.
. *Install Red Hat Ceph Storage Stretch Cluster With Arbiter.* +
Properly set up a Ceph cluster deployed on two different datacenters using the stretched mode functionality.
. *Install ODF 4.11 on managed clusters.* +
Install ODF 4.11 on primary and secondary OCP managed clusters and connect both instances to the stretched Ceph cluster.
. *Install ODF MultiCluster Orchestrator Operator on the ACM hub cluster.* +
Install from OperatorHub on the ACM hub cluster the ODF MultiCluster Orchestrator Operator.
. *Configure SSL access between managed clusters if (if needed).* +
For each managed cluster extract the ingress certificate and inject into the alternate cluster for MCG object bucket secure access. 
. *Create the DRPolicy resource on the hub cluster.* +
Create the DRPolicy using the MetroDR configuration settings. DRPolicy is an API available after the ODR Hub Operator is installed.
. *Create the Sample Application namespace on the hub cluster.* +
Because the ODR Hub Operator APIs are namespace scoped, the sample application namespace must be created first.
. *Create the DRPlacementControl resource on the hub cluster.* +
DRPlacementControl is an API available after the ODR Hub Operator is installed. 
. *Create the PlacementRule resource on the hub cluster.* +
Placement rules define the target clusters where resource templates can be deployed.
. *Create the Sample Application using ACM console.* +
Use the sample app example from https://github.com/RamenDR/ocm-ramen-samples to create a busybox deployment for failover and failback testing.
. *Validate Sample Application deployment and alternate cluster replication* +
Using CLI commands on both managed clusters validate that the application is running and that the volume was replicated to the alternate cluster.
. *Failover Sample Application to secondary managed cluster.* +
After creating a network fence for the primary managed cluster, modify the application DRPlacementControl resource on the Hub Cluster, add the action of Failover and specify the failoverCluster to trigger the failover.
. *Failback Sample Application to primary managed cluster.* +
After removing the network fence for the primary managed cluster and rebooting worker nodes, modify the application DRPlacementControl resource on the Hub Cluster and change the action to Relocate to trigger a failback to the preferredCluster.
 
== Deploy and Configure ACM for Multisite connectivity

This installation method requires you have three OpenShift clusters that have network reachability between them. For the purposes of this document we will use this reference for the clusters:

* *Hub cluster* is where ACM, ODF Multisite-orchestrator and ODR Hub controllers are installed.
* *Primary managed cluster* is where ODF, ODR Cluster controller, and Applications are installed.
* *Secondary managed cluster* is where ODF, ODR Cluster controller, and Applications are installed.

=== Install ACM and MultiClusterHub

Find ACM in OperatorHub on the *Hub cluster* and follow instructions to install this operator.

.OperatorHub filter for Advanced Cluster Management
image::ACM-OperatorHub.png[OperatorHub filter for Advanced Cluster Management]

Verify that the operator was successfully installed and that the `MultiClusterHub` is ready to be installed.

.ACM Installed Operator
image::ACM-Installed-Operator.png[ACM Installed Operator]

Select `MultiClusterHub` and use either `Form view` or `YAML view` to configure the deployment and select `Create`. 

NOTE: Most *MultiClusterHub* deployments can use default settings in the `Form view`.

Once the deployment is complete you can logon to the ACM console using your OpenShift credentials.

First, find the *Route* that has been created for the ACM console:

[source,role="execute"]
----
oc get route multicloud-console -n open-cluster-management -o jsonpath --template="https://{.spec.host}/multicloud/clusters{'\n'}"
----

This will return a route similar to this one.

.Example Output:
----
https://multicloud-console.apps.perf3.example.com/multicloud/clusters
----

After logging in you should see your local cluster imported.

.ACM local cluster imported
image::ACM-local-cluster-import.png[ACM local cluster imported]

=== Import or Create Managed clusters

Now that ACM is installed on the `Hub cluster` it is time to either create or import the `Primary managed cluster` and the `Secondary managed cluster`. You should see selections (as in above diagram) for *Create cluster* and *Import cluster*. Chose the selection appropriate for your environment. After the managed clusters are successfully created or imported you should see something similar to below.

.ACM managed cluster imported
image::ACM-managed-clusters-import.png[ACM managed cluster imported]

== Red Hat Ceph Storage Installation

xref:rhcs-stretched-deploy.adoc[Red Hat Ceph Storage Stretch Cluster With Arbiter Deployment]

== OpenShift Data Foundation Installation

In order to configure storage replication between the two OCP clusters `OpenShift Data Foundation` (ODF) must be installed first on each managed cluster. ODF deployment guides and instructions are specific to your infrastructure (i.e. AWS, VMware, BM, Azure, etc.). 

After the ODF operators are installed, select *Create StorageSystem* and choose `Connect an external storage platform` and `Red Hat Ceph Storage` as shown below. Select *Next*.

.ODF Connect external storage
image::ODF-connect-external-storage.png[ODF Connect external storage]

Download the ceph-external-cluster-details-exporter.py python script and upload
it to you RHCS bootstrap node, the script needs to be run from a host with the
ceph admin key, in our example the hostname for the RHCS
bootstrap node that has the admin keys available is `ceph1`.

.ODF Download the RHCS script
image::ODF_download_script_external_storage.png[ODF download RHCS script]

The ceph-external-cluster-details-exporter.py python script will create a configuration file
with details for ODF to connect with the RHCS cluster.

Because we are
connecting two OCP clusters to the RHCS storage, you need to run the
ceph-external-cluster-details-exporter.py script two times, one per OCP cluster.

To see all configuration options available for the
ceph-external-cluster-details-exporter.py script run the following command:

[source,role="execute"]
----
python3 ceph-external-cluster-details-exporter.py --help
----

To know more about the External ODF deployment options, see
https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.11/html-single/deploying_openshift_data_foundation_in_external_mode/index#overview-of-deploying-in-external-mode_rhodf[ODF external mode deployment.]

At a minimum, we need to use the following three flags with the
ceph-external-cluster-details-exporter.py script:

* **--rbd-data-pool-name** : With the name of the RBD pool we created during RHCS deployment for OCP, in  our example, the pool is called `rbdpool`.
* **--rgw-endpoint** : With the RGW IP of the RGW daemon running on the same site as the OCP cluster, we are configuring.
* **--run-as-user** : With a different client name for each site.

Run the following command on the bootstrap node, ceph1, to Get the IP for the RGW endpoints in datacenter1 and datacenter2:

[source,role="execute"]
----
ceph orch ps | grep rgw.objectgw
----
.Example output.
----
rgw.objectgw.ceph3.mecpzm  ceph3  *:8080       running (5d)     31s ago   7w     204M        -  16.2.7-112.el8cp
rgw.objectgw.ceph6.mecpzm  ceph6  *:8080       running (5d)     31s ago   7w     204M        -  16.2.7-112.el8cp
----

[source,role="execute"]
----
host ceph3
host ceph6
----
.Example output.
----
ceph3.example.com has address 10.0.40.24
ceph6.example.com has address 10.0.40.66
----

Execute the ceph-external-cluster-details-exporter.py with the parameters configured for our first ocp managed cluster `cluster1`.

[source,role="execute"]
----
python3 ceph-external-cluster-details-exporter.py --rbd-data-pool-name rbdpool --rgw-endpoint 10.0.40.24:8080 --run-as-user client.odf.cluster1 > ocp-cluster1.json
----

Execute the ceph-external-cluster-details-exporter.py with the parameters configured for our first ocp managed cluster `cluster2`

[source,role="execute"]
----
python3 ceph-external-cluster-details-exporter.py --rbd-data-pool-name rbdpool --rgw-endpoint 10.0.40.66:8080 --run-as-user client.odf.cluster2 > ocp-cluster2.json
----



Save the two files generated in the bootstrap cluster(ceph1) ocp-cluster1.json and ocp-cluster2.json to your
local machine.
* Use the contents of file ocp-cluster1.json on the OCP console on cluster1 where external ODF is being deployed. 
* Use the contents of file ocp-cluster2.json on the OCP console on cluster2 where external ODF is being deployed. 

The next figure has an example for OCP cluster1.

.ODF Connection details for external storage
image::ODF-external-storage-details.png[Connection details for external storage]

Review the settings and then select *Create StorageSystem*.

.ODF Create StorageSystem
image::ODF-create-storagesystem.png[ODF Create StorageSystem]

You can validate the successful deployment of ODF on each managed OCP cluster with the following command:

[source,role="execute"]
----
oc get storagecluster -n openshift-storage ocs-external-storagecluster -o jsonpath='{.status.phase}{"\n"}'
----

And for the Multi-Cluster Gateway (MCG):

[source,role="execute"]
----
oc get noobaa -n openshift-storage noobaa -o jsonpath='{.status.phase}{"\n"}'
----

If the result is `Ready` for both queries on the *Primary managed cluster* and the *Secondary managed cluster* continue on to the next step.

NOTE: The successful installation of ODF can also be validated in the *OCP Web Console* by navigating to *Storage* and then *Data Foundation*. 

== Install ODF Multicluster Orchestrator Operator on Hub cluster

On the *Hub cluster* navigate to *OperatorHub* and filter for `ODF Multicluster Orchestrator`. Follow instructions to *Install* the operator into the project `openshift-operators`. 

Check to see the operator *Pod* is in a `Running` state.

[source,role="execute"]
----
oc get pods -n openshift-operators
----
.Example output.
----
NAME                                       READY   STATUS    RESTARTS   AGE

odfmo-controller-manager-f6fc95f7f-7wtjl   1/1     Running   0          4m14s
----

== Configure SSL access between S3 endpoints

These steps are necessary so that metadata can be stored on the alternate cluster in a Multi-Cloud Gateway (MCG) object bucket using a secure transport protocol and in addition the *Hub cluster* needs to verify access to the object buckets.

NOTE: If all of your OpenShift clusters are deployed using signed and valid set of certificates for your environment then this section can be skipped.

Extract the ingress certificate for the *Primary managed cluster* and save the output to `primary.crt`.

[source,role="execute"]
----
oc get cm default-ingress-cert -n openshift-config-managed -o jsonpath="{['data']['ca-bundle\.crt']}" > primary.crt
----

Extract the ingress certificate for the *Secondary managed cluster* and save the output to `secondary.crt`.

[source,role="execute"]
----
oc get cm default-ingress-cert -n openshift-config-managed -o jsonpath="{['data']['ca-bundle\.crt']}" > secondary.crt
----

Create a new YAML file `cm-clusters-crt.yaml` to hold the certificate bundle for both the *Primary managed cluster* and the *Secondary managed cluster*.

NOTE: There could be more or less than three certificates for each cluster as shown in this example file.

[source,yaml]
----
apiVersion: v1
data:
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    <copy contents of cert1 from primary.crt here>
    -----END CERTIFICATE-----
    
    -----BEGIN CERTIFICATE-----
    <copy contents of cert2 from primary.crt here>
    -----END CERTIFICATE-----
    
    -----BEGIN CERTIFICATE-----
    <copy contents of cert3 primary.crt here>
    -----END CERTIFICATE----
    
    -----BEGIN CERTIFICATE-----
    <copy contents of cert1 from secondary.crt here>
    -----END CERTIFICATE-----
    
    -----BEGIN CERTIFICATE-----
    <copy contents of cert2 from secondary.crt here>
    -----END CERTIFICATE-----
    
    -----BEGIN CERTIFICATE-----
    <copy contents of cert3 from secondary.crt here>
    -----END CERTIFICATE-----  
kind: ConfigMap
metadata:
  name: user-ca-bundle 
  namespace: openshift-config
----

This *ConfigMap* needs to be created on the *Primary managed cluster*, *Secondary managed cluster*, _and_ the *Hub cluster*.

[source,role="execute"]
----
oc create -f cm-clusters-crt.yaml
----
.Example output.
----
configmap/user-ca-bundle created
----

IMPORTANT: The *Hub cluster* needs to verify access to the object buckets using the *DRPolicy* resource. Therefore the same *ConfigMap*, `cm-clusters-crt.yaml`, needs to be created on the *Hub cluster*.

After all the `user-ca-bundle` *ConfigMaps* are created, the default *Proxy* `cluster` resource needs to be modified.

Patch the default *Proxy* resource on the *Primary managed cluster*, *Secondary managed cluster*, and the *Hub cluster*.
  
[source,role="execute"]
----
oc patch proxy cluster --type=merge  --patch='{"spec":{"trustedCA":{"name":"user-ca-bundle"}}}'
----
.Example output.
----
proxy.config.openshift.io/cluster patched
----


== Create DRPolicy on Hub cluster

ODR uses the *DRPolicy* resources on the ACM hub cluster to deploy, failover, and relocate, workloads across managed clusters. A *DRPolicy* requires a set of two DRClusters.
 

On the *Hub cluster* navigate to `Installed Operators` in the `openshift-dr-system` project and select `OpenShift DR Hub Operator`. You should see three available APIs, *DRCluster*,  *DRPolicy* and *DRPlacementControl*.

.ODR Hub cluster APIs
image::ODR-DRPolicy-API.png[ODR Hub cluster APIs]

*Create instance* for *DRPolicy* and then go to *YAML view*.

.DRPolicy create instance
image::ODR-DRPolicy-create-instance.png[DRPolicy create instance]

Save the following YAML to filename drpolicy.yaml after replacing *<cluster1>* and *<cluster2>* with the correct names of your DRCluster CRs.

NOTE: There is no need to specify a namespace to create this resource because `DRPolicy` is a cluster-scoped resource.

[source,yaml]
----
apiVersion: ramendr.openshift.io/v1alpha1
kind: DRPolicy
metadata:
  name: odr-policy
spec:
  drClusters:
    - <cluster1>
    - <cluster2>
----    

Now create the `DRPolicy` resource by copying the contents of your unique `drpolicy.yaml` file into the `YAML view` (completely replacing original content). Select *Create* at the bottom of the `YAML view` screen.

You can also create this resource using CLI:

[source,role="execute"]
----
oc create -f drpolicy.yaml
----
.Example output.
----
drpolicy.ramendr.openshift.io/odr-policy created
----

To validate that the *DRPolicy* is created successfully and that the MCG object buckets can be accessed using the *Secrets* created earlier, run this command on the *Hub cluster*.

[source,role="execute"]
----
oc get drpolicy odr-policy -n openshift-dr-system -o jsonpath='{.status.conditions[].reason}{"\n"}'
----
.Example output.
----
Succeeded
----

== Enable Automatic Install of ODR Cluster operator

Once the *DRPolicy* is created successfully the `ODR Cluster operator` can be installed on the *Primary managed cluster* and *Secondary managed cluster* in the `openshift-dr-system` namespace.

This is done by editing the `ramen-hub-operator-config` *ConfigMap* on the *Hub cluster* and make `deploymentAutomationEnabled=true` (change false to true).

[source,role="execute"]
----
oc edit configmap ramen-hub-operator-config -n openshift-dr-system
----
[source,yaml]
----
apiVersion: v1
data:
  ramen_manager_config.yaml: |
    apiVersion: ramendr.openshift.io/v1alpha1
    [...]
    drClusterOperator:
      deploymentAutomationEnabled: true  ## <-- Modify to true if needed
      catalogSourceName: redhat-operators
      catalogSourceNamespaceName: openshift-marketplace
      channelName: stable-4.11
      clusterServiceVersionName: odr-cluster-operator.v4.11.0
      namespaceName: openshift-dr-system
      packageName: odr-cluster-operator
[...]
----

To validate that the installation was successful on the *Primary managed cluster* and the *Secondary managed cluster* do the following command:

[source,role="execute"]
----
oc get csv,pod -n openshift-dr-system
----
.Example output.
----
NAME                                                                      DISPLAY                         VERSION   REPLACES   PHASE
clusterserviceversion.operators.coreos.com/odr-cluster-operator.v4.11.0   Openshift DR Cluster Operator   4.11.0               Succeeded

NAME                                             READY   STATUS    RESTARTS   AGE
pod/ramen-dr-cluster-operator-5564f9d669-f6lbc   2/2     Running   0          5m32s
----

You can also go to *OperatorHub* on each of the managed clusters and look to see the `OpenShift DR Cluster Operator` is installed.

.ODR Cluster Operator
image::ODR-Cluster-operator.png[ODR Cluster Operator]

== Create S3 Secrets on Managed clusters

The MCG object bucket *Secrets* were created and stored on the *Hub cluster*.

[source,role="execute"]
----
oc get secrets -n openshift-dr-system | grep Opaque
----
.Example output.
----
odr-s3secret-primary                 Opaque                                2      39m
odr-s3secret-secondary               Opaque                                2      39m
----

These *Secrets* need to be copied to the *Primary managed cluster* and the *Secondary managed cluster*. 

The S3 secret YAML format for the *Primary managed cluster* is similar to the following: 

[source,yaml]
----
apiVersion: v1
data:
  AWS_ACCESS_KEY_ID: <primary cluster base-64 encoded access key>
  AWS_SECRET_ACCESS_KEY: <primary cluster base-64 encoded secret access key>
kind: Secret
metadata:
  name: odr-s3secret-primary
  namespace: openshift-dr-system
----

Create this secret on the *Primary managed cluster* and the *Secondary managed cluster*.

[source,role="execute"]
----
oc create -f odr-s3secret-primary.yaml
----
.Example output.
----
secret/odr-s3secret-primary created
----

The S3 secret YAML format for the *Secondary managed cluster* is similar to the following:

[source,yaml]
----
apiVersion: v1
data:
  AWS_ACCESS_KEY_ID: <secondary cluster base-64 encoded access key>
  AWS_SECRET_ACCESS_KEY: <secondary cluster base-64 encoded secret access key>
kind: Secret
metadata:
  name: odr-s3secret-secondary
  namespace: openshift-dr-system
----

Create this secret on the *Primary managed cluster* and the *Secondary managed cluster*.

[source,role="execute"]
----
oc create -f odr-s3secret-secondary.yaml
----
.Example output.
----
secret/odr-s3secret-secondary created
----

IMPORTANT: The values for the access key and secret key must be *base-64 encoded*. The encoded values for the keys were retrieved in a prior section. 

== Create Sample Application for DR testing

In order to test failover from the *Primary managed cluster* to the *Secondary managed cluster* and back again we need a simple application. The sample application used for this example with be `busybox`. 

The first step is to create a namespace or project on the *Hub cluster* for `busybox` sample application.

[source,role="execute"]
----
oc new-project busybox-sample
----

NOTE: A different project name other than `busybox-sample` can be used if desired. Make sure when deploying the sample application via the ACM console to use the same project name as what is created in this step.

=== Create DRPlacementControl resource

*DRPlacementControl* is an API available after the `ODR Hub Operator` is installed on the *Hub cluster*. It is broadly an ACM PlacementRule reconciler that orchestrates placement decisions based on data availability across clusters that are part of a *DRPolicy*.

On the *Hub cluster* navigate to `Installed Operators` in the `busybox-sample` project and select `ODR Hub Operator`. You should see two available APIs, *DRPolicy* and *DRPlacementControl*. 

.ODR Hub cluster APIs
image::ODR-DRPolicy-API.png[ODR Hub cluster APIs]

*Create instance* for *DRPlacementControl* and then go to *YAML view*. Make sure the `busybox-sample` namespace is selected at the top.

.DRPlacementControl create instance
image::ODR-DRPlacementControl-create-instance.png[DRPlacementControl create instance]

Save the following YAML (below) to filename busybox-drpc.yaml after replacing *<cluster1>* with the correct name of your managed cluster in *ACM*. 

[source,yaml]
----
apiVersion: ramendr.openshift.io/v1alpha1
kind: DRPlacementControl
metadata:
  labels:
    app: busybox-sample
  name: busybox-drpc
spec:
  drPolicyRef:
    name: odr-policy
  placementRef:
    kind: PlacementRule
    name: busybox-placement
  preferredCluster: <cluster1>
  pvcSelector:
    matchLabels:
      appname: busybox
----

Now create the *DRPlacementControl* resource by copying the contents of your unique `busybox-drpc.yaml` file into the `YAML view` (completely replacing original content). Select *Create* at the bottom of the `YAML view` screen.

You can also create this resource using CLI.

IMPORTANT: This resource must be created in the `busybox-sample` namespace (or whatever namespace you created earlier).

[source,role="execute"]
----
oc create -f busybox-drpc.yaml -n busybox-sample
----
.Example output.
----
drplacementcontrol.ramendr.openshift.io/busybox-drpc created
----

=== Create PlacementRule resource

Placement rules define the target clusters where resource templates can be deployed. Use placement rules to help you facilitate the multicluster deployment of your applications. 

Save the following YAML (below) to filename busybox-placementrule.yaml.

[source,yaml]
----
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  labels:
    app: busybox-sample
  name: busybox-placement
spec:
  clusterConditions:
  - status: "True"
    type: ManagedClusterConditionAvailable
  clusterReplicas: 1
  schedulerName: ramen
----

Now create the *PlacementRule* resource for the `busybox-sample` application.

IMPORTANT: This resource must be created in the `busybox-sample` namespace (or whatever namespace you created earlier).

[source,role="execute"]
----
oc create -f busybox-placementrule.yaml -n busybox-sample
----
.Example output.
----
placementrule.apps.open-cluster-management.io/busybox-placement created
----

=== Creating Sample Application using ACM console

Start by loggin into the ACM console using your OpenShift credentials if not already logged in.

[source,role="execute"]
----
oc get route multicloud-console -n open-cluster-management -o jsonpath --template="https://{.spec.host}/multicloud/applications{'\n'}"
----

This will return a route similar to this one.

.Example Output:
----
https://multicloud-console.apps.perf3.example.com/multicloud/applications
----

After logging in select *Create application* in the top right and choose *Subscription*.

.ACM Create application
image::ACM-Create-application.png[ACM Create application]

Fill out the top of the `Create an application` form as shown below and select repository type *Git*.

.ACM Application name and namespace
image::ACM-application-form1.png[ACM Application name and namespace]

The next section to fill out is below the *Git* box and is the repository URL for the sample application, the *github* branch and path to resources that will be created, the `busybox` *Pod* and *PVC*. 

NOTE: *Sample application repository* https://github.com/RamenDR/ocm-ramen-samples. Branch is `main` and path is `busybox-odr-metro`. 

.ACM application repository information
image::ACM-application-form2a-metro.png[ACM application repository information]

Scroll down in the form until you see *Select an existing placement configuration* and then put your cursor in the box below. You should see the *PlacementRule* created in prior section. Select this rule.

.ACM application placement rule 
image::ACM-application-form3.png[ACM application placement rule]

After selecting available rule then select *Save* in the upper right hand corner.

On the follow-on screen scroll to the bottom. You should see that there are all *Green* checkmarks on the application topology.

.ACM application successful topology view
image::ACM-application-successfull.png[ACM application successful topology view]

NOTE: To get more information click on any of the topology elements and a window will appear to right of the topology view.

=== Validating Sample Application deployment and replication

Now that the `busybox` application has been deployed to your *preferredCluster* (specified in the `DRPlacementControl`) the deployment can be validated.

Logon to your managed cluster where `busybox` was deployed by ACM. This is most likely your *Primary managed cluster*.

[source,role="execute"]
----
oc get pods,pvc -n busybox-sample
----
.Example output.
----
NAME          READY   STATUS    RESTARTS   AGE
pod/busybox   1/1     Running   0          6m

NAME                                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
persistentvolumeclaim/busybox-pvc   Bound    pvc-a56c138a-a1a9-4465-927f-af02afbbff37   1Gi        RWO            ocs-storagecluster-ceph-rbd   6m
----

To validate that the replication resource is also created for the `busybox` *PVC* do the following:

[source,role="execute"]
----
oc get volumereplicationgroup -n busybox-sa/mple
----
.Example output.
----
NAME                                                       AGE
volumereplicationgroup.ramendr.openshift.io/busybox-drpc   6m
----

=== Deleting the Sample Application

Deleting the `busybox` application can be done using the ACM console. Navigate to *Applications* and then find the application to be deleted (busybox in this case).

NOTE: The instructions to delete the sample application should not be executed until the failover and failback (relocate) testing is completed and you want to remove this application from RHACM and from the managed clusters.

.ACM delete busybox application
image::ACM-application-delete.png[ACM delete busybox application]

When *Delete application* is selected a new screen will appear asking if the `application related resources` should also be deleted. Make sure to `check` the box to delete the `Subscription` and `PlacementRule`.

.ACM delete busybox application resources
image::ACM-application-delete-resources.png[ACM delete busybox application resources]

Select *Delete* in this screen. This will delete the `busybox` application on the *Primary managed cluster* (or whatever cluster the application was running on).

In addition to the resources deleted using the ACM console, the `DRPlacementControl` must also be deleted immediately after deleting the `busybox` application. Logon to the OpenShift Web console for the *Hub cluster*. Navigate to `Installed Operators` for the project `busybox-sample`. Choose `OpenShift DR Hub Operator` and the *DRPlacementControl*.

.Delete busybox application DRPlacementControl
image::ODR-DRPlacementControl-delete.png[Delete busybox application DRPlacementControl]

Select *Delete DRPlacementControl*. 

NOTE: If desired, the `DRPlacementControl` resource can also be deleted in the application namespace using CLI.

NOTE: This process can be used to delete any application with a DRPlacementControl resource.

== Add annotations required for fencing operations

Ramen operator creates NetworkFencing CRs on the managedclusters as part of automated fencing operations. The NetworkFencing CR requires details about the storage CSI driver and that needs to be provided to Ramen through annotations on the DRCluster CR. Add the following annotations to all the DRCluster CRs.

----
drcluster.ramendr.openshift.io/storage-clusterid: openshift-storage
drcluster.ramendr.openshift.io/storage-driver: openshift-storage.rbd.csi.ceph.com
drcluster.ramendr.openshift.io/storage-secret-name: rook-csi-rbd-provisioner
drcluster.ramendr.openshift.io/storage-secret-namespace: openshift-storage
----

== Application Failover between managed clusters

This section will detail how to failover the `busybox` sample application. The failover method for `Metro Disaster Recovery` is application based. Each application that is to be protected in this manner must have a corresponding *DRPlacementControl* resource and a *PlacementRule* resource created in the application namespace as shown in the <<Create Sample Application for DR testing>> section.

=== Enable Fencing 

In order to failover the OpenShift cluster where the application is currently running all applications must be `fenced` from communicating with the external *ODF* storage. This is required to prevent simultaneous writes to the same persistent volume from both managed clusters. 


Edit the DRCluster resource from the above examples, edit `drcluster1.yaml` to fence your managed cluster  

[source,role="execute"]
----
oc edit -f drcluster1.yaml
----

[source,yaml]
----
apiVersion: ramendr.openshift.io/v1alpha1
kind: DRCluster
metadata:
  name: <cluster1>
spec:
  region: <string_value>
  s3ProfileName: <s3-for-cluster>
  cidrs:
    -  <IP_Address1>/32
    -  <IP_Address2>/32
    -  <IP_Address3>/32
    -  <IP_Address4>/32
    -  <IP_Address5>/32
    -  <IP_Address6>/32
  clusterFence: Fenced    # here is the change
----

CAUTION: Once the  managed cluster is fenced, *_ALL_* communication from applications to the *ODF* storage will fail and some *Pods* will be in an unhealthy state (e.g. CreateContainerError, CrashLoopBackOff) on the cluster that is now `fenced`.

Now validate the fencing status in the Hub cluster for the Primary managed cluster.

[source,role="execute"]
----
oc get drcluster.ramendr.openshift.io cluster1 -n openshift-dr-system -o jsonpath='{.status.conditions[].reason}{"\n"}'
----
.Example output.
----
Succeeded
----

=== Modify DRPlacementControl to failover

To failover requires modifying the *DRPlacementControl* YAML view. On the *Hub cluster* navigate to `Installed Operators` and then to `Openshift DR Hub Operator`. Select *DRPlacementControl* as show below.

.DRPlacementControl busybox instance
image::ODR-DRPlacementControl-instance.png[DRPlacementControl busybox instance]

Select `drpc-busybox` and then the YAML view. Add the `action` and `failoverCluster` as shown below. The `failoverCluster` should be the *ACM* cluster name for the *Secondary managed cluster*.

.DRPlacementControl add action Failover
image::ODR-DRPlacementControl-failover-metro.png[DRPlacementControl add action Failover]

Select *Save*.

In the `failoverCluster` specified in the YAML file (i.e., ocp4perf2), see if the application `busybox` is now running in the *Secondary managed cluster* using the following command:

[source,role="execute"]
----
oc get pods,pvc -n busybox-sample
----
.Example output.
----
NAME          READY   STATUS    RESTARTS   AGE
pod/busybox   1/1     Running   0          35s

NAME                                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
persistentvolumeclaim/busybox-pvc   Bound    pvc-79f2a74d-6e2c-48fb-9ed9-666b74cfa1bb   5Gi        RWO            ocs-storagecluster-ceph-rbd   35s
----

Next, using the same command check if `busybox` is running in the *Primary managed cluster*. The `busybox` application should no longer be running on this managed cluster.

[source,role="execute"]
----
oc get pods,pvc -n busybox-sample
----
.Example output.
----
No resources found in busybox-sample namespace.
----

== Application Failback between managed clusters

A failback operation is very similar to failover. The failback is application based and uses the *DRPlacementControl* to trigger the failback


=== Disable Fencing

Before a failback or relocate action can be successful the *DRCluster* for the *Primary managed cluster* must be unfenced.

==== Modify DRCluster to Unfenced

Edit the *DRCluster* on the *Hub cluster* and change `clusterFence` value from `Fenced` to `Unfenced`.

[source,role="execute"]
----
oc edit drcluster1.yaml
----

[source,yaml]
----
apiVersion: ramendr.openshift.io/v1alpha1
kind: DRCluster
metadata:
  name: cluster1
spec:
  region: <string_value>
  s3ProfileName: <s3-for-cluster>
  cidrs:
    -  <IP_Address1>/32
    -  <IP_Address2>/32
    -  <IP_Address3>/32
    -  <IP_Address4>/32
    -  <IP_Address5>/32
    -  <IP_Address6>/32
  clusterFence: Unfenced   # here is the change
----

.Example output.
----
drcluster.ramendr.openshift.io/cluster1 edited
----

Now validate the *DRCluster* status in the *Hub cluster* that has changed to `Unfenced` for the *Primary managed cluster*.

[source,role="execute"]
----
oc get drcluster.ramendr.openshift.io cluster1 -n openshift-dr-system -o jsonpath='{.status.conditions[].reason}{"\n"}'
----
.Example output.
----
Succeeded
----

==== Reboot OCP nodes that were Fenced

This step is required because some application *Pods* on the prior `fenced` cluster, in this case the *Primary managed cluster*, are in an unhealthy state (e.g. CreateContainerError, CrashLoopBackOff). This can be most easily fixed by *rebooting all worker OpenShift nodes* one at a time.

After all OpenShift nodes are rebooted and again in a `Ready` status, verify all *Pods* are in a healthy state by running this command on the *Primary managed cluster*. The output for this query should be zero *Pods*. 

NOTE: The *OpenShift Web Console* dashboards and *Overview* can also be used to assess the health of applications and the external storage. The detailed *ODF* dashboard is found by navigating to `Storage` -> `Data Foundation`.

[source,role="execute"]
----
oc get pods -A | egrep -v 'Running|Completed'
----
.Example output.
----
NAMESPACE                                          NAME                                                              READY   STATUS      RESTARTS       AGE
----

IMPORTANT: If there are *Pods* still in an unhealthy status because of severed storage communication, troubleshoot and resolve before continuing. Because the storage cluster is external to OpenShift, it also has to be properly recovered after a site outage for OpenShift applications to be healthy.

=== Modify DRPlacementControl to failback

To failback requires modifying the *DRPlacementControl* YAML view. On the *Hub cluster* navigate to `Installed Operators` and then to `Openshift DR Hub Operator`. Select *DRPlacementControl* as show below.

.DRPlacementControl busybox instance
image::ODR-DRPlacementControl-instance.png[DRPlacementControl busybox instance]

Select `drpc-busybox` and then the YAML form. Modify the `action` to `Relocate` as shown below.

.DRPlacementControl modify action to Relocate
image::ODR-DRPlacementControl-failback-metro.png[DRPlacementControl modify action to Relocate]

Select *Save*.

Check if the application `busybox` is now running in the *Primary managed cluster* using the following command. The failback is to the `preferredCluster` which should be where the application was running before the failover operation.

[source,role="execute"]
----
oc get pods,pvc -n busybox-sample
----
.Example output.
----
NAME          READY   STATUS    RESTARTS   AGE
pod/busybox   1/1     Running   0          60s

NAME                                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
persistentvolumeclaim/busybox-pvc   Bound    pvc-79f2a74d-6e2c-48fb-9ed9-666b74cfa1bb   5Gi        RWO            ocs-storagecluster-ceph-rbd   61s
----

Next, using the same command, check if `busybox` is running in the *Secondary managed cluster*. The `busybox` application should no longer be running on this managed cluster.

[source,role="execute"]
----
oc get pods,pvc -n busybox-sample
----
.Example output.
----
No resources found in busybox-sample namespace.
----
