= Metro Multi-Cluster disaster recovery
//:toc: right
//:toclevels: 3
:icons: font
:source-highlighter: pygments
:source-language: shell
:numbered:
:imagesdir: ../docs/imgs/
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:

== Introduction

The intent of this guide is to detail the different steps and commands required to achieve the deployment
of a Multi Red Hat OpenShift Container Platform Cluster highly available architecture backed by an external
Red Hat Ceph Storage cluster. This setup is referred to as the Red Hat OpenShift Data Foundation (ODF)
Multi-Cluster Metro DR Disaster Recovery solution. This procedure allows you to failover an application
from one `Red Hat OpenShift Container Platform` (OCP) cluster to another and then failback the same application
to the original OCP cluster.

=== Prerequisites

The necessary components are two OCP 4.6 (or greater) clusters, `OpenShift Data Foundation` (ODF) installed
on both OCP clusters. ODF version *4.7* (or greater) is required. In order to replicate the Kubernetes
resources (pods, services, routes, etc.) from one cluster to another, this guide will make use of the Velero
`Backup` and `Restore` APIs exposed via the OCP community operator `OpenShift APIs for Data Protection` or `OADP`.

You must have access to a Red Hat Ceph Storage (RHCS) cluster.

=== High level procedure 

IMPORTANT: These steps are considered Development Preview in ODF 4.7 and 4.8, and are provided for POCs purposes.
They will be supported for production usage in a later ODF release.

[start=1]
. *Install ODF 4.7.* +
Install ODF 4.7 on primary and secondary OCP clusters and validate deployment.
. *Configure RHCS cephx authentication.* +
There is currently no automatic fencing of an OCP cluster in case it becomes unavailable thus preventing a cluster from accessing a PV. As a workaround each cluster is configured to access the cluster with a specific `cephx` username that will be disabled during the failover to another cluster or enabled when falling back to the original cluster.
. *Read Affinity Configuration.* +
This step is recommended only if you are in an active-active configuration and each OCP cluster uses a dedicated RBD pool. This configuration is recommended to customize the RHCS CRUSH configuration to make sure each clusters issues its read IO operations against a set of RHCS OSDs located in the same network availability zone as the OCP cluster.
. *Configure ODF CSI driver.* +
The CSI driver has to be configured to add some metadata to each PV created to preserve the original characteristics of the volume when accessed by the application restarted in another OCP cluster.
. *Storage Class Customizaton.* +
Specific storage classes are created or existing storage classes are modified to use the specific `cephx` username created for each cluster and each storage class requires the `retainPolicy` to be set to `Retain`. The specifc storage classes must have the same name in each OCP cluster.
. *Deploy Sample Application.* +
In the example, we will create a Sample Application which will use a single PVC claimed from the StorageClass mirror.
. *Install OADP (OpenShift API for Data Protection).* +
Using OperatorHub, install OADP on both OCP clusters. We will use OADP for copying target application metadata resources (Kubernetes CRs) from the primary to the secondary OCP cluster.
. *Backup OpenShift resources to S3 target.* +
Using the Backup API from OADP, we will backup all Kubernetes resources (CRs) for the Sample Application on the primary cluster to a S3 compatible object bucket. 
. *Simulate Primary OCP cluster failure event.* +
In our example, we will simulate a failure event simply by scaling the deployment(s) for our Sample Application to zero. This makes the application on the primary cluster unavailable.
. *Fence Primary Cluster.* +
Change the `cephx` capabilities for the user used by the Primary cluster and shutdown all nodes in the primary cluster to prevent any access to the volumes from the Primary OCP cluster.
. *Restore OpenShift resources from S3 target*. +
Using OADP and the Restore API copy all Kubernetes resources for the `Sample Application` from the `S3` combatible object bucket to the secondary cluster. The Backup and Restore could be scheduled to run at a desired frequency to ensure that the secondary cluster always has the most recent metadata from the applications targeted for failover on the primary cluster.
. *Verify application availability on the secondary cluster.* +
Verify the Sample Application now is operational on the secondary cluster and that new data can be saved.


=== Reference Architecture

The diagram below illustrates the global architecture of the solution.

.Multi-Cluster Metro DR General Architecture
image::ODF-4.7-OCP4-multicluster-dr-architecture.png[Multi-Cluster General Architecture]

IMPORTANT: The external cluster can be a cluster with Arbiter mode
enabled or a regular 3 failure domain cluster.

=== Solution Guide Architecture

The diagram below illustrates the specific details of the setup used for this solution
guide.

.Solution Guide Architecture
image::ODF-4.7-OCP4-multicluster-dr-lab.png[Soution Guide Architecture]

CAUTION: This rudimentary architecture was chosen to simplify demonstrations and limit
the cost of the test environment. This is NOT a supported configuration.

== Solution Prerequisites

The following technical prerequisites must be met:

* Red Hat Ceph Storage cluster
** Ports to be opened
*** Monitors: 6789 and 3300
*** OSDs: 6800-7100
*** Prometheus: 9283
* Red Hat OpenShift Container Platform
** Two OCP clusters
*** Network connectivity from each to external RHCS cluster
*** ODF deployed in external mode (see xref:ocs4-metro-multi-rhcs-aws.adoc#_odf_external_cluster_aws_based[procedure])
*** Same ODF storage classes exists in each OCP cluster
*** ODF shares the same underlying pools
**** No specific action needed
*** ODF do not share the same underlying pools
**** Setup read affinity for each OCP cluster https://github.com/red-hat-storage/read-affinty-scripts[here]

For this procedure we will refer to the clusters as:

* OCP-A (Active OCP *primary cluster*)
* OCP-DR (Standby OCP *secondary cluster*)

== Verify ODF Is Deployed

On OCP-A

[source,shell]
----
oc get pods -n openshift-storage
----
.Example
----
$ oc get pods -n openshift-storage
NAME                                            READY   STATUS    RESTARTS   AGE
csi-cephfsplugin-296jn                          3/3     Running   0          2m1s
csi-cephfsplugin-c5ll4                          3/3     Running   0          2m1s
csi-cephfsplugin-provisioner-76b7c894b9-8kq7m   6/6     Running   0          2m1s
csi-cephfsplugin-provisioner-76b7c894b9-jnf9j   6/6     Running   0          2m1s
csi-cephfsplugin-wk8sp                          3/3     Running   0          2m1s
csi-rbdplugin-9scp7                             3/3     Running   0          2m2s
csi-rbdplugin-provisioner-5866f86d44-dghsc      6/6     Running   0          2m2s
csi-rbdplugin-provisioner-5866f86d44-sb9hp      6/6     Running   0          2m2s
csi-rbdplugin-sgg52                             3/3     Running   0          2m2s
csi-rbdplugin-wkq9s                             3/3     Running   0          2m2s
noobaa-core-0                                   1/1     Running   0          2m1s
noobaa-db-pg-0                                  1/1     Running   0          2m1s
noobaa-endpoint-7bb9d49898-fms5r                1/1     Running   0          46s
noobaa-operator-8b6c658f-gl82f                  1/1     Running   0          123m
ocs-metrics-exporter-5f5679bdb8-66gzd           1/1     Running   0          123m
ocs-operator-8664f5945f-6djbv                   1/1     Running   0          123m
rook-ceph-operator-74795b5c46-2gv2c             1/1     Running   0          123m
----

On OCP-DR

[source,shell]
----
oc get pods -n openshift-storage
----
.Example
----
$ oc get pods -n openshift-storage
NAME                                            READY   STATUS    RESTARTS   AGE
csi-cephfsplugin-f5snt                          3/3     Running   0          5m43s
csi-cephfsplugin-gkqdw                          3/3     Running   0          5m43s
csi-cephfsplugin-provisioner-76b7c894b9-pk9jb   6/6     Running   0          5m42s
csi-cephfsplugin-provisioner-76b7c894b9-sk5sd   6/6     Running   0          5m42s
csi-cephfsplugin-ttnt2                          3/3     Running   0          5m43s
csi-rbdplugin-8l6lh                             3/3     Running   0          5m44s
csi-rbdplugin-ft6m2                             3/3     Running   0          5m44s
csi-rbdplugin-m9n4l                             3/3     Running   0          5m44s
csi-rbdplugin-provisioner-5866f86d44-tlrxq      6/6     Running   0          5m43s
csi-rbdplugin-provisioner-5866f86d44-zr2nv      6/6     Running   0          5m43s
noobaa-core-0                                   1/1     Running   0          5m43s
noobaa-db-pg-0                                  1/1     Running   0          5m43s
noobaa-endpoint-c9878c9d6-mkq92                 1/1     Running   0          4m19s
noobaa-operator-8b6c658f-wgd8t                  1/1     Running   0          68m
ocs-metrics-exporter-5f5679bdb8-xj2js           1/1     Running   0          68m
ocs-operator-8664f5945f-vrp8n                   1/1     Running   0          68m
rook-ceph-operator-74795b5c46-cbmtk             1/1     Running   0          68m
----

== Configure RHCS `cephx` Authentication

For Multi-Cluster Metro DR architecture to be operational and proceed with the failover, the *primary
cluster* must be prevented from accessing the PVs to preserve the coherence of
the data hosted in the RHCS cluster. This will be achieved by dedicating a specific set of `cephx`
keys for each OCP cluster to access the external RHCS cluster.

Connect to your RHCS management node or any RHCS node that has the `client.admin` keyring.

=== OCP-A `cephx` Keys

[source,shell]
----
ceph auth get-or-create client.rbd.ocpa mon 'profile rbd' osd 'profile rbd' mgr 'allow rw' -o /etc/ceph/ceph.client.rbd.ocpa.keyring
ceph auth get-or-create client.cephfs-k.ocpa mon 'allow r' osd 'allow rw tag cephfs *=*' mgr 'allow rw' mds 'allow rw'   -o /etc/ceph/ceph.client.cephfs-k.ocpa.keyring
ceph auth list
----
.Example output
----
# ceph auth get-or-create client.rbd.ocpa mon 'profile rbd' osd 'profile rbd' mgr 'allow rw' -o /etc/ceph/ceph.client.rbd.ocpa.keyring
# ceph auth get-or-create client.cephfs-k.ocpa mon 'allow r' osd 'allow rw tag cephfs *=*' mgr 'allow rw' mds 'allow rw'   -o /etc/ceph/ceph.client.cephfs-k.ocpa.keyring
# ceph auth list
...[Truncated]...
client.cephfs-k.ocpa
        key: AQBuhmNgaSXMBxAA5OMASYxgUs6xkhGAuebnAw==
        caps: [mds] allow rw
        caps: [mgr] allow rw
        caps: [mon] allow r
        caps: [osd] allow rw tag cephfs *=*
  ...[Truncated]...
client.rbd.ocpa
        key: AQBthmNgeGFMGBAAXELMMDLzqUJqBhPp0xSfmQ==
        caps: [mgr] allow rw
        caps: [mon] profile rbd
        caps: [osd] profile rbd
...[Truncated]...
----

=== OCP-DR `cephx` Keys

[source,shell]
----
ceph auth get-or-create client.rbd.ocpdr mon 'profile rbd' osd 'profile rbd' mgr 'allow rw' -o /etc/ceph/ceph.client.rbd.ocpdr.keyring
ceph auth get-or-create client.cephfs-k.ocpdr mon 'allow r' osd 'allow rw tag cephfs *=*' mgr 'allow rw' mds 'allow rw'   -o /etc/ceph/ceph.client.cephfs-k.ocpdr.keyring
ceph auth list
----
.Example output
----
# ceph auth get-or-create client.rbd.ocpdr mon 'profile rbd' osd 'profile rbd' mgr 'allow rw' -o /etc/ceph/ceph.client.rbd.ocpdr.keyring
# ceph auth get-or-create client.cephfs-k.ocpdr mon 'allow r' osd 'allow rw tag cephfs *=*' mgr 'allow rw' mds 'allow rw'   -o /etc/ceph/ceph.client.cephfs-k.ocpdr.keyring
# ceph auth list
...[Truncated]...
client.cephfs-k.ocpdr
        key: AQBRh2NgRVi2BhAAxiJKSMhQtWUL219TudGdtQ==
        caps: [mds] allow rw
        caps: [mgr] allow rw
        caps: [mon] allow r
        caps: [osd] allow rw tag cephfs *=*
  ...[Truncated]...
client.rbd.ocpdr
        key: AQBQh2NgURtIFxAA+VSKMLcUNgidRQu6K8ufgQ==
        caps: [mgr] allow rw
        caps: [mon] profile rbd
        caps: [osd] profile rbd
...[Truncated]...
----

== Configure CSI Driver

=== Enable OMAP Generator

Omap generator is a sidecar container that, when deployed with the CSI provisioner pod, generates
the internal CSI `omaps` between the PV and the RBD image. The name of the new container is
`csi-omap-generator`. This is required as static *PVs* are transferred across peer clusters in
the DR use case, and hence is needed to preserve *PVC* to storage mappings.

NOTE: Execute these steps on the *primary cluster* (OCP-A) and the *seconday cluster* (OCP-DR).

Edit the rook-ceph-operator-config configmap and add `CSI_ENABLE_OMAP_GENERATOR` set to true.

[source,shell]
----
oc patch cm rook-ceph-operator-config -n openshift-storage --type json --patch  '[{ "op": "add", "path": "/data/CSI_ENABLE_OMAP_GENERATOR", "value": "true" }]'
----
.Example output:
----
configmap/rook-ceph-operator-config patched
----

Validate that there are now 7 sidecar containers and that the `csi-omap-generator` container is now running.

[source,shell]
----
oc get pods -l app=csi-rbdplugin-provisioner -o jsonpath={.items[*].spec.containers[*].name}
----
.Example output:
----
csi-provisioner csi-resizer csi-attacher csi-snapshotter csi-omap-generator csi-rbdplugin liveness-prometheus csi-provisioner csi-resizer csi-attacher csi-snapshotter csi-omap-generator csi-rbdplugin liveness-prometheus
----

There are two `csi-rbdplugin-provisioner` pods for availability so there should be two groups
of the same 7 containers for each pod.

IMPORTANT: Repeat these steps for the *secondary cluster* (OCP-DR) before proceeding and also
repeat the validation for the new `csi-omap-generator` container.

=== Customize Storage Classes

==== OCP-A CSI Secrets

CAUTION: The name of the secrets on both OCP cluster has to be identical. The only
thing that is different between the two clusters are the credentials embedded 
in the secrets.

Create a secret for your CSI RBD Plugin.

[source,shell]
----
cat rbd-secret-ocpa.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: client.rbd.metrodr
  namespace: openshift-storage
stringData:
  # Key values correspond to a user name and its key, as defined in the
  # ceph cluster. User ID should have required access to the 'pool'
  # specified in the storage class
  userID: {your-rbd-cephx-user-id}
  userKey: {cephx-key-for-user}

  # Encryption passphrase
  encryptionPassphrase: test_passphrase
oc create -f rbd-secret-ocpa.yaml
----
.Example output
----
cat <<EOF | oc create -f -
> ---
> apiVersion: v1
> kind: Secret
> metadata:
>   name: client.rbd.metrodr
>   namespace: openshift-storage
> stringData:
>   # Key values correspond to a user name and its key, as defined in the
>   # ceph cluster. User ID should have required access to the 'pool'
>   # specified in the storage class
>   userID: rbd.ocpa
>   userKey: AQBthmNgeGFMGBAAXELMMDLzqUJqBhPp0xSfmQ==
>
>   # Encryption passphrase
>   encryptionPassphrase: test_passphrase
EOF
secret/client.rbd.metrodr created
----

Create a secret for your CSI CephFS Plugin.

[source,shell]
----
cat cephfs-secret-ocpa.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: client.cephfs.metrodr
  namespace: openshift-storage
stringData:
  # Key values correspond to a user name and its key, as defined in the
  # ceph cluster. User ID should have required access to the 'pool'
  # specified in the storage class
  userID: {your-cephfs-cephx-user-id}
  userKey: {cephx-key-for-user}

  # Encryption passphrase
  encryptionPassphrase: test_passphrase
oc create -f cephfs-secret-ocpa.yaml
----
.Example output
----
cat <<EOF | oc create -f -
> ---
> apiVersion: v1
> kind: Secret
> metadata:
>   name: client.cephfs.metrodr
>   namespace: openshift-storage
> stringData:
>   # Key values correspond to a user name and its key, as defined in the
>   # ceph cluster. User ID should have required access to the 'pool'
>   # specified in the storage class
>   userID: cephfs-k.ocpa
>   userKey: AQBuhmNgaSXMBxAA5OMASYxgUs6xkhGAuebnAw==
>
>   # Encryption passphrase
>   encryptionPassphrase: test_passphrase
EOF
secret/client.cephfs.metrodr created
----

==== OCP-DR CSI Secrets

Create a secret for your CSI RBD Plugin.

[source,shell]
----
cat rbd-secret-ocpdr.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: client.rbd.metrodr
  namespace: openshift-storage
stringData:
  # Key values correspond to a user name and its key, as defined in the
  # ceph cluster. User ID should have required access to the 'pool'
  # specified in the storage class
  userID: {your-rbd-cephx-user-id}
  userKey: {cephx-key-for-user}

  # Encryption passphrase
  encryptionPassphrase: test_passphrase
oc create -f rbd-secret-ocpdr.yaml
----
.Example output
----
cat <<EOF | oc create -f -
> ---
> apiVersion: v1
> kind: Secret
> metadata:
>   name: client.rbd.metrodr
>   namespace: openshift-storage
> stringData:
>   # Key values correspond to a user name and its key, as defined in the
>   # ceph cluster. User ID should have required access to the 'pool'
>   # specified in the storage class
>   userID: rbd.ocpdr
>   userKey: AQBQh2NgURtIFxAA+VSKMLcUNgidRQu6K8ufgQ==
>
>   # Encryption passphrase
>   encryptionPassphrase: test_passphrase
EOF
secret/client.rbd.metrodr created
----

Create a secret for your CSI CephFS Plugin.

[source,shell]
----
cat cephfs-secret-ocpdr.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: client.cephfs.metrodr
  namespace: openshift-storage
stringData:
  # Key values correspond to a user name and its key, as defined in the
  # ceph cluster. User ID should have required access to the 'pool'
  # specified in the storage class
  userID: {your-cephfs-cephx-user-id}
  userKey: {cephx-key-for-user}

  # Encryption passphrase
  encryptionPassphrase: test_passphrase
oc create -f cephfs-secret-ocpdr.yaml
----
.Example output
----
cat <<EOF | oc create -f -
> ---
> apiVersion: v1
> kind: Secret
> metadata:
>   name: client.cephfs.metrodr
>   namespace: openshift-storage
> stringData:
>   # Key values correspond to a user name and its key, as defined in the
>   # ceph cluster. User ID should have required access to the 'pool'
>   # specified in the storage class
>   userID: cephfs-k.ocpdr
>   userKey: AQBRh2NgRVi2BhAAxiJKSMhQtWUL219TudGdtQ==
>
>   # Encryption passphrase
>   encryptionPassphrase: test_passphrase
EOF
secret/client.cephfs.metrodr created
----

==== Customize OCP-A Storage Classes

Create a new storage class configured with the secret we created for RBD access.

[source,shell]
----
cat sc-rbd-ocpa.yaml
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    description: Provides RWO Filesystem volumes, and RWO and RWX Block volumes
  name: ocs-external-storagecluster-ceph-rbd-dr
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: client.rbd.metrodr
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  imageFeatures: layering
  imageFormat: "2"
  pool: {your-rbd-pool-name}
provisioner: openshift-storage.rbd.csi.ceph.com
reclaimPolicy: Retain
volumeBindingMode: Immediate
oc create -f sc-rbd-ocpa.yaml
----
.Example output
----
cat <<EOF | oc create -f -
> ---
> allowVolumeExpansion: true
> apiVersion: storage.k8s.io/v1
> kind: StorageClass
> metadata:
>   annotations:
>     description: Provides RWO Filesystem volumes, and RWO and RWX Block volumes
>   name: ocs-external-storagecluster-ceph-rbd-dr
> parameters:
>   clusterID: openshift-storage
>   csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
>   csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
>   csi.storage.k8s.io/fstype: ext4
>   csi.storage.k8s.io/node-stage-secret-name: client.rbd.metrodr
>   csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
>   csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
>   csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
>   imageFeatures: layering
>   imageFormat: "2"
>   pool: rbd
> provisioner: openshift-storage.rbd.csi.ceph.com
> reclaimPolicy: Retain
> volumeBindingMode: Immediate
EOF
storageclass.storage.k8s.io/ocs-external-storagecluster-ceph-rbd-dr created
----

Create a new storage class configured with the secret we created for CephFS access.

[source,shell]
----
cat sc-cephfs-ocpa.yaml
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    description: Provides RWO and RWX Filesystem volumes
  name: ocs-external-storagecluster-cephfs-dr
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  csi.storage.k8s.io/node-stage-secret-name: client.cephfs.metrodr
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  fsName: {your-cephfs-fs-name}
  pool: {your-cephfs-data-pool}
provisioner: openshift-storage.cephfs.csi.ceph.com
reclaimPolicy: Retain
volumeBindingMode: Immediate
oc create -f sc-cephfs-ocpa.yaml
----
.Example output
----
cat <<EOF | oc create -f -
> ---
> allowVolumeExpansion: true
> apiVersion: storage.k8s.io/v1
> kind: StorageClass
> metadata:
>   annotations:
>     description: Provides RWO and RWX Filesystem volumes
>   name: ocs-external-storagecluster-cephfs-dr
> parameters:
>   clusterID: openshift-storage
>   csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
>   csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
>   csi.storage.k8s.io/node-stage-secret-name: client.cephfs.metrodr
>   csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
>   csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
>   csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
>   fsName: cephfs
>   pool: cephfs_data
> provisioner: openshift-storage.cephfs.csi.ceph.com
> reclaimPolicy: Retain
> volumeBindingMode: Immediate
EOF
storageclass.storage.k8s.io/ocs-external-storagecluster-cephfs-dr created
----

==== Test OCP-A Storage Classes

Make sure the new storage classes work as expected with the specific credentials
that were created and configured into the RBD and CephFS storage classes.

===== Verify Storage Classes

include::partial$metro-multi-basictesting.adoc[]

IMPORTANT: Once the testing is complete you can clean up the left over *PersistentVolumes* as
a result of the *ReclaimPolicy* specific to the new storage classes.

==== Customize OCP-DR Storage Classes

Create a new storage class configured with the secret we created for RBD access.

[source,shell]
----
cat sc-rbd-ocpdr.yaml
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    description: Provides RWO Filesystem volumes, and RWO and RWX Block volumes
  name: ocs-external-storagecluster-ceph-rbd-dr
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: client.rbd.metrodr
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  imageFeatures: layering
  imageFormat: "2"
  pool: {your-rbd-pool-name}
provisioner: openshift-storage.rbd.csi.ceph.com
reclaimPolicy: Retain
volumeBindingMode: Immediate
oc create -f sc-rbd-ocpdr.yaml
----
.Example output
----
cat <<EOF | oc create -f -
> ---
> allowVolumeExpansion: true
> apiVersion: storage.k8s.io/v1
> kind: StorageClass
> metadata:
>   annotations:
>     description: Provides RWO Filesystem volumes, and RWO and RWX Block volumes
>   name: ocs-external-storagecluster-ceph-rbd-dr
> parameters:
>   clusterID: openshift-storage
>   csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
>   csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
>   csi.storage.k8s.io/fstype: ext4
>   csi.storage.k8s.io/node-stage-secret-name: client.rbd.metrodr
>   csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
>   csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
>   csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
>   imageFeatures: layering
>   imageFormat: "2"
>   pool: rbd
> provisioner: openshift-storage.rbd.csi.ceph.com
> reclaimPolicy: Retain
> volumeBindingMode: Immediate
EOF
storageclass.storage.k8s.io/ocs-external-storagecluster-ceph-rbd-dr created
----

Create a new storage class configured with the secret we created for CephFS access.

[source,shell]
----
cat sc-cephfs-ocpdr.yaml
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    description: Provides RWO and RWX Filesystem volumes
  name: ocs-external-storagecluster-cephfs-dr
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  csi.storage.k8s.io/node-stage-secret-name: client.cephfs.metrodr
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  fsName: {your-cephfs-fs-name}
  pool: {your-cephfs-data-pool}
provisioner: openshift-storage.cephfs.csi.ceph.com
reclaimPolicy: Retain
volumeBindingMode: Immediate
oc create -f sc-cephfs-ocpdr.yaml
----
.Example output
----
cat <<EOF | oc create -f -
> ---
> allowVolumeExpansion: true
> apiVersion: storage.k8s.io/v1
> kind: StorageClass
> metadata:
>   annotations:
>     description: Provides RWO and RWX Filesystem volumes
>   name: ocs-external-storagecluster-cephfs-dr
> parameters:
>   clusterID: openshift-storage
>   csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
>   csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
>   csi.storage.k8s.io/node-stage-secret-name: client.cephfs.metrodr
>   csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
>   csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
>   csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
>   fsName: cephfs
>   pool: cephfs_data
> provisioner: openshift-storage.cephfs.csi.ceph.com
> reclaimPolicy: Retain
> volumeBindingMode: Immediate
EOF
storageclass.storage.k8s.io/ocs-external-storagecluster-cephfs-dr created
----

IMPORTANT: The name of the storage classes have to be identical on both clusters.

===== Verify Storage Classes

include::partial$metro-multi-basictesting.adoc[]

IMPORTANT: Once the testing is complete you can clean up the left over *PersistentVolumes* as
a result of the *ReclaimPolicy* specific to the new storage classes.

== Deploy Sample Application

include::partial$dr_multicluster_sample_application.adoc[]

== Install OADP (OpenShift API for Data Protection)

include::partial$install_oadp_ui.adoc[]

== Failover from OCP-A to OCP-DR

=== Backup Application Metadata

The Kubernetese objects or resources for the OpenShift namespace `my-database-app` have to be
backed up and stored in a location where the *secondary cluster* can access. In this case
using the `OADP` or `Velero` *Backup* API is how this will be done. 

Here is a sample `backup.yaml` file for the sample application.

[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
  namespace: oadp-operator
  name: backup1
spec:
  includedNamespaces:
  - my-database-app
  excludedResources:
  - imagetags.image.openshift.io
  snapshotVolumes: false
----  

Given the persistent data lives in a single external cluster shared by the *primary cluster* (OCP-A)
and the *secondary cluster* (OCP-DR) clusters, we do not need the `OADP` *Backup* to include the data
and therefore set `snapshotVolumes: false`.

There is one additional resource to exclude that will be done by adding a label to the specific `configmap`. 

[source,shell]
----
oc label -n my-database-app configmaps rails-pgsql-persistent-1-ca velero.io/exclude-from-backup=true
----
.Example output:
----
configmap/rails-pgsql-persistent-1-ca labeled
----

Now create the *Backup* for `my-database-app` namespace.

[source,shell]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/backup.yaml | oc apply -f -
----
.Example output:
----
backup.velero.io/backup1 created
----

Verify the *Backup* completed successfully using your `S3` bucket using the following command:

[source,shell]
----
oc describe backup backup1 -n oadp-operator
----
.Example output:
----
Name:         backup1
Namespace:    oadp-operator
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion: v1.20.0+bafe72f
              velero.io/source-cluster-k8s-major-version: 1
              velero.io/source-cluster-k8s-minor-version: 20
API Version:  velero.io/v1
Kind:         Backup

[...]
Spec:
  Default Volumes To Restic:  false
  Excluded Resources:
    imagetags.image.openshift.io <1>
  Hooks:
  Included Namespaces:
    my-database-app <2>
  Snapshot Volumes:  false
  Storage Location:  default
  Ttl:               720h0m0s
Status:
  Completion Timestamp:  2021-04-14T21:32:28Z
  Expiration:            2021-05-14T21:31:44Z
  Format Version:        1.1.0
  Phase:                 Completed <3>
  Progress:
    Items Backed Up:  101 <4>
    Total Items:      101
  Start Timestamp:    2021-04-14T21:31:44Z
  Version:            1
  Warnings:           8
Events:               <none>
----
<1> Excluded resources for backup
<2> Namespace for which resources copied to object bucket
<3> Successul backup with Completed status
<4> The number of Kubernetes resources backed up

=== Shutdown OCP-A

==== Scaling application down on primary cluster

The reason for Disaster Recovery (DR) of an OCP cluster or application would usually happen
because the *primary cluster* has become partially or completely unavailable. In order to
simulate this behavior for our sample application the easiest way is to scale the deployments
down on the *primary cluster* so as to make the application unavailable and to shutdown down
the nodes of the cluster.

Let's take a look at the *DeploymentConfig* for our application.

[source,shell]
----
oc get deploymentconfig -n my-database-app
----
.Example output:
----
NAME                     REVISION   DESIRED   CURRENT   TRIGGERED BY
postgresql               1          1         1         config,image(postgresql:10)
rails-pgsql-persistent   1          1         1         config,image(rails-pgsql-persistent:latest)
----

There are two *DeploymentConfig* to scale to zero.

[source,shell]
----
oc scale deploymentconfig postgresql -n my-database-app --replicas=0
----
.Example output:
----
deploymentconfig.apps.openshift.io/postgresql scaled
----

Now scale the second deployment to zero.

[source,shell]
----
oc scale deploymentconfig rails-pgsql-persistent -n my-database-app --replicas=0
----
.Example output:
----
deploymentconfig.apps.openshift.io/rails-pgsql-persistent scaled
----

Check to see the *Pods* are deleted. The following command should return *_no_* results
if both *DeploymentConfig* are scaled to zero.

[source,shell]
----
oc get pods -n my-database-app | grep Running
----

Test that the application is down on the *primary cluster* by refreshing the route in
your browser or get route again and copy to browser tab.

[source,shell]
----
oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"
----

You show see something like this now.

.Sample application is offline
image::sample-app-down-primary.png[Sample application is offline]

==== Fencing OCP-A Cluster

IMPORTANT: Shutdown all nodes in cluster OCP-A.

Once this is done, make sure the `cephx` user configured
in the cluster in *down* status can not longer access the external RHCS cluster. This is
to prevent any risk of concurrent access to block devices hosted in the RHCS cluster shared
by the two OCP clusters.

Connect to your RHCS management node and delete the `client.rbd.ocpa` and `client.cephfs-k.ocpa`
`cephx` user names after backing them up.

[source,shell]
----
ceph auth export {rbd_ocpa_user_name} >client.rbd.data
ceph auth del {rbd_ocpa_user_name}
ceph auth export {cephfs_ocpa_user_name} >client.cephfs.data
ceph auth del {cephfs_ocpa_user_name}
----
.Example output
----
# ceph auth export client.rbd.ocpa >client.rbd.ocpa.data
export auth(key=AQBthmNgeGFMGBAAXELMMDLzqUJqBhPp0xSfmQ==)
# ceph auth del client.rbd.ocpa
updated
# ceph auth export client.cephfs-k.ocpa >client.cephfs.ocpa.data
export auth(key=AQBuhmNgaSXMBxAA5OMASYxgUs6xkhGAuebnAw==)
# ceph auth del client.cephfs-k.ocpa
updated
----

NOTE: When needed you can easily import the old `cephx` definition that you exported to
make sure the `cephx` user is recreated indentically.

=== Restore Application on OCP-DR

The last step in the process to failover to the *secondary cluster* (OCP-DR) is to now use `OADP` and
the *Restore* CR to copy the application metadata to the *_remote_* cluster. The persistent data is
already present in the external RHCS cluster and therefore does not need to be restored.

Here is a the `restore.yaml` file for the sample application.

[source,yaml]
----
apiVersion: velero.io/v1
kind: Restore
metadata:
  namespace: oadp-operator
  name: restore1
spec:
  backupName: backup1
  includedNamespaces:
  - my-database-app
----

Now create the *Restore* on the *secondary cluster* (OCP-DR) for the `my-database-app` namespace.
You notice in the *Restore* that the `backup1` created earlier is referenced.

IMPORTANT: Make sure to issue this command on the *secondary cluster* (OCP-DR). The namespace
`my-database-app` should not exist on this cluster at this point.

[source,shell]
----
curl -s https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/training/modules/ocs4/attachments/restore.yaml | oc apply -f -
----
.Example output:
----
restore.velero.io/restore1 created
----

Verify the *Restore* completed successfully using your `S3` bucket using the following command.

[source,shell]
----
oc describe restore restore1 -n oadp-operator
----
.Example output
----
Name:         restore1
Namespace:    oadp-operator
Labels:       <none>
Annotations:  <none>
API Version:  velero.io/v1
Kind:         Restore

[...]
Spec:
  Backup Name:  backup1 <1>
  Excluded Resources:
    nodes
    events
    events.events.k8s.io
    backups.velero.io
    restores.velero.io
    resticrepositories.velero.io
  Included Namespaces:
    my-database-app <2>
Status:
  Completion Timestamp:  2021-04-14T23:11:40Z
  Phase:                 Completed <3>
  Start Timestamp:       2021-04-14T23:11:26Z
  Warnings:              7
Events:                  <none>
----
<1> Name of backup used for restore operation
<2> Namespace to be restored from backup1
<3> Successul restore with Completed status 

Check to see that the *PODs* and *PVC* are created correctly in `my-database-app`namespace
on *secondary cluster* (OCP-DR).

[source,shell]
----
oc get pods,pvc -n my-database-app
----
.Example output:
----
NAME                                    READY   STATUS      RESTARTS   AGE
pod/postgresql-1-bthzg                  1/1     Running     0          111s
pod/postgresql-1-deploy                 0/1     Completed   0          115s
pod/rails-pgsql-persistent-1-build      1/1     Running     0          107s
pod/rails-pgsql-persistent-1-deploy     0/1     Completed   0          107s
pod/rails-pgsql-persistent-1-hook-pre   0/1     Completed   0          104s
pod/rails-pgsql-persistent-1-x9ktb      1/1     Running     0          56s

NAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                              AGE
persistentvolumeclaim/postgresql   Bound    pvc-35f21284-cc83-479e-97db-3f778980908f   5Gi        RWO            ocs-external-storagecluster-ceph-rbd-dr   119s
----

=== Verify Application Running on OCP-DR

To verify the application on the *secondary cluster* you will want to access the
application again and create a new article. 

[source,shell]
----
oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"
----

This will return a route similar to this one.

.Example output:
----
http://rails-pgsql-persistent-my-database-app.apps.ocp45dr.ocstraining.com/articles
----

Copy your route (different than above) to a browser window to create another article on the *secondary cluster*.

Enter the `username` and `password` below to create articles and comments.

----
username: openshift
password: secret
----

Once you have added a new article you can verify it exists in the `postgresql` database by issuing this command:

[source,shell]
----
oc rsh -n my-database-app $(oc get pods -n my-database-app|grep postgresql | grep -v deploy | awk {'print $1}') psql -c "\c root" -c "select * from articles"
----
.Example output:
----
You are now connected to database "root" as user "postgres".
 id |           title            |                                                              body                                                              |         created_at         |         updated_at

----+----------------------------+--------------------------------------------------------------------------------------------------------------------------------+----------------------------+-----------------------
-----
  1 | Test Metro Multicluster DR | This article is to prove that we can restart the application in the OCP-DR cluster when the OCP-A cluster becomes unavailable. | 2021-04-14 21:27:35.537882 | 2021-04-14 21:27:35.53
7882
  2 | Second Article             | Creating a new article in the second OCP cluster while the original cluster is down.                                           | 2021-04-14 23:17:10.971044 | 2021-04-14 23:17:10.97
1044
(2 rows)
----

You should see your first article created on the *primary cluster* (OCP-A) and the second article created on
the *secondary cluster* (OCP-DR). The application is now verified and the failover is completed.

IMPORTANT: If you want to delete the `my-database-app` project from the both clusters, it is important to modify
the associated *PV* `reclaimPolicy` from `Retain` to `Delete`. Then, when the `my-database-app` project and *PVC*
is deleted, the associated *PV* will be deleted as well as the associated image in Ceph.

== Failback from OCP-DR to OCP-A

In order to failback to the *primary cluster* (OCP-A) from the *secondary cluster* (OCP-DR) repeat the steps for
failover except reverse the order.

CAUTION: When failing back it is imperative that the application be stopped in cluster OCP-DR and that no
application accesses the PVs from that cluster.

=== Un-Fencing OCP-A Cluster

Only exception to the procedure is that after fencing the OCP-DR cluster you will need to recreate the `cephx`
user names for the original cluster in your RHCS cluster. Use the following commands to recreate the correct
user name definition.

[source,shell]
----
ceph auth import -i client.rbd.data
ceph auth get {your_rbd_username}
ceph auth import -i client.cephfs.data
ceph auth get {your_cephfs_username}
----
.Example output
----
# ceph auth import -i client.rbd.ocpa.data
imported keyring
# ceph auth get client.rbd.ocpa
[client.rbd.ocpa]
	key = AQBthmNgeGFMGBAAXELMMDLzqUJqBhPp0xSfmQ==
	caps mgr = "allow rw"
	caps mon = "profile rbd"
	caps osd = "profile rbd"
# ceph auth import -i client.cephfs.ocpa.data
imported keyring
# ceph auth get client.cephfs-k.ocpa
exported keyring for client.cephfs-k.ocpa
[client.cephfs-k.ocpa]
	key = AQBuhmNgaSXMBxAA5OMASYxgUs6xkhGAuebnAw==
	caps mds = "allow rw"
	caps mgr = "allow rw"
	caps mon = "allow r"
	caps osd = "allow rw tag cephfs *=*"
----

IMPORTANT: Make sure your user name capabilities and keys are identical to what they used to be
before you deleted them.

=== Verify Application Running on OCP-A

To verify the application on the *primary cluster* (OCP-A) you will want to access the application again
and verify that the article you created on the *secondary custer* (OCP-DR) is present.

There are two *DeploymentConfig* to scale to 1.

[source,shell]
----
oc scale deploymentconfig postgresql -n my-database-app --replicas=1
----
.Example output:
----
deploymentconfig.apps.openshift.io/postgresql scaled
----

Now scale the second deployment to 1.

[source,shell]
----
oc scale deploymentconfig rails-pgsql-persistent -n my-database-app --replicas=1
----
.Example output:
----
deploymentconfig.apps.openshift.io/rails-pgsql-persistent scaled
----

Check to see the *Pods* are running. The following command should return two pods if both
*DeploymentConfig* are scaled corerctly.

[source,shell]
----
oc get pods -n my-database-app | grep Running
----

[source,shell]
----
oc get route rails-pgsql-persistent -n my-database-app -o jsonpath --template="http://{.spec.host}/articles{'\n'}"
----

This will return a route similar to this one.

.Example output:
----
http://rails-pgsql-persistent-my-database-app.apps.ocp45.ocstraining.com/articles
----

Copy your route (different than above) to a browser window to create another article
on the *primary cluster*.

Enter the `username` and `password` below to create articles and comments.

----
username: openshift
password: secret
----

Once you have added a new article you can verify it exists in the `postgresql` database together with the original
article, the one added after the failover and the last one you just added by issuing this command:

[source,shell]
----
oc rsh -n my-database-app $(oc get pods -n my-database-app|grep postgresql | grep -v deploy | awk {'print $1}') psql -c "\c root" -c "select * from articles"
----
.Example output:
----
You are now connected to database "root" as user "postgres".
 id |           title            |                                                              body                                                              |         created_at         |         updated_at

----+----------------------------+--------------------------------------------------------------------------------------------------------------------------------+----------------------------+-----------------------
-----
  1 | Test Metro Multicluster DR | This article is to prove that we can restart the application in the OCP-DR cluster when the OCP-A cluster becomes unavailable. | 2021-04-14 21:27:35.537882 | 2021-04-14 21:27:35.53
7882
  2 | Second Article             | Creating a new article in the second OCP cluster while the original cluster is down.                                           | 2021-04-14 23:17:10.971044 | 2021-04-14 23:17:10.97
1044
  3 | Third Article              | Created after we failed over to the original OCP cluster (OCP-A).                                                              | 2021-04-14 23:44:08.454192 | 2021-04-14 23:44:42.53
0012
(3 rows)
----

You should see all three (3) articles. The application is now verified and the failback is completed.

.Articles After Failover and Failback
image::ODF-4.7-OCP4-ApplicationArticles-FO-FB.png[Verify Application After Failback]

Et voil√†!!!

