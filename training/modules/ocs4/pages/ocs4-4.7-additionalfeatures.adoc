= OpenShift Data Foundation 4.7 Additional Features
:toc:
:toclevels: 4
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:hide-uri-scheme:

== Overview

The intent of this guide is to detail additional OpenShift Data Foundation (ODF)
4.7 features, some being considered Development Preview and undocumented
until a later version becomes available.

IMPORTANT: Starting April 2021, `OpenShift Container Storage` (OCS) has been rebranded
to `OpenShift Data Foundation` (ODF).

[start=1]
. *OSD BlueStore RocksDB metadata and WAL placement customization.* +
Learn how to create a custom *StorageCluster* to specify which devices are used for placing the OSD components.
. *Mixed device type OSD.* +
Learn how to create a custom *StorageCluster* that uses mixed device types (HDD, SSD and NVMe).
. *Ceph configuration override.* +
Learn how to pass custom Ceph parameters during your cluster deployment.

== OSD BlueStore RocksDB metadata and WAL placement

With ODF 4.7 you can customized the deployment of your OSDs when it comes to what device to use
for what part of BlueStore. Let's consider a standard *StorageCluster* Custom Resource below:

[source,shell]
----
---
apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  manageNodes: false
  monDataDirHostPath: /var/lib/rook
  storageDeviceSets:
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
    name: {deviceset-prefix}
    portable: false
    replica: 3
----

This will deploy a cluster in a very standard way where every piece of the OSD is collocated
on the PVC that will be provisioned using the storage class `\{storageclass\}`. The size of
PersistentVolume that will be provisioned is specified as `\{size\}`.

=== RocksDB metadata placement

IMPORTANT: This feature is Development Preview.

You can now add the following section to your `storageDeviceSets` parameter to customize
the placement of RocksDB at deployment time.

[source,shell]
----
    metadataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
----

When added to your storage cluster the following will happen:

. Each OSD data will claim a PVC as `\{deviceset-prefix-x-data-y\}`
. Each OSD RockDB metadata will claim a PVC as `\{deviceset-prefix-x-metadata-y\}`

CAUTION: The deployment process will provision PVC and will not be able to partition
an existing device. Make sure to configure your RocksDB metadata partitions through
LSO before deploying your storage cluster.

=== RocksDB WAL placement

IMPORTANT: This feature is Development Preview.

You can now add the following section to your `storageDeviceSets` parameter to customize
the placement of the RocksDB Write Ahead Log (WAL) at deployment time.

[source,shell]
----
    walPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
----

When added to your storage cluster the following will happen:

. Each OSD data will claim a PVC as `\{deviceset-prefix-x-data-y\}`
. Each OSD RockDB WAL will claim a PVC as `\{deviceset-prefix-x-wal-y\}`

NOTE: The WAL placement can be combined with the RocksDB metadata placement.

CAUTION: The deployment process will provision PVC and will not be able to partition
an existing device. Make sure to configure your RocksDB WAL partitions through LSO
before deploying your storage cluster.

== Mixed OSD device type configuration

With ODF 4.7 you can customize the deployment of your OSDs to consume different device
types. This feature can be combined with the BlueStore placement customization and
it is illustrated below.

[source,shell]
----
---
apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  managedResources:
  manageNodes: false
  monDataDirHostPath: /var/lib/rook
  storageDeviceSets:
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
    name: ocs-deviceset-nvme
    portable: false
    replica: 3
    deviceType: nvme <1>
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
    name: ocs-deviceset-hdd
    portable: false
    replica: 3
    deviceType: hdd <2>
    metadataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
  - count: 1
    dataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
    name: ocs-deviceset-mix
    portable: false
    replica: 3
    deviceType: ssd <3>
    metadataPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
    walPVCTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: {size}
        storageClassName: {storageclass}
        volumeMode: Block
----
<1> The authorized values for the device types are `hdd`, `ssd` and `nvme`. Those device types will be used
to assign a CRUSH device class within your underlying cluster.

Here is an example of the CRUSH tree being generated in the underlying cluster with the
specific CRUSH device class value assigned.

.CRUSH tree
----
ID  CLASS WEIGHT   TYPE NAME                        STATUS REWEIGHT PRI-AFF
 -1       39.75000 root default
 -7       39.75000     region us-east-2
-18       13.25000         zone us-east-2a
-33        8.50000             host ip-10-0-149-187
  0   hdd  8.50000                 osd.0                up  1.00000 1.00000
-17        4.75000             host ip-10-0-152-149
  3  nvme  0.50000                 osd.3                up  1.00000 1.00000
  5   ssd  4.25000                 osd.5                up  1.00000 1.00000
 -6       13.25000         zone us-east-2b
-41        8.50000             host ip-10-0-161-186
  8   hdd  8.50000                 osd.8                up  1.00000 1.00000
 -5        4.75000             host ip-10-0-179-156
  1  nvme  0.50000                 osd.1                up  1.00000 1.00000
  2   ssd  4.25000                 osd.2                up  1.00000 1.00000
-26       13.25000         zone us-east-2c
-25        4.75000             host ip-10-0-196-12
  4  nvme  0.50000                 osd.4                up  1.00000 1.00000
  7   ssd  4.25000                 osd.7                up  1.00000 1.00000
-37        8.50000             host ip-10-0-211-21
  6   hdd  8.50000                 osd.6                up  1.00000 1.00000
----

NOTE: The CRUSH weight assigned to the OSDs does not reflect the reality of what was
allocated in the *StorageCluster* definition when using `metadataPVCTemplate`
and `dataPVCTTemplate`. A bug report was filed to address this
minor issue https://bugzilla.redhat.com/show_bug.cgi?id=1952661[here].

== Ceph configuration override

ODF 4.7 allows you to create a custom configuration map containing Ceph configuration
parameters that will be added to the default Ceph configuration parameters when deployed
via the ODF operator.

To achieve this, your *StorageCluster* CR must be configured specifically to inform 
the ODF operator that a custom configuration is configured for the cluster.

[source,shell]
----
spec:
  managedResources:
    cephConfig:
      reconcileStrategy: ignore
[...]
----

Once this parameter is added to your *StorageClkuster* CR you simply have to create
a specific ConfigurationMap to be used by the operator during the deployment.
[source,shell]
----
apiVersion: v1
data:
  config: |2

    [global]
    mon_osd_full_ratio = .85
    mon_osd_backfillfull_ratio = .80
    mon_osd_nearfull_ratio = .75
    mon_max_pg_per_osd = 600
    [osd]
    osd_pool_default_min_size = 1
    osd_pool_default_size = 2
    osd_memory_target_cgroup_limit_ratio = 0.5
kind: ConfigMap
metadata:
  name: rook-config-override
  namespace: openshift-storage
----

Et voil√†!
